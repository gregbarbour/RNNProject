{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import math\n",
    "import pickle\n",
    "from keras.models import Sequential, Model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from keras.layers import BatchNormalization, Layer, TimeDistributed, Dropout\n",
    "from keras.layers import Dense, Input, Masking, LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by trying to load in our toy jets, and then separate into features (to train on) and labels (to predict).\n",
    "\n",
    "For features, first try d0, z0, phi, theta, qOverP, (refPx, refPy, refPz)?. Essentially the track parameters.\n",
    "\n",
    "For labels, Xs, Ys, Zs, Xt, Yt, Zt. That is the secondary and tertiary vertices. Omit the primary as this has been fixed to (0,0,0). This will require some smart selection for c and light jets, where not all vertices are present. If a vertex is not present, could try predicting (0,0,0) or (-1,-1,-1) or previous vertex (prim or sec)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bjets_DF = pd.read_pickle(\"./bjets.pkl\")\n",
    "cjets_DF = pd.read_pickle(\"./cjets.pkl\")\n",
    "ljets_DF = pd.read_pickle(\"./ljets.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the track parameters as our features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 9.55 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trks=np.zeros((len(bjets_DF), 30, 5))\n",
    "\n",
    "for i in range(len(bjets_DF)):\n",
    "    trks[i] = np.array([bjets_DF['tracks'][i]])[:,:,0:5]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = trks # following convention name the features as the vector 'X'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the secondary and tertiary vertices as our labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = bjets_DF[['secVtx_x','secVtx_y','secVtx_z','terVtx_x','terVtx_y','terVtx_z']].values \n",
    "y = y*1000 # change units of vertices from m to mm, keep vals close to unity\n",
    "# again convention call labels 'y'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split first 80000 jets as train and next 20000 as test. Below some plots to show these jets are equivalently distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=X[:80000]\n",
    "X_test=X[80000:]\n",
    "y_train=y[:80000]\n",
    "y_test=y[80000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_s1=np.linalg.norm(bjets_DF[['secVtx_x','secVtx_y','secVtx_z']],axis=1)[:80000]\n",
    "b_s2=np.linalg.norm(bjets_DF[['secVtx_x','secVtx_y','secVtx_z']],axis=1)[80000:]\n",
    "b_t1=np.linalg.norm(bjets_DF[['terVtx_x','terVtx_y','terVtx_z']],axis=1)[:80000]\n",
    "b_t2=np.linalg.norm(bjets_DF[['terVtx_x','terVtx_y','terVtx_z']],axis=1)[80000:]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0036985106615352826\n",
      "0.007068593500451309\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(b_s1,bins='scott',range=[0,0.01])\n",
    "#plt.hist(c_SecVtx,bins='scott',range=[0,0.01])\n",
    "plt.hist(b_t1,bins='scott',range=[0,0.01])\n",
    "\n",
    "print(np.mean(b_s1))\n",
    "print(np.mean(b_t1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0036550417303922366\n",
      "0.006973962985526242\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(b_s2,bins='scott',range=[0,0.01])\n",
    "#plt.hist(c_SecVtx,bins='scott',range=[0,0.01])\n",
    "plt.hist(b_t2,bins='scott',range=[0,0.01])\n",
    "\n",
    "print(np.mean(b_s2))\n",
    "print(np.mean(b_t2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So finally we have our features, X, and labels, y. Split into training and testing samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing and Training an RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create an RNN based on LSTM cells using keras and tensorflow. The RNN will for each jet candidate take the tracks as inputs and attempt to predict the secondary and tertiary vertex positions. Let's see how well it does.\n",
    "\n",
    "I anticipate having to set a tolerance on the predicted values, it will never get them perfectly but we need to tell it how close it has to get for it to be considered successful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin by creating the RNN architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select number of hidden and dense layers. Initially use same as RNNIP but these can be tuned going forward.\n",
    "\n",
    "nHidden = 100\n",
    "nDense = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nJets, nTrks, nFeatures = X_train.shape\n",
    "nOutputs = y.shape[1] # ie sec and ter vtx xyz, so 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trk_inputs = Input(shape=(nTrks,nFeatures),name=\"Trk_inputs\")\n",
    "masked_input = Masking()(trk_inputs)\n",
    "\n",
    "# Feed this merged layer to an RNN\n",
    "lstm = LSTM(nHidden, return_sequences=False, name='LSTM')(masked_input)\n",
    "dpt = Dropout(rate=0.8)(lstm)\n",
    "\n",
    "my_inputs = trk_inputs\n",
    "\n",
    "# Fully connected layer: This will convert the output of the RNN to our vtx postion predicitons\n",
    "FC = Dense(nDense, activation='relu', name=\"Dense\")(dpt)\n",
    "\n",
    "# Ouptut layer. Sec and Ter Vtx. No activation as this is a regression problem\n",
    "output = Dense(nOutputs, name=\"Vertex_Predictions\")(FC)\n",
    "\n",
    "myRNN = Model(inputs=my_inputs, outputs=output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Trk_inputs (InputLayer)      (None, 30, 5)             0         \n",
      "_________________________________________________________________\n",
      "masking_1 (Masking)          (None, 30, 5)             0         \n",
      "_________________________________________________________________\n",
      "LSTM (LSTM)                  (None, 100)               42400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "Dense (Dense)                (None, 20)                2020      \n",
      "_________________________________________________________________\n",
      "Vertex_Predictions (Dense)   (None, 6)                 126       \n",
      "=================================================================\n",
      "Total params: 44,546\n",
      "Trainable params: 44,546\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "myRNN.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "myRNN.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mae']) # do i want to add a metric like mse to evaluate during training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " checkpoints, training, evaluation of performance\n",
    " different ways of evaluating performance obviously\n",
    " either akin to Nicole's method for RNNIP\n",
    " or the slighlty different method in https://github.com/agu3rra/NeuralNetwork-RegressionExample/blob/master/Tutorial.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "myRNN_mChkPt = ModelCheckpoint('myRNN_weights.h5',monitor='val_loss', verbose=True,\n",
    "                               save_best_only=True,\n",
    "                               save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStop = EarlyStopping(monitor='val_loss', verbose=True, patience=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 20000 samples\n",
      "Epoch 1/100\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 2.1386 - mean_absolute_error: 2.1386Epoch 00001: val_loss improved from inf to 1.84613, saving model to myRNN_weights.h5\n",
      "60000/60000 [==============================] - 34s 563us/step - loss: 2.1380 - mean_absolute_error: 2.1380 - val_loss: 1.8461 - val_mean_absolute_error: 1.8461\n",
      "Epoch 2/100\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 1.9275 - mean_absolute_error: 1.9275Epoch 00002: val_loss improved from 1.84613 to 1.80344, saving model to myRNN_weights.h5\n",
      "60000/60000 [==============================] - 29s 478us/step - loss: 1.9278 - mean_absolute_error: 1.9278 - val_loss: 1.8034 - val_mean_absolute_error: 1.8034\n",
      "Epoch 3/100\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 1.8929 - mean_absolute_error: 1.8929Epoch 00003: val_loss improved from 1.80344 to 1.79633, saving model to myRNN_weights.h5\n",
      "60000/60000 [==============================] - 28s 472us/step - loss: 1.8925 - mean_absolute_error: 1.8925 - val_loss: 1.7963 - val_mean_absolute_error: 1.7963\n",
      "Epoch 4/100\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 1.8816 - mean_absolute_error: 1.8816Epoch 00004: val_loss improved from 1.79633 to 1.78619, saving model to myRNN_weights.h5\n",
      "60000/60000 [==============================] - 33s 547us/step - loss: 1.8812 - mean_absolute_error: 1.8812 - val_loss: 1.7862 - val_mean_absolute_error: 1.7862\n",
      "Epoch 5/100\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 1.8678 - mean_absolute_error: 1.8678Epoch 00005: val_loss improved from 1.78619 to 1.77789, saving model to myRNN_weights.h5\n",
      "60000/60000 [==============================] - 32s 529us/step - loss: 1.8678 - mean_absolute_error: 1.8678 - val_loss: 1.7779 - val_mean_absolute_error: 1.7779\n",
      "Epoch 6/100\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 1.8577 - mean_absolute_error: 1.8577Epoch 00006: val_loss improved from 1.77789 to 1.77057, saving model to myRNN_weights.h5\n",
      "60000/60000 [==============================] - 33s 556us/step - loss: 1.8572 - mean_absolute_error: 1.8572 - val_loss: 1.7706 - val_mean_absolute_error: 1.7706\n",
      "Epoch 7/100\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 1.8495 - mean_absolute_error: 1.8495Epoch 00007: val_loss improved from 1.77057 to 1.77056, saving model to myRNN_weights.h5\n",
      "60000/60000 [==============================] - 33s 548us/step - loss: 1.8495 - mean_absolute_error: 1.8495 - val_loss: 1.7706 - val_mean_absolute_error: 1.7706\n",
      "Epoch 8/100\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 1.8460 - mean_absolute_error: 1.8460Epoch 00008: val_loss improved from 1.77056 to 1.76942, saving model to myRNN_weights.h5\n",
      "60000/60000 [==============================] - 33s 550us/step - loss: 1.8456 - mean_absolute_error: 1.8456 - val_loss: 1.7694 - val_mean_absolute_error: 1.7694\n",
      "Epoch 9/100\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 1.8382 - mean_absolute_error: 1.8382Epoch 00009: val_loss did not improve\n",
      "60000/60000 [==============================] - 32s 537us/step - loss: 1.8383 - mean_absolute_error: 1.8383 - val_loss: 1.7696 - val_mean_absolute_error: 1.7696\n",
      "Epoch 10/100\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 1.8348 - mean_absolute_error: 1.8348Epoch 00010: val_loss improved from 1.76942 to 1.75680, saving model to myRNN_weights.h5\n",
      "60000/60000 [==============================] - 33s 556us/step - loss: 1.8350 - mean_absolute_error: 1.8350 - val_loss: 1.7568 - val_mean_absolute_error: 1.7568\n",
      "Epoch 11/100\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 1.8309 - mean_absolute_error: 1.8309Epoch 00011: val_loss did not improve\n",
      "60000/60000 [==============================] - 33s 542us/step - loss: 1.8311 - mean_absolute_error: 1.8311 - val_loss: 1.7622 - val_mean_absolute_error: 1.7622\n",
      "Epoch 12/100\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 1.8253 - mean_absolute_error: 1.8253Epoch 00012: val_loss did not improve\n",
      "60000/60000 [==============================] - 38s 626us/step - loss: 1.8259 - mean_absolute_error: 1.8259 - val_loss: 1.7620 - val_mean_absolute_error: 1.7620\n",
      "Epoch 13/100\n",
      "59904/60000 [============================>.] - ETA: 4s - loss: 1.8180 - mean_absolute_error: 1.8180  - ETA: 1:59 - loss: 1.8177 - mean_absolute_Epoch 00013: val_loss improved from 1.75680 to 1.74759, saving model to myRNN_weights.h5\n",
      "60000/60000 [==============================] - 2867s 48ms/step - loss: 1.8182 - mean_absolute_error: 1.8182 - val_loss: 1.7476 - val_mean_absolute_error: 1.7476\n",
      "Epoch 14/100\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 1.8142 - mean_absolute_error: 1.8142Epoch 00014: val_loss improved from 1.74759 to 1.74481, saving model to myRNN_weights.h5\n",
      "60000/60000 [==============================] - 20s 340us/step - loss: 1.8140 - mean_absolute_error: 1.8140 - val_loss: 1.7448 - val_mean_absolute_error: 1.7448\n",
      "Epoch 15/100\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 1.8137 - mean_absolute_error: 1.8137Epoch 00015: val_loss did not improve\n",
      "60000/60000 [==============================] - 23s 376us/step - loss: 1.8131 - mean_absolute_error: 1.8131 - val_loss: 1.7470 - val_mean_absolute_error: 1.7470\n",
      "Epoch 16/100\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 1.8040 - mean_absolute_error: 1.8040Epoch 00016: val_loss improved from 1.74481 to 1.74032, saving model to myRNN_weights.h5\n",
      "60000/60000 [==============================] - 22s 365us/step - loss: 1.8042 - mean_absolute_error: 1.8042 - val_loss: 1.7403 - val_mean_absolute_error: 1.7403\n",
      "Epoch 17/100\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 1.8023 - mean_absolute_error: 1.8023Epoch 00017: val_loss improved from 1.74032 to 1.72496, saving model to myRNN_weights.h5\n",
      "60000/60000 [==============================] - 20s 336us/step - loss: 1.8028 - mean_absolute_error: 1.8028 - val_loss: 1.7250 - val_mean_absolute_error: 1.7250\n",
      "Epoch 18/100\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 1.7928 - mean_absolute_error: 1.7928Epoch 00018: val_loss did not improve\n",
      "60000/60000 [==============================] - 21s 343us/step - loss: 1.7931 - mean_absolute_error: 1.7931 - val_loss: 1.7259 - val_mean_absolute_error: 1.7259\n",
      "Epoch 19/100\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 1.7845 - mean_absolute_error: 1.7845Epoch 00019: val_loss improved from 1.72496 to 1.71416, saving model to myRNN_weights.h5\n",
      "60000/60000 [==============================] - 21s 346us/step - loss: 1.7847 - mean_absolute_error: 1.7847 - val_loss: 1.7142 - val_mean_absolute_error: 1.7142\n",
      "Epoch 20/100\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 1.7780 - mean_absolute_error: 1.7780Epoch 00020: val_loss improved from 1.71416 to 1.70373, saving model to myRNN_weights.h5\n",
      "60000/60000 [==============================] - 19s 313us/step - loss: 1.7785 - mean_absolute_error: 1.7785 - val_loss: 1.7037 - val_mean_absolute_error: 1.7037\n",
      "Epoch 21/100\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 1.7567 - mean_absolute_error: 1.7567Epoch 00021: val_loss improved from 1.70373 to 1.67768, saving model to myRNN_weights.h5\n",
      "60000/60000 [==============================] - 23s 384us/step - loss: 1.7568 - mean_absolute_error: 1.7568 - val_loss: 1.6777 - val_mean_absolute_error: 1.6777\n",
      "Epoch 22/100\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 1.7404 - mean_absolute_error: 1.7404Epoch 00022: val_loss improved from 1.67768 to 1.63355, saving model to myRNN_weights.h5\n",
      "60000/60000 [==============================] - 19s 318us/step - loss: 1.7402 - mean_absolute_error: 1.7402 - val_loss: 1.6336 - val_mean_absolute_error: 1.6336\n",
      "Epoch 23/100\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 1.7161 - mean_absolute_error: 1.7161Epoch 00023: val_loss improved from 1.63355 to 1.60601, saving model to myRNN_weights.h5\n",
      "60000/60000 [==============================] - 18s 299us/step - loss: 1.7161 - mean_absolute_error: 1.7161 - val_loss: 1.6060 - val_mean_absolute_error: 1.6060\n",
      "Epoch 24/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59904/60000 [============================>.] - ETA: 0s - loss: 1.6827 - mean_absolute_error: 1.6827Epoch 00024: val_loss improved from 1.60601 to 1.54269, saving model to myRNN_weights.h5\n",
      "60000/60000 [==============================] - 19s 309us/step - loss: 1.6823 - mean_absolute_error: 1.6823 - val_loss: 1.5427 - val_mean_absolute_error: 1.5427\n",
      "Epoch 25/100\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 1.6319 - mean_absolute_error: 1.6319Epoch 00025: val_loss improved from 1.54269 to 1.52027, saving model to myRNN_weights.h5\n",
      "60000/60000 [==============================] - 19s 317us/step - loss: 1.6321 - mean_absolute_error: 1.6321 - val_loss: 1.5203 - val_mean_absolute_error: 1.5203\n",
      "Epoch 26/100\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 1.5893 - mean_absolute_error: 1.5893Epoch 00026: val_loss improved from 1.52027 to 1.43631, saving model to myRNN_weights.h5\n",
      "60000/60000 [==============================] - 19s 320us/step - loss: 1.5889 - mean_absolute_error: 1.5889 - val_loss: 1.4363 - val_mean_absolute_error: 1.4363\n",
      "Epoch 27/100\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 1.5599 - mean_absolute_error: 1.5599Epoch 00027: val_loss improved from 1.43631 to 1.38836, saving model to myRNN_weights.h5\n",
      "60000/60000 [==============================] - 19s 315us/step - loss: 1.5600 - mean_absolute_error: 1.5600 - val_loss: 1.3884 - val_mean_absolute_error: 1.3884\n",
      "Epoch 28/100\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 1.5242 - mean_absolute_error: 1.5242Epoch 00028: val_loss improved from 1.38836 to 1.35093, saving model to myRNN_weights.h5\n",
      "60000/60000 [==============================] - 20s 331us/step - loss: 1.5239 - mean_absolute_error: 1.5239 - val_loss: 1.3509 - val_mean_absolute_error: 1.3509\n",
      "Epoch 29/100\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 1.5037 - mean_absolute_error: 1.5037Epoch 00029: val_loss did not improve\n",
      "60000/60000 [==============================] - 20s 341us/step - loss: 1.5035 - mean_absolute_error: 1.5035 - val_loss: 1.3639 - val_mean_absolute_error: 1.3639\n",
      "Epoch 30/100\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 1.4918 - mean_absolute_error: 1.4918Epoch 00030: val_loss improved from 1.35093 to 1.33173, saving model to myRNN_weights.h5\n",
      "60000/60000 [==============================] - 22s 366us/step - loss: 1.4919 - mean_absolute_error: 1.4919 - val_loss: 1.3317 - val_mean_absolute_error: 1.3317\n",
      "Epoch 31/100\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 1.4680 - mean_absolute_error: 1.4680Epoch 00031: val_loss improved from 1.33173 to 1.26493, saving model to myRNN_weights.h5\n",
      "60000/60000 [==============================] - 23s 378us/step - loss: 1.4683 - mean_absolute_error: 1.4683 - val_loss: 1.2649 - val_mean_absolute_error: 1.2649\n",
      "Epoch 32/100\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 1.4520 - mean_absolute_error: 1.4520Epoch 00032: val_loss did not improve\n",
      "60000/60000 [==============================] - 21s 347us/step - loss: 1.4518 - mean_absolute_error: 1.4518 - val_loss: 1.2941 - val_mean_absolute_error: 1.2941\n",
      "Epoch 33/100\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 1.4424 - mean_absolute_error: 1.4424Epoch 00033: val_loss did not improve\n",
      "60000/60000 [==============================] - 21s 352us/step - loss: 1.4424 - mean_absolute_error: 1.4424 - val_loss: 1.2687 - val_mean_absolute_error: 1.2687\n",
      "Epoch 34/100\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 1.4257 - mean_absolute_error: 1.4257Epoch 00034: val_loss improved from 1.26493 to 1.23113, saving model to myRNN_weights.h5\n",
      "60000/60000 [==============================] - 25s 410us/step - loss: 1.4256 - mean_absolute_error: 1.4256 - val_loss: 1.2311 - val_mean_absolute_error: 1.2311\n",
      "Epoch 35/100\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 1.4207 - mean_absolute_error: 1.4207Epoch 00035: val_loss did not improve\n",
      "60000/60000 [==============================] - 22s 361us/step - loss: 1.4202 - mean_absolute_error: 1.4202 - val_loss: 1.2727 - val_mean_absolute_error: 1.2727\n",
      "Epoch 36/100\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 1.4095 - mean_absolute_error: 1.4095Epoch 00036: val_loss did not improve\n",
      "60000/60000 [==============================] - 21s 344us/step - loss: 1.4094 - mean_absolute_error: 1.4094 - val_loss: 1.2331 - val_mean_absolute_error: 1.2331\n",
      "Epoch 37/100\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 1.3989 - mean_absolute_error: 1.3989Epoch 00037: val_loss improved from 1.23113 to 1.22927, saving model to myRNN_weights.h5\n",
      "60000/60000 [==============================] - 18s 308us/step - loss: 1.3993 - mean_absolute_error: 1.3993 - val_loss: 1.2293 - val_mean_absolute_error: 1.2293\n",
      "Epoch 38/100\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 1.3912 - mean_absolute_error: 1.3912Epoch 00038: val_loss improved from 1.22927 to 1.19332, saving model to myRNN_weights.h5\n",
      "60000/60000 [==============================] - 20s 330us/step - loss: 1.3914 - mean_absolute_error: 1.3914 - val_loss: 1.1933 - val_mean_absolute_error: 1.1933\n",
      "Epoch 39/100\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 1.3869 - mean_absolute_error: 1.3869Epoch 00039: val_loss improved from 1.19332 to 1.18749, saving model to myRNN_weights.h5\n",
      "60000/60000 [==============================] - 19s 317us/step - loss: 1.3872 - mean_absolute_error: 1.3872 - val_loss: 1.1875 - val_mean_absolute_error: 1.1875\n",
      "Epoch 40/100\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 1.3768 - mean_absolute_error: 1.3768Epoch 00040: val_loss did not improve\n",
      "60000/60000 [==============================] - 20s 328us/step - loss: 1.3768 - mean_absolute_error: 1.3768 - val_loss: 1.1894 - val_mean_absolute_error: 1.1894\n",
      "Epoch 41/100\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 1.3698 - mean_absolute_error: 1.3698Epoch 00041: val_loss improved from 1.18749 to 1.17105, saving model to myRNN_weights.h5\n",
      "60000/60000 [==============================] - 20s 335us/step - loss: 1.3698 - mean_absolute_error: 1.3698 - val_loss: 1.1710 - val_mean_absolute_error: 1.1710\n",
      "Epoch 42/100\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 1.3660 - mean_absolute_error: 1.366 - ETA: 0s - loss: 1.3656 - mean_absolute_error: 1.3656Epoch 00042: val_loss did not improve\n",
      "60000/60000 [==============================] - 23s 379us/step - loss: 1.3656 - mean_absolute_error: 1.3656 - val_loss: 1.1722 - val_mean_absolute_error: 1.1722\n",
      "Epoch 43/100\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 1.3577 - mean_absolute_error: 1.3577Epoch 00043: val_loss did not improve\n",
      "60000/60000 [==============================] - 20s 341us/step - loss: 1.3576 - mean_absolute_error: 1.3576 - val_loss: 1.1773 - val_mean_absolute_error: 1.1773\n",
      "Epoch 44/100\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 1.3591 - mean_absolute_error: 1.3591Epoch 00044: val_loss improved from 1.17105 to 1.15700, saving model to myRNN_weights.h5\n",
      "60000/60000 [==============================] - 20s 340us/step - loss: 1.3592 - mean_absolute_error: 1.3592 - val_loss: 1.1570 - val_mean_absolute_error: 1.1570\n",
      "Epoch 45/100\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 1.3463 - mean_absolute_error: 1.3463Epoch 00045: val_loss improved from 1.15700 to 1.14155, saving model to myRNN_weights.h5\n",
      "60000/60000 [==============================] - 21s 346us/step - loss: 1.3466 - mean_absolute_error: 1.3466 - val_loss: 1.1416 - val_mean_absolute_error: 1.1416\n",
      "Epoch 46/100\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 1.3458 - mean_absolute_error: 1.3458Epoch 00046: val_loss did not improve\n",
      "60000/60000 [==============================] - 19s 320us/step - loss: 1.3457 - mean_absolute_error: 1.3457 - val_loss: 1.1487 - val_mean_absolute_error: 1.1487\n",
      "Epoch 47/100\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 1.3336 - mean_absolute_error: 1.3336Epoch 00047: val_loss did not improve\n",
      "60000/60000 [==============================] - 20s 340us/step - loss: 1.3336 - mean_absolute_error: 1.3336 - val_loss: 1.1437 - val_mean_absolute_error: 1.1437\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/100\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 1.3332 - mean_absolute_error: 1.3332- ETA: 3s - loss: 1Epoch 00048: val_loss improved from 1.14155 to 1.11094, saving model to myRNN_weights.h5\n",
      "60000/60000 [==============================] - 23s 390us/step - loss: 1.3335 - mean_absolute_error: 1.3335 - val_loss: 1.1109 - val_mean_absolute_error: 1.1109\n",
      "Epoch 49/100\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 1.3269 - mean_absolute_error: 1.3269Epoch 00049: val_loss did not improve\n",
      "60000/60000 [==============================] - 25s 410us/step - loss: 1.3269 - mean_absolute_error: 1.3269 - val_loss: 1.1496 - val_mean_absolute_error: 1.1496\n",
      "Epoch 50/100\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 1.3270 - mean_absolute_error: 1.3270Epoch 00050: val_loss improved from 1.11094 to 1.09858, saving model to myRNN_weights.h5\n",
      "60000/60000 [==============================] - 25s 414us/step - loss: 1.3268 - mean_absolute_error: 1.3268 - val_loss: 1.0986 - val_mean_absolute_error: 1.0986\n",
      "Epoch 51/100\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 1.3187 - mean_absolute_error: 1.3187Epoch 00051: val_loss did not improve\n",
      "60000/60000 [==============================] - 23s 388us/step - loss: 1.3191 - mean_absolute_error: 1.3191 - val_loss: 1.1098 - val_mean_absolute_error: 1.1098\n",
      "Epoch 52/100\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 1.3186 - mean_absolute_error: 1.3186Epoch 00052: val_loss improved from 1.09858 to 1.09719, saving model to myRNN_weights.h5\n",
      "60000/60000 [==============================] - 21s 354us/step - loss: 1.3187 - mean_absolute_error: 1.3187 - val_loss: 1.0972 - val_mean_absolute_error: 1.0972\n",
      "Epoch 53/100\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 1.3103 - mean_absolute_error: 1.3103Epoch 00053: val_loss did not improve\n",
      "60000/60000 [==============================] - 20s 336us/step - loss: 1.3102 - mean_absolute_error: 1.3102 - val_loss: 1.1761 - val_mean_absolute_error: 1.1761\n",
      "Epoch 54/100\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 1.3116 - mean_absolute_error: 1.3116Epoch 00054: val_loss improved from 1.09719 to 1.08723, saving model to myRNN_weights.h5\n",
      "60000/60000 [==============================] - 20s 336us/step - loss: 1.3115 - mean_absolute_error: 1.3115 - val_loss: 1.0872 - val_mean_absolute_error: 1.0872\n",
      "Epoch 55/100\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 1.3059 - mean_absolute_error: 1.3059Epoch 00055: val_loss did not improve\n",
      "60000/60000 [==============================] - 24s 407us/step - loss: 1.3059 - mean_absolute_error: 1.3059 - val_loss: 1.1798 - val_mean_absolute_error: 1.1798\n",
      "Epoch 56/100\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 1.3035 - mean_absolute_error: 1.3035Epoch 00056: val_loss did not improve\n",
      "60000/60000 [==============================] - 26s 431us/step - loss: 1.3036 - mean_absolute_error: 1.3036 - val_loss: 1.1133 - val_mean_absolute_error: 1.1133\n",
      "Epoch 57/100\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 1.3012 - mean_absolute_error: 1.3012Epoch 00057: val_loss did not improve\n",
      "60000/60000 [==============================] - 27s 442us/step - loss: 1.3012 - mean_absolute_error: 1.3012 - val_loss: 1.0897 - val_mean_absolute_error: 1.0897\n",
      "Epoch 58/100\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 1.2964 - mean_absolute_error: 1.2964Epoch 00058: val_loss improved from 1.08723 to 1.08039, saving model to myRNN_weights.h5\n",
      "60000/60000 [==============================] - 27s 444us/step - loss: 1.2967 - mean_absolute_error: 1.2967 - val_loss: 1.0804 - val_mean_absolute_error: 1.0804\n",
      "Epoch 59/100\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 1.2951 - mean_absolute_error: 1.2951Epoch 00059: val_loss did not improve\n",
      "60000/60000 [==============================] - 27s 446us/step - loss: 1.2950 - mean_absolute_error: 1.2950 - val_loss: 1.0898 - val_mean_absolute_error: 1.0898\n",
      "Epoch 60/100\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 1.2916 - mean_absolute_error: 1.2916Epoch 00060: val_loss did not improve\n",
      "60000/60000 [==============================] - 26s 426us/step - loss: 1.2915 - mean_absolute_error: 1.2915 - val_loss: 1.1040 - val_mean_absolute_error: 1.1040\n",
      "Epoch 61/100\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 1.2840 - mean_absolute_error: 1.2840Epoch 00061: val_loss did not improve\n",
      "60000/60000 [==============================] - 26s 435us/step - loss: 1.2839 - mean_absolute_error: 1.2839 - val_loss: 1.1041 - val_mean_absolute_error: 1.1041\n",
      "Epoch 62/100\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 1.2834 - mean_absolute_error: 1.2834Epoch 00062: val_loss improved from 1.08039 to 1.06427, saving model to myRNN_weights.h5\n",
      "60000/60000 [==============================] - 25s 415us/step - loss: 1.2836 - mean_absolute_error: 1.2836 - val_loss: 1.0643 - val_mean_absolute_error: 1.0643\n",
      "Epoch 63/100\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 1.2881 - mean_absolute_error: 1.2881Epoch 00063: val_loss did not improve\n",
      "60000/60000 [==============================] - 23s 391us/step - loss: 1.2884 - mean_absolute_error: 1.2884 - val_loss: 1.0963 - val_mean_absolute_error: 1.0963\n",
      "Epoch 64/100\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 1.2865 - mean_absolute_error: 1.2865Epoch 00064: val_loss did not improve\n",
      "60000/60000 [==============================] - 27s 444us/step - loss: 1.2864 - mean_absolute_error: 1.2864 - val_loss: 1.0672 - val_mean_absolute_error: 1.0672\n",
      "Epoch 65/100\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 1.2777 - mean_absolute_error: 1.2777- ETA:Epoch 00065: val_loss did not improve\n",
      "60000/60000 [==============================] - 26s 429us/step - loss: 1.2780 - mean_absolute_error: 1.2780 - val_loss: 1.1416 - val_mean_absolute_error: 1.1416\n",
      "Epoch 66/100\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 1.2796 - mean_absolute_error: 1.2796- ETA: 0s - loss: 1.2796 - mean_absolute_error: 1.279Epoch 00066: val_loss improved from 1.06427 to 1.05147, saving model to myRNN_weights.h5\n",
      "60000/60000 [==============================] - 24s 399us/step - loss: 1.2797 - mean_absolute_error: 1.2797 - val_loss: 1.0515 - val_mean_absolute_error: 1.0515\n",
      "Epoch 67/100\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 1.2739 - mean_absolute_error: 1.2739Epoch 00067: val_loss did not improve\n",
      "60000/60000 [==============================] - 26s 427us/step - loss: 1.2740 - mean_absolute_error: 1.2740 - val_loss: 1.0568 - val_mean_absolute_error: 1.0568\n",
      "Epoch 68/100\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 1.2808 - mean_absolute_error: 1.2808Epoch 00068: val_loss did not improve\n",
      "60000/60000 [==============================] - 26s 428us/step - loss: 1.2809 - mean_absolute_error: 1.2809 - val_loss: 1.0650 - val_mean_absolute_error: 1.0650\n",
      "Epoch 69/100\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 1.2723 - mean_absolute_error: 1.2723Epoch 00069: val_loss did not improve\n",
      "60000/60000 [==============================] - 24s 401us/step - loss: 1.2725 - mean_absolute_error: 1.2725 - val_loss: 1.1006 - val_mean_absolute_error: 1.1006\n",
      "Epoch 70/100\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 1.2729 - mean_absolute_error: 1.2729Epoch 00070: val_loss improved from 1.05147 to 1.05126, saving model to myRNN_weights.h5\n",
      "60000/60000 [==============================] - 26s 427us/step - loss: 1.2731 - mean_absolute_error: 1.2731 - val_loss: 1.0513 - val_mean_absolute_error: 1.0513\n",
      "Epoch 71/100\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 1.2592 - mean_absolute_error: 1.2592Epoch 00071: val_loss did not improve\n",
      "60000/60000 [==============================] - 27s 448us/step - loss: 1.2588 - mean_absolute_error: 1.2588 - val_loss: 1.0536 - val_mean_absolute_error: 1.0536\n",
      "Epoch 72/100\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 1.2673 - mean_absolute_error: 1.2673Epoch 00072: val_loss did not improve\n",
      "60000/60000 [==============================] - 24s 405us/step - loss: 1.2675 - mean_absolute_error: 1.2675 - val_loss: 1.0825 - val_mean_absolute_error: 1.0825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/100\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 1.2602 - mean_absolute_error: 1.2602Epoch 00073: val_loss did not improve\n",
      "60000/60000 [==============================] - 22s 362us/step - loss: 1.2596 - mean_absolute_error: 1.2596 - val_loss: 1.0628 - val_mean_absolute_error: 1.0628\n",
      "Epoch 74/100\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 1.2636 - mean_absolute_error: 1.2636Epoch 00074: val_loss improved from 1.05126 to 1.04001, saving model to myRNN_weights.h5\n",
      "60000/60000 [==============================] - 19s 325us/step - loss: 1.2638 - mean_absolute_error: 1.2638 - val_loss: 1.0400 - val_mean_absolute_error: 1.0400\n",
      "Epoch 75/100\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 1.2653 - mean_absolute_error: 1.2653Epoch 00075: val_loss did not improve\n",
      "60000/60000 [==============================] - 22s 367us/step - loss: 1.2651 - mean_absolute_error: 1.2651 - val_loss: 1.0481 - val_mean_absolute_error: 1.0481\n",
      "Epoch 76/100\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 1.2595 - mean_absolute_error: 1.2595Epoch 00076: val_loss did not improve\n",
      "60000/60000 [==============================] - 21s 356us/step - loss: 1.2594 - mean_absolute_error: 1.2594 - val_loss: 1.0941 - val_mean_absolute_error: 1.0941\n",
      "Epoch 77/100\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 1.2562 - mean_absolute_error: 1.2562Epoch 00077: val_loss did not improve\n",
      "60000/60000 [==============================] - 19s 323us/step - loss: 1.2560 - mean_absolute_error: 1.2560 - val_loss: 1.0671 - val_mean_absolute_error: 1.0671\n",
      "Epoch 78/100\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 1.2498 - mean_absolute_error: 1.2498Epoch 00078: val_loss improved from 1.04001 to 1.03003, saving model to myRNN_weights.h5\n",
      "60000/60000 [==============================] - 22s 361us/step - loss: 1.2498 - mean_absolute_error: 1.2498 - val_loss: 1.0300 - val_mean_absolute_error: 1.0300\n",
      "Epoch 79/100\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 1.2438 - mean_absolute_error: 1.2438Epoch 00079: val_loss did not improve\n",
      "60000/60000 [==============================] - 22s 359us/step - loss: 1.2436 - mean_absolute_error: 1.2436 - val_loss: 1.0354 - val_mean_absolute_error: 1.0354\n",
      "Epoch 80/100\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 1.2552 - mean_absolute_error: 1.2552Epoch 00080: val_loss did not improve\n",
      "60000/60000 [==============================] - 20s 330us/step - loss: 1.2551 - mean_absolute_error: 1.2551 - val_loss: 1.0445 - val_mean_absolute_error: 1.0445\n",
      "Epoch 81/100\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 1.2508 - mean_absolute_error: 1.2508- ETA: 3s - lEpoch 00081: val_loss did not improve\n",
      "60000/60000 [==============================] - 22s 369us/step - loss: 1.2510 - mean_absolute_error: 1.2510 - val_loss: 1.0305 - val_mean_absolute_error: 1.0305\n",
      "Epoch 82/100\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 1.2430 - mean_absolute_error: 1.2430Epoch 00082: val_loss improved from 1.03003 to 1.02327, saving model to myRNN_weights.h5\n",
      "60000/60000 [==============================] - 21s 344us/step - loss: 1.2429 - mean_absolute_error: 1.2429 - val_loss: 1.0233 - val_mean_absolute_error: 1.0233\n",
      "Epoch 83/100\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 1.2474 - mean_absolute_error: 1.2474Epoch 00083: val_loss did not improve\n",
      "60000/60000 [==============================] - 22s 366us/step - loss: 1.2475 - mean_absolute_error: 1.2475 - val_loss: 1.0506 - val_mean_absolute_error: 1.0506\n",
      "Epoch 84/100\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 1.2388 - mean_absolute_error: 1.2388Epoch 00084: val_loss did not improve\n",
      "60000/60000 [==============================] - 21s 351us/step - loss: 1.2389 - mean_absolute_error: 1.2389 - val_loss: 1.0341 - val_mean_absolute_error: 1.0341\n",
      "Epoch 85/100\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 1.2424 - mean_absolute_error: 1.2424Epoch 00085: val_loss did not improve\n",
      "60000/60000 [==============================] - 20s 325us/step - loss: 1.2424 - mean_absolute_error: 1.2424 - val_loss: 1.0300 - val_mean_absolute_error: 1.0300\n",
      "Epoch 86/100\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 1.2378 - mean_absolute_error: 1.2378- ETA: 3s -Epoch 00086: val_loss improved from 1.02327 to 1.02149, saving model to myRNN_weights.h5\n",
      "60000/60000 [==============================] - 23s 377us/step - loss: 1.2376 - mean_absolute_error: 1.2376 - val_loss: 1.0215 - val_mean_absolute_error: 1.0215\n",
      "Epoch 87/100\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 1.2461 - mean_absolute_error: 1.2461Epoch 00087: val_loss did not improve\n",
      "60000/60000 [==============================] - 22s 360us/step - loss: 1.2461 - mean_absolute_error: 1.2461 - val_loss: 1.0425 - val_mean_absolute_error: 1.0425\n",
      "Epoch 88/100\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 1.2384 - mean_absolute_error: 1.2384Epoch 00088: val_loss did not improve\n",
      "60000/60000 [==============================] - 20s 331us/step - loss: 1.2385 - mean_absolute_error: 1.2385 - val_loss: 1.0335 - val_mean_absolute_error: 1.0335\n",
      "Epoch 89/100\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 1.2401 - mean_absolute_error: 1.2401Epoch 00089: val_loss did not improve\n",
      "60000/60000 [==============================] - 22s 371us/step - loss: 1.2403 - mean_absolute_error: 1.2403 - val_loss: 1.0319 - val_mean_absolute_error: 1.0319\n",
      "Epoch 90/100\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 1.2321 - mean_absolute_error: 1.2321Epoch 00090: val_loss improved from 1.02149 to 1.01519, saving model to myRNN_weights.h5\n",
      "60000/60000 [==============================] - 21s 343us/step - loss: 1.2320 - mean_absolute_error: 1.2320 - val_loss: 1.0152 - val_mean_absolute_error: 1.0152\n",
      "Epoch 91/100\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 1.2343 - mean_absolute_error: 1.2343Epoch 00091: val_loss improved from 1.01519 to 1.00773, saving model to myRNN_weights.h5\n",
      "60000/60000 [==============================] - 21s 347us/step - loss: 1.2339 - mean_absolute_error: 1.2339 - val_loss: 1.0077 - val_mean_absolute_error: 1.0077\n",
      "Epoch 92/100\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 1.2382 - mean_absolute_error: 1.2382- ETA: 1s - loss: 1.2379 - mean_abEpoch 00092: val_loss improved from 1.00773 to 1.00711, saving model to myRNN_weights.h5\n",
      "60000/60000 [==============================] - 23s 380us/step - loss: 1.2381 - mean_absolute_error: 1.2381 - val_loss: 1.0071 - val_mean_absolute_error: 1.0071\n",
      "Epoch 93/100\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 1.2274 - mean_absolute_error: 1.2274Epoch 00093: val_loss did not improve\n",
      "60000/60000 [==============================] - 20s 334us/step - loss: 1.2280 - mean_absolute_error: 1.2280 - val_loss: 1.0561 - val_mean_absolute_error: 1.0561\n",
      "Epoch 94/100\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 1.2347 - mean_absolute_error: 1.2347Epoch 00094: val_loss did not improve\n",
      "60000/60000 [==============================] - 21s 353us/step - loss: 1.2347 - mean_absolute_error: 1.2347 - val_loss: 1.0481 - val_mean_absolute_error: 1.0481\n",
      "Epoch 95/100\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 1.2252 - mean_absolute_error: 1.2252Epoch 00095: val_loss did not improve\n",
      "60000/60000 [==============================] - 23s 376us/step - loss: 1.2250 - mean_absolute_error: 1.2250 - val_loss: 1.0179 - val_mean_absolute_error: 1.0179\n",
      "Epoch 96/100\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 1.2332 - mean_absolute_error: 1.2332Epoch 00096: val_loss did not improve\n",
      "60000/60000 [==============================] - 20s 337us/step - loss: 1.2334 - mean_absolute_error: 1.2334 - val_loss: 1.0228 - val_mean_absolute_error: 1.0228\n",
      "Epoch 97/100\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 1.2313 - mean_absolute_error: 1.2313Epoch 00097: val_loss did not improve\n",
      "60000/60000 [==============================] - 23s 378us/step - loss: 1.2308 - mean_absolute_error: 1.2308 - val_loss: 1.0330 - val_mean_absolute_error: 1.0330\n",
      "Epoch 98/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59904/60000 [============================>.] - ETA: 0s - loss: 1.2227 - mean_absolute_error: 1.2227Epoch 00098: val_loss improved from 1.00711 to 1.00528, saving model to myRNN_weights.h5\n",
      "60000/60000 [==============================] - 20s 339us/step - loss: 1.2226 - mean_absolute_error: 1.2226 - val_loss: 1.0053 - val_mean_absolute_error: 1.0053\n",
      "Epoch 99/100\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 1.2212 - mean_absolute_error: 1.2212Epoch 00099: val_loss improved from 1.00528 to 1.00211, saving model to myRNN_weights.h5\n",
      "60000/60000 [==============================] - 23s 386us/step - loss: 1.2213 - mean_absolute_error: 1.2213 - val_loss: 1.0021 - val_mean_absolute_error: 1.0021\n",
      "Epoch 100/100\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 1.2224 - mean_absolute_error: 1.2224Epoch 00100: val_loss did not improve\n",
      "60000/60000 [==============================] - 21s 354us/step - loss: 1.2225 - mean_absolute_error: 1.2225 - val_loss: 1.0157 - val_mean_absolute_error: 1.0157\n"
     ]
    }
   ],
   "source": [
    "nEpochs = 100\n",
    "\n",
    "myRNN_hist = myRNN.fit(X_train, y_train, epochs=nEpochs, batch_size=256,validation_split=0.25,\n",
    "                 callbacks=[earlyStop, myRNN_mChkPt],) # callbacks=[earlyStop, myRNN_mChkPt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1db34699128>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = np.arange(1,len(myRNN_hist.history['loss'])+1)\n",
    "\n",
    "plt.plot(epochs,myRNN_hist.history['loss'],label='training')\n",
    "plt.plot(epochs,myRNN_hist.history['val_loss'],label='validation')\n",
    "plt.xlabel('epochs',fontsize=14)\n",
    "plt.ylabel('MSE loss',fontsize=14)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "results=myRNN.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.24140129e+00, -8.88192000e-01,  9.94241735e-01,\n",
       "        -3.48758960e+00, -2.26321569e+00,  2.46167972e+00],\n",
       "       [-8.51117036e+00, -1.26717577e+00, -6.36053884e+00,\n",
       "        -1.91485971e+01, -2.83072612e+00, -1.51046790e+01],\n",
       "       [-1.36862621e-02, -4.78984242e-02, -2.67514389e-01,\n",
       "        -4.80987654e-02, -1.49361153e-01, -6.03017945e-01],\n",
       "       ...,\n",
       "       [-1.60835268e-01, -1.19740543e-02, -1.07830609e-01,\n",
       "        -1.10618069e+00, -1.17571043e-01, -7.76054401e-01],\n",
       "       [-7.25770497e-02, -3.50309500e-02, -2.60381413e-01,\n",
       "        -3.58773002e+00, -1.45284391e+00, -1.34700448e+01],\n",
       "       [ 4.17528040e+00, -1.00886561e+00,  2.89703501e+00,\n",
       "         5.59305105e+00, -1.35187053e+00,  4.07725993e+00]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.41204707,  0.58094349,  0.37507779, -0.02337146,  0.21841183,\n",
       "         0.05194956],\n",
       "       [-0.56747293, -0.43625976, -0.47986848, -0.66950336, -0.54154851,\n",
       "        -0.6110813 ],\n",
       "       [ 1.09298498,  0.11684774,  0.93079942,  0.84642937, -0.06236174,\n",
       "         1.25966563],\n",
       "       ...,\n",
       "       [ 3.53251925,  4.77539957,  3.57591692,  0.6798615 ,  0.63707265,\n",
       "         0.71351295],\n",
       "       [14.09553344, 11.46632475, 10.55105835, -0.43284465, -0.4594923 ,\n",
       "        -0.57675777],\n",
       "       [-0.398921  , -0.41353872, -0.44652154, -0.15028829, -0.1378671 ,\n",
       "        -0.20613938]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(results-y_test)/y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "jet_energy                                               43075.4\n",
       "jet_flavour                                                    5\n",
       "secVtx_x                                              0.00417528\n",
       "secVtx_y                                             -0.00100887\n",
       "secVtx_z                                              0.00289704\n",
       "terVtx_x                                              0.00559305\n",
       "terVtx_y                                             -0.00135187\n",
       "terVtx_z                                              0.00407726\n",
       "tracks         [[2.010621651929816e-06, -4.989406079403125e-0...\n",
       "Name: 99999, dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bjets_DF.iloc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at worst perfoming cases and examine the tracks\n",
    "# look at actual uncertainty on overlap for the vertex\n",
    "# re-run with minimised errors on tracks see if performance is better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.05486715, -0.45196092, -2.455217  , -0.1861686 , -0.99255735,\n",
       "        -5.3714914 ]], dtype=float32)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRNN.predict(np.array([X_test[2]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "couple of things, first the RNN looks like it predicts the same value for all cases. This might be because it uses the null tracks at the end of every single jet and just predicts off of those.\n",
    "Maybe should consider predicting values that are at least order unity, because mse is gonna be very small otherwise, so convert the vertices into different units (I dunno maybe millimeters or microns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
