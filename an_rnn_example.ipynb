{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import math\n",
    "import copy\n",
    "import pickle\n",
    "from keras.models import Sequential, Model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from keras.layers import BatchNormalization, Layer, TimeDistributed, Dropout\n",
    "from keras.layers import Dense, Input, Masking, LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by trying to load in our toy jets, and then separate into features (to train on) and labels (to predict).\n",
    "\n",
    "For features, first try d0, z0, phi, theta, qOverP, (refPx, refPy, refPz)?. Essentially the track parameters.\n",
    "\n",
    "For labels, Xs, Ys, Zs, Xt, Yt, Zt. That is the secondary and tertiary vertices. Omit the primary as this has been fixed to (0,0,0). This will require some smart selection for c and light jets, where not all vertices are present. If a vertex is not present, could try predicting (0,0,0) or (-1,-1,-1) or previous vertex (prim or sec)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bjets_DF = pd.read_pickle(\"./bjets_NaN_and_oneOverP_errs.pkl\")\n",
    "#cjets_DF = pd.read_pickle(\"./cjets.pkl\")\n",
    "#ljets_DF = pd.read_pickle(\"./ljets.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the track parameters as our features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 15.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trks=np.zeros((len(bjets_DF), 30, 5))\n",
    "\n",
    "for i in range(len(bjets_DF)):\n",
    "    trks[i] = np.array([bjets_DF['tracks'][i]])[:,:,0:5]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = trks # following convention name the features as the vector 'X'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = np.nan_to_num(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Handling: Change 1/p to p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try changing the 1/p feature to p, see what happens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " No Change to end accuracy nor speed-up so won't do this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4850.46696803,  3060.74292196,  4054.83541082, ...,\n",
       "                   nan,            nan,            nan],\n",
       "       [ 2050.96833157,  5200.69668043,  1918.47163884, ...,\n",
       "                   nan,            nan,            nan],\n",
       "       [ 3774.24046227, 14472.05631949,  2894.37128725, ...,\n",
       "                   nan,            nan,            nan],\n",
       "       ...,\n",
       "       [ 2883.77907606,  5114.71929641,  2072.69149129, ...,\n",
       "                   nan,            nan,            nan],\n",
       "       [ 2566.318433  ,   620.46562403,   563.21545658, ...,\n",
       "                   nan,            nan,            nan],\n",
       "       [ 4284.22553045,  7638.42365794,  1493.57056224, ...,\n",
       "                   nan,            nan,            nan]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1/X[:,:,4]  # these are individual tracks remember so p<10,000 is allowed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X[:,:,4] = 1/X[:,:,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = np.nan_to_num(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Track Ordering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use numpy sort function should work, order in descending value say of impact parameter or something (shouldnt be any negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(X[:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4.37 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# This code should do the trick, ordering here by d0 in descinding order,\n",
    "# checked that there are no d0 negative vals, so i don't take absolute values\n",
    "\n",
    "for jet in range(len(bjets_DF)):\n",
    "    mat=np.nan_to_num(X[jet]) # Converts nan to 0.0 !!! May need to run this step later if want to minmax scale\n",
    "    mat_sort = mat[mat[:,0].argsort()[::-1]]\n",
    "    X[jet]=mat_sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 7.48773333e-04, -9.65963118e-05, -2.11493938e+00,\n",
       "          2.93390487e+00,  1.62024732e-04],\n",
       "        [ 4.67363941e-04,  2.17068009e-05, -1.77145540e+00,\n",
       "          3.09125710e+00,  7.33671864e-05],\n",
       "        [ 4.35472118e-04, -1.23074278e-05, -1.04056037e+00,\n",
       "          2.98124820e+00,  1.58138612e-04],\n",
       "        ...,\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00]],\n",
       "\n",
       "       [[ 1.95433368e-03, -2.58651399e-03,  6.42807661e-01,\n",
       "          1.04267568e+00,  2.32396558e-04],\n",
       "        [ 1.93720514e-03,  1.43839722e-03,  5.82598525e-01,\n",
       "          1.40702647e+00,  1.48369940e-04],\n",
       "        [ 7.81975509e-04,  1.57580711e-04,  8.11333647e-01,\n",
       "          1.30535613e+00,  4.47963290e-05],\n",
       "        ...,\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00]],\n",
       "\n",
       "       [[ 7.92387572e-04,  2.42781360e-04,  7.29107086e-01,\n",
       "          2.97427249e-01,  3.45498176e-04],\n",
       "        [ 7.87020778e-04,  2.25221971e-04,  3.10944700e+00,\n",
       "          2.78857555e-01,  1.02178690e-03],\n",
       "        [ 6.55551969e-05,  1.05984509e-06, -2.48764440e+00,\n",
       "          1.73680307e-02,  2.26673348e-05],\n",
       "        ...,\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 9.95209125e-04,  8.60995613e-04,  1.40893737e+00,\n",
       "          2.42435778e+00,  1.02252208e-04],\n",
       "        [ 7.93154173e-04, -6.37480058e-04,  1.45604682e+00,\n",
       "          2.31399696e+00,  6.02234664e-05],\n",
       "        [ 5.07041824e-04,  3.32136638e-04,  1.39735028e+00,\n",
       "          2.38328797e+00,  7.60725043e-05],\n",
       "        ...,\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00]],\n",
       "\n",
       "       [[ 2.37889742e-03,  1.60526135e-03, -1.31018586e+00,\n",
       "          1.07768110e+00,  8.79707683e-04],\n",
       "        [ 8.37746294e-04,  1.15534547e-03, -2.28380308e+00,\n",
       "          1.25740524e+00,  2.93613839e-04],\n",
       "        [ 4.86087987e-04, -3.35207675e-04, -2.00347956e+00,\n",
       "          6.84392273e-01,  1.22923228e-04],\n",
       "        ...,\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00]],\n",
       "\n",
       "       [[ 1.11233585e-03, -1.06636097e-03,  2.81803219e-01,\n",
       "          7.81472580e-01,  1.83810025e-03],\n",
       "        [ 4.62435196e-04, -1.85796758e-04,  3.57198762e-01,\n",
       "          1.26751011e+00,  3.07055390e-04],\n",
       "        [ 1.24872255e-04,  2.03627511e-04,  1.30879477e-01,\n",
       "          1.42412917e+00,  6.50138354e-05],\n",
       "        ...,\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00]]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think we want to ensure all paramters are approx unity in order not to bias the RNN towards a particular feature. phi and theta are already approx unity (order pi), but qOverP is very small order 1e-4 and IP are also small (but scale multiple orders of magnitude) 1e-6 to 1e-2 (unfortunately)\n",
    "\n",
    "So maybe want to use a min max scaler or some thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([4.13328e+05, 3.94200e+04, 2.76730e+04, ..., 0.00000e+00,\n",
       "        0.00000e+00, 1.00000e+00]),\n",
       " array([1.44259736e-09, 2.48700174e-05, 4.97385922e-05, ...,\n",
       "        2.58135821e-02, 2.58384507e-02, 2.58633192e-02]),\n",
       " <a list of 1040 Patch objects>)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAD4CAYAAADy46FuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAX+ElEQVR4nO3df4yd1Z3f8fdnMdtFTSA2GOS1rZoGr1RYqSRcGaRUVRq2tpVd1UQiWlfq4j8sOYuIlEhbVbD9gyz8E6pNqJAKEikIQ9OARbLCyoZSL2QVVSKGcUoAQ1jPFhocLOzVeAn5h9bk2z/umeZ6Mhxfz3hm7OH9kq7uc7/Pc848J0/w5z4/Zk6qCkmSPshvLPUOSJLObgaFJKnLoJAkdRkUkqQug0KS1LViqXfgTLvkkktqw4YNS70bknROOXDgwN9V1erZ1i27oNiwYQMTExNLvRuSdE5J8r8/aJ2XniRJXQaFJKnLoJAkdRkUkqQug0KS1GVQSJK6DApJUpdBIUnqMigkSV0GhSSpy6CQJHUZFJKkrrGDIsl5Sf5nku+2z6uS7EtyqL2vHNn2tiSTSV5LsmWkfk2Sl9q6e5Kk1f9BksdafX+SDSNtdrSfcSjJjjMxaEnS+E7njOJLwKsjn28Fnq6qjcDT7TNJrgS2A1cBW4F7k5zX2twH7AI2ttfWVt8JHK+qK4C7gbtaX6uA24FrgU3A7aOBJElaeGMFRZJ1wO8D/3mkvA3Y3ZZ3AzeM1B+tqveq6nVgEtiUZA1wYVU9W1UFPDyjzXRfjwPXt7ONLcC+qpqqquPAPn4VLpKkRTDuGcV/BP4d8MuR2mVVdQSgvV/a6muBN0e2O9xqa9vyzPpJbarqBPAOcHGnr5Mk2ZVkIsnEsWPHxhySJGkcpwyKJH8AHK2qA2P2mVlq1anPtc2vClX3V9WgqgarV886QZMkaY7GOaP4FPCvkrwBPAp8Jsl/Ad5ul5No70fb9oeB9SPt1wFvtfq6WeontUmyArgImOr0JUlaJKcMiqq6rarWVdUGhjepn6mqfwPsBaafQtoBPNGW9wLb25NMlzO8af1cuzz1bpLr2v2Hm2a0me7rxvYzCngK2JxkZbuJvbnVJEmLZD5zZn8V2JNkJ/BT4PMAVXUwyR7gFeAEcEtVvd/a3Aw8BFwAPNleAA8AjySZZHgmsb31NZXkTuD5tt0dVTU1j32WJJ2mDL+4Lx+DwaAmJiaWejck6ZyS5EBVDWZb529mS5K6DApJUpdBIUnqMigkSV0GhSSpy6CQJHUZFJKkLoNCktRlUEiSugwKSVKXQSFJ6jIoJEldBoUkqcugkCR1GRSSpC6DQpLUdcqgSPJbSZ5L8uMkB5P8Wat/JcnPkrzQXp8daXNbkskkryXZMlK/JslLbd09bUpU2rSpj7X6/iQbRtrsSHKovXYgSVpU40yF+h7wmar6RZLzgf+RZHoK07ur6s9HN05yJcOpTK8Cfhv4qyS/06ZDvQ/YBfwQ+B6wleF0qDuB41V1RZLtwF3AHyZZBdwODIACDiTZW1XH5zdsSdK4TnlGUUO/aB/Pb6/e/KnbgEer6r2qeh2YBDYlWQNcWFXP1nD+1YeBG0ba7G7LjwPXt7ONLcC+qppq4bCPYbhIkhbJWPcokpyX5AXgKMN/uPe3VV9M8mKSB5OsbLW1wJsjzQ+32tq2PLN+UpuqOgG8A1zc6Wvm/u1KMpFk4tixY+MMSZI0prGCoqrer6qrgXUMzw5+l+FlpI8DVwNHgK+1zTNbF536XNuM7t/9VTWoqsHq1au7Y5EknZ7Teuqpqv4e+Gtga1W93QLkl8A3gE1ts8PA+pFm64C3Wn3dLPWT2iRZAVwETHX6kiQtknGeelqd5GNt+QLg94CftHsO0z4HvNyW9wLb25NMlwMbgeeq6gjwbpLr2v2Hm4AnRtpMP9F0I/BMu4/xFLA5ycp2aWtzq0mSFsk4Tz2tAXYnOY9hsOypqu8meSTJ1QwvBb0BfAGgqg4m2QO8ApwAbmlPPAHcDDwEXMDwaafpp6ceAB5JMsnwTGJ762sqyZ3A8227O6pqah7jlSSdpgy/uC8fg8GgJiYmlno3JOmckuRAVQ1mW+dvZkuSugwKSVKXQSFJ6jIoJEldBoUkqcugkCR1GRSSpC6DQpLUZVBIkroMCklSl0EhSeoyKCRJXQaFJKnLoJAkdRkUkqQug0KS1DXOVKi/leS5JD9OcjDJn7X6qiT7khxq7ytH2tyWZDLJa0m2jNSvSfJSW3dPmxKVNm3qY62+P8mGkTY72s84lGQHkqRFNc4ZxXvAZ6rqnwJXA1uTXAfcCjxdVRuBp9tnklzJcCrTq4CtwL1tGlWA+4BdDOfR3tjWA+wEjlfVFcDdwF2tr1XA7cC1wCbg9tFAkiQtvFMGRQ39on08v70K2AbsbvXdwA1teRvwaFW9V1WvA5PApiRrgAur6tkazr/68Iw20309Dlzfzja2APuqaqqqjgP7+FW4SJIWwVj3KJKcl+QF4CjDf7j3A5dV1RGA9n5p23wt8OZI88OttrYtz6yf1KaqTgDvABd3+pq5f7uSTCSZOHbs2DhDkiSNaaygqKr3q+pqYB3Ds4Pf7Wye2bro1OfaZnT/7q+qQVUNVq9e3dk1SdLpOq2nnqrq74G/Znj55+12OYn2frRtdhhYP9JsHfBWq6+bpX5SmyQrgIuAqU5fkqRFMs5TT6uTfKwtXwD8HvATYC8w/RTSDuCJtrwX2N6eZLqc4U3r59rlqXeTXNfuP9w0o810XzcCz7T7GE8Bm5OsbDexN7eaJGmRrBhjmzXA7vbk0m8Ae6rqu0meBfYk2Qn8FPg8QFUdTLIHeAU4AdxSVe+3vm4GHgIuAJ5sL4AHgEeSTDI8k9je+ppKcifwfNvujqqams+AJUmnJ8Mv7svHYDCoiYmJpd4NSTqnJDlQVYPZ1vmb2ZKkLoNCktRlUEiSugwKSVKXQSFJ6jIoJEldBoUkqcugkCR1GRSSpC6DQpLUZVBIkroMCklSl0EhSeoyKCRJXQaFJKnLoJAkdY0zFer6JN9P8mqSg0m+1OpfSfKzJC+012dH2tyWZDLJa0m2jNSvSfJSW3dPmxKVNm3qY62+P8mGkTY7khxqrx1IkhbVOFOhngD+pKp+lOSjwIEk+9q6u6vqz0c3TnIlw6lMrwJ+G/irJL/TpkO9D9gF/BD4HrCV4XSoO4HjVXVFku3AXcAfJlkF3A4MgGo/e29VHZ/fsCVJ4zrlGUVVHamqH7Xld4FXgbWdJtuAR6vqvap6HZgENiVZA1xYVc/WcP7Vh4EbRtrsbsuPA9e3s40twL6qmmrhsI9huEiSFslp3aNol4Q+AexvpS8meTHJg0lWttpa4M2RZodbbW1bnlk/qU1VnQDeAS7u9CVJWiRjB0WSjwDfBr5cVT9neBnp48DVwBHga9ObztK8OvW5thndt11JJpJMHDt2rDsOSdLpGSsokpzPMCS+WVXfAaiqt6vq/ar6JfANYFPb/DCwfqT5OuCtVl83S/2kNklWABcBU52+TlJV91fVoKoGq1evHmdIkqQxjfPUU4AHgFer6usj9TUjm30OeLkt7wW2tyeZLgc2As9V1RHg3STXtT5vAp4YaTP9RNONwDPtPsZTwOYkK9ulrc2tJklaJOM89fQp4I+Al5K80Gp/CvzrJFczvBT0BvAFgKo6mGQP8ArDJ6ZuaU88AdwMPARcwPBppydb/QHgkSSTDM8ktre+ppLcCTzftrujqqbmNlRJ0lxk+MV9+RgMBjUxMbHUuyFJ55QkB6pqMNs6fzNbktRlUEiSugwKSVKXQSFJ6jIoJEldBoUkqcugkCR1GRSSpC6DQpLUZVBIkroMCklSl0EhSeoyKCRJXQaFJKnLoJAkdRkUkqSucaZCXZ/k+0leTXIwyZdafVWSfUkOtfeVI21uSzKZ5LUkW0bq1yR5qa27p02JSps29bFW359kw0ibHe1nHEqyA0nSohrnjOIE8CdV9U+A64BbklwJ3Ao8XVUbgafbZ9q67cBVwFbg3iTntb7uA3YxnEd7Y1sPsBM4XlVXAHcDd7W+VgG3A9cCm4DbRwNJkrTwThkUVXWkqn7Ult8FXgXWAtuA3W2z3cANbXkb8GhVvVdVrwOTwKYka4ALq+rZGs6/+vCMNtN9PQ5c3842tgD7qmqqqo4D+/hVuEiSFsFp3aNol4Q+AewHLquqIzAME+DSttla4M2RZodbbW1bnlk/qU1VnQDeAS7u9CVJWiRjB0WSjwDfBr5cVT/vbTpLrTr1ubYZ3bddSSaSTBw7dqyza5Kk0zVWUCQ5n2FIfLOqvtPKb7fLSbT3o61+GFg/0nwd8Farr5ulflKbJCuAi4CpTl8nqar7q2pQVYPVq1ePMyRJ0pjGeeopwAPAq1X19ZFVe4Hpp5B2AE+M1Le3J5kuZ3jT+rl2eerdJNe1Pm+a0Wa6rxuBZ9p9jKeAzUlWtpvYm1tNkrRIVoyxzaeAPwJeSvJCq/0p8FVgT5KdwE+BzwNU1cEke4BXGD4xdUtVvd/a3Qw8BFwAPNleMAyiR5JMMjyT2N76mkpyJ/B82+6Oqpqa41glSXOQ4Rf35WMwGNTExMRS74YknVOSHKiqwWzr/M1sSVKXQSFJ6jIoJEldBoUkqcugkCR1GRSSpC6DQpLUZVBIkroMCklSl0EhSeoyKCRJXQaFJKnLoJAkdRkUkqQug0KS1GVQSJK6xpkK9cEkR5O8PFL7SpKfJXmhvT47su62JJNJXkuyZaR+TZKX2rp72nSotClTH2v1/Uk2jLTZkeRQe01PlSpJWkTjnFE8BGydpX53VV3dXt8DSHIlw2lMr2pt7k1yXtv+PmAXwzm0N470uRM4XlVXAHcDd7W+VgG3A9cCm4Db27zZkqRFdMqgqKofMJzHehzbgEer6r2qeh2YBDYlWQNcWFXP1nDu1YeBG0ba7G7LjwPXt7ONLcC+qpqqquPAPmYPLEnSAprPPYovJnmxXZqa/qa/FnhzZJvDrba2Lc+sn9Smqk4A7wAXd/qSJC2iuQbFfcDHgauBI8DXWj2zbFud+lzbnCTJriQTSSaOHTvW229J0mmaU1BU1dtV9X5V/RL4BsN7CDD81r9+ZNN1wFutvm6W+kltkqwALmJ4qeuD+pptf+6vqkFVDVavXj2XIUmSPsCcgqLdc5j2OWD6iai9wPb2JNPlDG9aP1dVR4B3k1zX7j/cBDwx0mb6iaYbgWfafYyngM1JVrZLW5tbTZK0iFacaoMk3wI+DVyS5DDDJ5E+neRqhpeC3gC+AFBVB5PsAV4BTgC3VNX7raubGT5BdQHwZHsBPAA8kmSS4ZnE9tbXVJI7gefbdndU1bg31SVJZ0iGX96Xj8FgUBMTE0u9G5J0TklyoKoGs63zN7MlSV0GhSSpy6CQJHUZFJKkLoNCktRlUEiSugwKSVKXQSFJ6jIoJEldBoUkqcugkCR1GRSSpC6DQpLUZVBIkroMCklSl0EhSeoyKCRJXacMiiQPJjma5OWR2qok+5Icau8rR9bdlmQyyWtJtozUr0nyUlt3T5s7mza/9mOtvj/JhpE2O9rPOJRkel5tSdIiGueM4iFg64zarcDTVbUReLp9JsmVDOe8vqq1uTfJea3NfcAuYGN7Tfe5EzheVVcAdwN3tb5WMZyf+1pgE3D7aCBJkhbHKYOiqn4ATM0obwN2t+XdwA0j9Uer6r2qeh2YBDYlWQNcWFXP1nCS7odntJnu63Hg+na2sQXYV1VTVXUc2MevB5YkaYHN9R7FZVV1BKC9X9rqa4E3R7Y73Gpr2/LM+kltquoE8A5wcaevX5NkV5KJJBPHjh2b45AkSbM50zezM0utOvW5tjm5WHV/VQ2qarB69eqxdlSSNJ65BsXb7XIS7f1oqx8G1o9stw54q9XXzVI/qU2SFcBFDC91fVBfkqRFNNeg2AtMP4W0A3hipL69Pcl0OcOb1s+1y1PvJrmu3X+4aUab6b5uBJ5p9zGeAjYnWdluYm9uNUnSIlpxqg2SfAv4NHBJksMMn0T6KrAnyU7gp8DnAarqYJI9wCvACeCWqnq/dXUzwyeoLgCebC+AB4BHkkwyPJPY3vqaSnIn8Hzb7o6qmnlTXZK0wDL88r58DAaDmpiYWOrdkKRzSpIDVTWYbZ2/mS1J6jIoJEldBoUkqcugkCR1GRSSpC6DQpLUZVBIkroMCklSl0EhSeoyKCRJXQaFJKnLoJAkdRkUkqQug0KS1GVQSJK6DApJUte8giLJG0leSvJCkolWW5VkX5JD7X3lyPa3JZlM8lqSLSP1a1o/k0nuadOl0qZUfazV9yfZMJ/9lSSdvjNxRvEvqurqkZmRbgWerqqNwNPtM0muZDjN6VXAVuDeJOe1NvcBuxjOsb2xrQfYCRyvqiuAu4G7zsD+SpJOw0JcetoG7G7Lu4EbRuqPVtV7VfU6MAlsSrIGuLCqnq3hvKwPz2gz3dfjwPXTZxuSpMUx36Ao4L8nOZBkV6tdVlVHANr7pa2+FnhzpO3hVlvblmfWT2pTVSeAd4CLZ+5Ekl1JJpJMHDt2bJ5DkiSNWjHP9p+qqreSXArsS/KTzraznQlUp95rc3Kh6n7gfoDBYPBr6yVJczevM4qqequ9HwX+AtgEvN0uJ9Hej7bNDwPrR5qvA95q9XWz1E9qk2QFcBEwNZ99liSdnjkHRZJ/mOSj08vAZuBlYC+wo222A3iiLe8FtrcnmS5neNP6uXZ56t0k17X7DzfNaDPd143AM+0+hiRpkczn0tNlwF+0e8srgP9aVf8tyfPAniQ7gZ8CnweoqoNJ9gCvACeAW6rq/dbXzcBDwAXAk+0F8ADwSJJJhmcS2+exv5KkOchy+4I+GAxqYmJiqXdDks4pSQ6M/JrDSfzNbElSl0EhSeoyKCRJXQaFJKnLoJAkdRkUM2y49S+Xehck6axiUEiSugwKSVKXQSFJ6jIoZuF9Ckn6FYNCktRlUHwAzyokacigkCR1GRQdnlVIkkFxSoaFpA87g2IMhoWkD7NzIiiSbE3yWpLJJLcuxT5suPUvDQxJH0rzmQp1USQ5D/hPwL8EDgPPJ9lbVa8sxf7MDIs3vvr7S7EbkrRozvqgADYBk1X1vwCSPApsYzj39pI7nbMMQ0XSuehcCIq1wJsjnw8D145ukGQXsKt9/EWS1+bx8y4B/m4e7T9Q7lqIXudlwcZ6lvmwjBMc63K1GGP9Rx+04lwIisxSq5M+VN0P3H9Gflgy8UETjC83H5axfljGCY51uVrqsZ4LN7MPA+tHPq8D3lqifZGkD51zISieBzYmuTzJbwLbgb1LvE+S9KFx1l96qqoTSb4IPAWcBzxYVQcX8EeekUtY54gPy1g/LOMEx7pcLelYU1Wn3kqS9KF1Llx6kiQtIYNCktS1rIPiVH/6I0P3tPUvJvnkqdomWZVkX5JD7X3lYo2nZ4HG+pUkP0vyQnt9drHG0zPPsT6Y5GiSl2e0OeuO6wKNc1kd0yTrk3w/yatJDib50kibs+6YwoKNdWGPa1UtyxfDG99/C/xj4DeBHwNXztjms8CTDH9X4zpg/6naAv8BuLUt3wrctYzH+hXg3y71+M7UWNu6fw58Enh5Rpuz6rgu4DiX1TEF1gCfbMsfBf5mGf+32hvrgh7X5XxG8f//9EdV/R9g+k9/jNoGPFxDPwQ+lmTNKdpuA3a35d3ADQs9kDEs1FjPRvMZK1X1A2Bqln7PtuO6UOM8G815rFV1pKp+BFBV7wKvMvxrDtNtzqZjCgs31gW1nINitj/9MfN/1A/aptf2sqo6AtDeLz2D+zxXCzVWgC+2098Hz5JT9/mMtedsO64LNU5Ypsc0yQbgE8D+Vjrbjiks3FhhAY/rcg6KU/7pj84247Q9myzUWO8DPg5cDRwBvjbXHTyD5jPWc8lCjXNZHtMkHwG+DXy5qn5+BvftTFuosS7ocV3OQTHOn/74oG16bd+ePr1v70fP4D7P1YKMtarerqr3q+qXwDcYnjYvtfmMtedsO64LMs7leEyTnM/wH85vVtV3RrY5244pLNBYF/q4LuegGOdPf+wFbmpPGVwHvNNOUXtt9wI72vIO4ImFHsgYFmSs0/+RNZ8DXmbpzWesPWfbcV2QcS63Y5okwAPAq1X19VnanE3HFBZorAt+XBfqLvnZ8GL49MDfMHzK4N+32h8Df9yWw3BSpL8FXgIGvbatfjHwNHCova9a6nEu4Fgfadu+yPD/vGuWepxnYKzfYnhq/n8ZfnPbebYe1wUa57I6psA/Y3hZ5kXghfb67Nl6TBdwrAt6XP0THpKkruV86UmSdAYYFJKkLoNCktRlUEiSugwKSVKXQSFJ6jIoJEld/w/SAXfJX5eGYAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "d0s = np.reshape(X[:,:,0],(3000000))\n",
    "d0s = d0s[~(d0s == np.NaN)]\n",
    "d0s\n",
    "plt.hist(d0s,bins='scott') # I believe this is the array containing all the d0s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9568257995346341\n",
      "0.31995557915629463\n",
      "0.18227903038956314\n",
      "\n",
      "4.016282118952634\n",
      "347.65219898040107\n",
      "0.5286935888204137\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(X[:,:,0]*1e4))\n",
    "print(np.mean(X[:,:,1]*1e6))\n",
    "print(np.mean(X[:,:,4]*1e4))\n",
    "print()\n",
    "print(np.std(X[:,:,0]*1e4)) # these are troublesome yes\n",
    "print(np.std(X[:,:,1]*1e6))\n",
    "print(np.std(X[:,:,4]*1e4))\n",
    "# so we will apply the above multiples, this will bring the averages to about unity (unfortunately some outliers will\n",
    "# be very large, so probs better to use a min max scaler or something but then exclude 0?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we scale all the features to bring them to average unity, this is very simplistic so we need a better way in future\n",
    "\n",
    "It means it works faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "firsttime=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "if firsttime==True:\n",
    "    X[:,:,0]=X[:,:,0]*1e4\n",
    "    X[:,:,1]=X[:,:,1]*1e6\n",
    "    X[:,:,4]=X[:,:,4]/1e4\n",
    "    firsttime = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we instead use a min-max scaler to try and solve the problem of scale i.e. certain IPs are 10^4 times larger than others. The min-max scaler means all features will be scaled equivalently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we go, minmax scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xscaled = copy.copy(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Greg\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:355: RuntimeWarning: All-NaN slice encountered\n",
      "  data_min = np.nanmin(X, axis=0)\n",
      "C:\\Users\\Greg\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:356: RuntimeWarning: All-NaN slice encountered\n",
      "  data_max = np.nanmax(X, axis=0)\n",
      "C:\\Users\\Greg\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:355: RuntimeWarning: All-NaN slice encountered\n",
      "  data_min = np.nanmin(X, axis=0)\n",
      "C:\\Users\\Greg\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:356: RuntimeWarning: All-NaN slice encountered\n",
      "  data_max = np.nanmax(X, axis=0)\n",
      "C:\\Users\\Greg\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:355: RuntimeWarning: All-NaN slice encountered\n",
      "  data_min = np.nanmin(X, axis=0)\n",
      "C:\\Users\\Greg\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:356: RuntimeWarning: All-NaN slice encountered\n",
      "  data_max = np.nanmax(X, axis=0)\n",
      "C:\\Users\\Greg\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:355: RuntimeWarning: All-NaN slice encountered\n",
      "  data_min = np.nanmin(X, axis=0)\n",
      "C:\\Users\\Greg\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:356: RuntimeWarning: All-NaN slice encountered\n",
      "  data_max = np.nanmax(X, axis=0)\n",
      "C:\\Users\\Greg\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:355: RuntimeWarning: All-NaN slice encountered\n",
      "  data_min = np.nanmin(X, axis=0)\n",
      "C:\\Users\\Greg\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:356: RuntimeWarning: All-NaN slice encountered\n",
      "  data_max = np.nanmax(X, axis=0)\n"
     ]
    }
   ],
   "source": [
    "for track_variable in range(5):\n",
    "    var_to_scale = Xscaled[:,:,track_variable]\n",
    "    if (track_variable == 2 or track_variable == 3):\n",
    "        scaler=MinMaxScaler([-1,1])\n",
    "    else:\n",
    "        scaler=MinMaxScaler([0,1])\n",
    "    scaler.fit(var_to_scale)\n",
    "    Xscaled[:,:,track_variable] = scaler.transform(var_to_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Greg\\Anaconda3\\lib\\site-packages\\numpy\\lib\\histograms.py:391: RuntimeWarning: invalid value encountered in greater_equal\n",
      "  keep = (a >= first_edge)\n",
      "C:\\Users\\Greg\\Anaconda3\\lib\\site-packages\\numpy\\lib\\histograms.py:392: RuntimeWarning: invalid value encountered in less_equal\n",
      "  keep &= (a <= last_edge)\n",
      "C:\\Users\\Greg\\Anaconda3\\lib\\site-packages\\numpy\\lib\\histograms.py:824: RuntimeWarning: invalid value encountered in greater_equal\n",
      "  keep = (tmp_a >= first_edge)\n",
      "C:\\Users\\Greg\\Anaconda3\\lib\\site-packages\\numpy\\lib\\histograms.py:825: RuntimeWarning: invalid value encountered in less_equal\n",
      "  keep &= (tmp_a <= last_edge)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAD4CAYAAADy46FuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAX+ElEQVR4nO3df4yd1Z3f8fdnMdtFTSA2GOS1rZoGr1RYqSRcGaRUVRq2tpVd1UQiWlfq4j8sOYuIlEhbVbD9gyz8E6pNqJAKEikIQ9OARbLCyoZSL2QVVSKGcUoAQ1jPFhocLOzVeAn5h9bk2z/umeZ6Mhxfz3hm7OH9kq7uc7/Pc848J0/w5z4/Zk6qCkmSPshvLPUOSJLObgaFJKnLoJAkdRkUkqQug0KS1LViqXfgTLvkkktqw4YNS70bknROOXDgwN9V1erZ1i27oNiwYQMTExNLvRuSdE5J8r8/aJ2XniRJXQaFJKnLoJAkdRkUkqQug0KS1GVQSJK6DApJUpdBIUnqMigkSV0GhSSpy6CQJHUZFJKkrrGDIsl5Sf5nku+2z6uS7EtyqL2vHNn2tiSTSV5LsmWkfk2Sl9q6e5Kk1f9BksdafX+SDSNtdrSfcSjJjjMxaEnS+E7njOJLwKsjn28Fnq6qjcDT7TNJrgS2A1cBW4F7k5zX2twH7AI2ttfWVt8JHK+qK4C7gbtaX6uA24FrgU3A7aOBJElaeGMFRZJ1wO8D/3mkvA3Y3ZZ3AzeM1B+tqveq6nVgEtiUZA1wYVU9W1UFPDyjzXRfjwPXt7ONLcC+qpqqquPAPn4VLpKkRTDuGcV/BP4d8MuR2mVVdQSgvV/a6muBN0e2O9xqa9vyzPpJbarqBPAOcHGnr5Mk2ZVkIsnEsWPHxhySJGkcpwyKJH8AHK2qA2P2mVlq1anPtc2vClX3V9WgqgarV886QZMkaY7GOaP4FPCvkrwBPAp8Jsl/Ad5ul5No70fb9oeB9SPt1wFvtfq6WeontUmyArgImOr0JUlaJKcMiqq6rarWVdUGhjepn6mqfwPsBaafQtoBPNGW9wLb25NMlzO8af1cuzz1bpLr2v2Hm2a0me7rxvYzCngK2JxkZbuJvbnVJEmLZD5zZn8V2JNkJ/BT4PMAVXUwyR7gFeAEcEtVvd/a3Aw8BFwAPNleAA8AjySZZHgmsb31NZXkTuD5tt0dVTU1j32WJJ2mDL+4Lx+DwaAmJiaWejck6ZyS5EBVDWZb529mS5K6DApJUpdBIUnqMigkSV0GhSSpy6CQJHUZFJKkLoNCktRlUEiSugwKSVKXQSFJ6jIoJEldBoUkqcugkCR1GRSSpC6DQpLUdcqgSPJbSZ5L8uMkB5P8Wat/JcnPkrzQXp8daXNbkskkryXZMlK/JslLbd09bUpU2rSpj7X6/iQbRtrsSHKovXYgSVpU40yF+h7wmar6RZLzgf+RZHoK07ur6s9HN05yJcOpTK8Cfhv4qyS/06ZDvQ/YBfwQ+B6wleF0qDuB41V1RZLtwF3AHyZZBdwODIACDiTZW1XH5zdsSdK4TnlGUUO/aB/Pb6/e/KnbgEer6r2qeh2YBDYlWQNcWFXP1nD+1YeBG0ba7G7LjwPXt7ONLcC+qppq4bCPYbhIkhbJWPcokpyX5AXgKMN/uPe3VV9M8mKSB5OsbLW1wJsjzQ+32tq2PLN+UpuqOgG8A1zc6Wvm/u1KMpFk4tixY+MMSZI0prGCoqrer6qrgXUMzw5+l+FlpI8DVwNHgK+1zTNbF536XNuM7t/9VTWoqsHq1au7Y5EknZ7Teuqpqv4e+Gtga1W93QLkl8A3gE1ts8PA+pFm64C3Wn3dLPWT2iRZAVwETHX6kiQtknGeelqd5GNt+QLg94CftHsO0z4HvNyW9wLb25NMlwMbgeeq6gjwbpLr2v2Hm4AnRtpMP9F0I/BMu4/xFLA5ycp2aWtzq0mSFsk4Tz2tAXYnOY9hsOypqu8meSTJ1QwvBb0BfAGgqg4m2QO8ApwAbmlPPAHcDDwEXMDwaafpp6ceAB5JMsnwTGJ762sqyZ3A8227O6pqah7jlSSdpgy/uC8fg8GgJiYmlno3JOmckuRAVQ1mW+dvZkuSugwKSVKXQSFJ6jIoJEldBoUkqcugkCR1GRSSpC6DQpLUZVBIkroMCklSl0EhSeoyKCRJXQaFJKnLoJAkdRkUkqQug0KS1DXOVKi/leS5JD9OcjDJn7X6qiT7khxq7ytH2tyWZDLJa0m2jNSvSfJSW3dPmxKVNm3qY62+P8mGkTY72s84lGQHkqRFNc4ZxXvAZ6rqnwJXA1uTXAfcCjxdVRuBp9tnklzJcCrTq4CtwL1tGlWA+4BdDOfR3tjWA+wEjlfVFcDdwF2tr1XA7cC1wCbg9tFAkiQtvFMGRQ39on08v70K2AbsbvXdwA1teRvwaFW9V1WvA5PApiRrgAur6tkazr/68Iw20309Dlzfzja2APuqaqqqjgP7+FW4SJIWwVj3KJKcl+QF4CjDf7j3A5dV1RGA9n5p23wt8OZI88OttrYtz6yf1KaqTgDvABd3+pq5f7uSTCSZOHbs2DhDkiSNaaygqKr3q+pqYB3Ds4Pf7Wye2bro1OfaZnT/7q+qQVUNVq9e3dk1SdLpOq2nnqrq74G/Znj55+12OYn2frRtdhhYP9JsHfBWq6+bpX5SmyQrgIuAqU5fkqRFMs5TT6uTfKwtXwD8HvATYC8w/RTSDuCJtrwX2N6eZLqc4U3r59rlqXeTXNfuP9w0o810XzcCz7T7GE8Bm5OsbDexN7eaJGmRrBhjmzXA7vbk0m8Ae6rqu0meBfYk2Qn8FPg8QFUdTLIHeAU4AdxSVe+3vm4GHgIuAJ5sL4AHgEeSTDI8k9je+ppKcifwfNvujqqams+AJUmnJ8Mv7svHYDCoiYmJpd4NSTqnJDlQVYPZ1vmb2ZKkLoNCktRlUEiSugwKSVKXQSFJ6jIoJEldBoUkqcugkCR1GRSSpC6DQpLUZVBIkroMCklSl0EhSeoyKCRJXQaFJKnLoJAkdY0zFer6JN9P8mqSg0m+1OpfSfKzJC+012dH2tyWZDLJa0m2jNSvSfJSW3dPmxKVNm3qY62+P8mGkTY7khxqrx1IkhbVOFOhngD+pKp+lOSjwIEk+9q6u6vqz0c3TnIlw6lMrwJ+G/irJL/TpkO9D9gF/BD4HrCV4XSoO4HjVXVFku3AXcAfJlkF3A4MgGo/e29VHZ/fsCVJ4zrlGUVVHamqH7Xld4FXgbWdJtuAR6vqvap6HZgENiVZA1xYVc/WcP7Vh4EbRtrsbsuPA9e3s40twL6qmmrhsI9huEiSFslp3aNol4Q+AexvpS8meTHJg0lWttpa4M2RZodbbW1bnlk/qU1VnQDeAS7u9CVJWiRjB0WSjwDfBr5cVT9neBnp48DVwBHga9ObztK8OvW5thndt11JJpJMHDt2rDsOSdLpGSsokpzPMCS+WVXfAaiqt6vq/ar6JfANYFPb/DCwfqT5OuCtVl83S/2kNklWABcBU52+TlJV91fVoKoGq1evHmdIkqQxjfPUU4AHgFer6usj9TUjm30OeLkt7wW2tyeZLgc2As9V1RHg3STXtT5vAp4YaTP9RNONwDPtPsZTwOYkK9ulrc2tJklaJOM89fQp4I+Al5K80Gp/CvzrJFczvBT0BvAFgKo6mGQP8ArDJ6ZuaU88AdwMPARcwPBppydb/QHgkSSTDM8ktre+ppLcCTzftrujqqbmNlRJ0lxk+MV9+RgMBjUxMbHUuyFJ55QkB6pqMNs6fzNbktRlUEiSugwKSVKXQSFJ6jIoJEldBoUkqcugkCR1GRSSpC6DQpLUZVBIkroMCklSl0EhSeoyKCRJXQaFJKnLoJAkdRkUkqSucaZCXZ/k+0leTXIwyZdafVWSfUkOtfeVI21uSzKZ5LUkW0bq1yR5qa27p02JSps29bFW359kw0ibHe1nHEqyA0nSohrnjOIE8CdV9U+A64BbklwJ3Ao8XVUbgafbZ9q67cBVwFbg3iTntb7uA3YxnEd7Y1sPsBM4XlVXAHcDd7W+VgG3A9cCm4DbRwNJkrTwThkUVXWkqn7Ult8FXgXWAtuA3W2z3cANbXkb8GhVvVdVrwOTwKYka4ALq+rZGs6/+vCMNtN9PQ5c3842tgD7qmqqqo4D+/hVuEiSFsFp3aNol4Q+AewHLquqIzAME+DSttla4M2RZodbbW1bnlk/qU1VnQDeAS7u9CVJWiRjB0WSjwDfBr5cVT/vbTpLrTr1ubYZ3bddSSaSTBw7dqyza5Kk0zVWUCQ5n2FIfLOqvtPKb7fLSbT3o61+GFg/0nwd8Farr5ulflKbJCuAi4CpTl8nqar7q2pQVYPVq1ePMyRJ0pjGeeopwAPAq1X19ZFVe4Hpp5B2AE+M1Le3J5kuZ3jT+rl2eerdJNe1Pm+a0Wa6rxuBZ9p9jKeAzUlWtpvYm1tNkrRIVoyxzaeAPwJeSvJCq/0p8FVgT5KdwE+BzwNU1cEke4BXGD4xdUtVvd/a3Qw8BFwAPNleMAyiR5JMMjyT2N76mkpyJ/B82+6Oqpqa41glSXOQ4Rf35WMwGNTExMRS74YknVOSHKiqwWzr/M1sSVKXQSFJ6jIoJEldBoUkqcugkCR1GRSSpC6DQpLUZVBIkroMCklSl0EhSeoyKCRJXQaFJKnLoJAkdRkUkqQug0KS1GVQSJK6xpkK9cEkR5O8PFL7SpKfJXmhvT47su62JJNJXkuyZaR+TZKX2rp72nSotClTH2v1/Uk2jLTZkeRQe01PlSpJWkTjnFE8BGydpX53VV3dXt8DSHIlw2lMr2pt7k1yXtv+PmAXwzm0N470uRM4XlVXAHcDd7W+VgG3A9cCm4Db27zZkqRFdMqgqKofMJzHehzbgEer6r2qeh2YBDYlWQNcWFXP1nDu1YeBG0ba7G7LjwPXt7ONLcC+qpqqquPAPmYPLEnSAprPPYovJnmxXZqa/qa/FnhzZJvDrba2Lc+sn9Smqk4A7wAXd/qSJC2iuQbFfcDHgauBI8DXWj2zbFud+lzbnCTJriQTSSaOHTvW229J0mmaU1BU1dtV9X5V/RL4BsN7CDD81r9+ZNN1wFutvm6W+kltkqwALmJ4qeuD+pptf+6vqkFVDVavXj2XIUmSPsCcgqLdc5j2OWD6iai9wPb2JNPlDG9aP1dVR4B3k1zX7j/cBDwx0mb6iaYbgWfafYyngM1JVrZLW5tbTZK0iFacaoMk3wI+DVyS5DDDJ5E+neRqhpeC3gC+AFBVB5PsAV4BTgC3VNX7raubGT5BdQHwZHsBPAA8kmSS4ZnE9tbXVJI7gefbdndU1bg31SVJZ0iGX96Xj8FgUBMTE0u9G5J0TklyoKoGs63zN7MlSV0GhSSpy6CQJHUZFJKkLoNCktRlUEiSugwKSVKXQSFJ6jIoJEldBoUkqcugkCR1GRSSpC6DQpLUZVBIkroMCklSl0EhSeoyKCRJXacMiiQPJjma5OWR2qok+5Icau8rR9bdlmQyyWtJtozUr0nyUlt3T5s7mza/9mOtvj/JhpE2O9rPOJRkel5tSdIiGueM4iFg64zarcDTVbUReLp9JsmVDOe8vqq1uTfJea3NfcAuYGN7Tfe5EzheVVcAdwN3tb5WMZyf+1pgE3D7aCBJkhbHKYOiqn4ATM0obwN2t+XdwA0j9Uer6r2qeh2YBDYlWQNcWFXP1nCS7odntJnu63Hg+na2sQXYV1VTVXUc2MevB5YkaYHN9R7FZVV1BKC9X9rqa4E3R7Y73Gpr2/LM+kltquoE8A5wcaevX5NkV5KJJBPHjh2b45AkSbM50zezM0utOvW5tjm5WHV/VQ2qarB69eqxdlSSNJ65BsXb7XIS7f1oqx8G1o9stw54q9XXzVI/qU2SFcBFDC91fVBfkqRFNNeg2AtMP4W0A3hipL69Pcl0OcOb1s+1y1PvJrmu3X+4aUab6b5uBJ5p9zGeAjYnWdluYm9uNUnSIlpxqg2SfAv4NHBJksMMn0T6KrAnyU7gp8DnAarqYJI9wCvACeCWqnq/dXUzwyeoLgCebC+AB4BHkkwyPJPY3vqaSnIn8Hzb7o6qmnlTXZK0wDL88r58DAaDmpiYWOrdkKRzSpIDVTWYbZ2/mS1J6jIoJEldBoUkqcugkCR1GRSSpC6DQpLUZVBIkroMCklSl0EhSeoyKCRJXQaFJKnLoJAkdRkUkqQug0KS1GVQSJK6DApJUte8giLJG0leSvJCkolWW5VkX5JD7X3lyPa3JZlM8lqSLSP1a1o/k0nuadOl0qZUfazV9yfZMJ/9lSSdvjNxRvEvqurqkZmRbgWerqqNwNPtM0muZDjN6VXAVuDeJOe1NvcBuxjOsb2xrQfYCRyvqiuAu4G7zsD+SpJOw0JcetoG7G7Lu4EbRuqPVtV7VfU6MAlsSrIGuLCqnq3hvKwPz2gz3dfjwPXTZxuSpMUx36Ao4L8nOZBkV6tdVlVHANr7pa2+FnhzpO3hVlvblmfWT2pTVSeAd4CLZ+5Ekl1JJpJMHDt2bJ5DkiSNWjHP9p+qqreSXArsS/KTzraznQlUp95rc3Kh6n7gfoDBYPBr6yVJczevM4qqequ9HwX+AtgEvN0uJ9Hej7bNDwPrR5qvA95q9XWz1E9qk2QFcBEwNZ99liSdnjkHRZJ/mOSj08vAZuBlYC+wo222A3iiLe8FtrcnmS5neNP6uXZ56t0k17X7DzfNaDPd143AM+0+hiRpkczn0tNlwF+0e8srgP9aVf8tyfPAniQ7gZ8CnweoqoNJ9gCvACeAW6rq/dbXzcBDwAXAk+0F8ADwSJJJhmcS2+exv5KkOchy+4I+GAxqYmJiqXdDks4pSQ6M/JrDSfzNbElSl0EhSeoyKCRJXQaFJKnLoJAkdRkUM2y49S+Xehck6axiUEiSugwKSVKXQSFJ6jIoZuF9Ckn6FYNCktRlUHwAzyokacigkCR1GRQdnlVIkkFxSoaFpA87g2IMhoWkD7NzIiiSbE3yWpLJJLcuxT5suPUvDQxJH0rzmQp1USQ5D/hPwL8EDgPPJ9lbVa8sxf7MDIs3vvr7S7EbkrRozvqgADYBk1X1vwCSPApsYzj39pI7nbMMQ0XSuehcCIq1wJsjnw8D145ukGQXsKt9/EWS1+bx8y4B/m4e7T9Q7lqIXudlwcZ6lvmwjBMc63K1GGP9Rx+04lwIisxSq5M+VN0P3H9Gflgy8UETjC83H5axfljGCY51uVrqsZ4LN7MPA+tHPq8D3lqifZGkD51zISieBzYmuTzJbwLbgb1LvE+S9KFx1l96qqoTSb4IPAWcBzxYVQcX8EeekUtY54gPy1g/LOMEx7pcLelYU1Wn3kqS9KF1Llx6kiQtIYNCktS1rIPiVH/6I0P3tPUvJvnkqdomWZVkX5JD7X3lYo2nZ4HG+pUkP0vyQnt9drHG0zPPsT6Y5GiSl2e0OeuO6wKNc1kd0yTrk3w/yatJDib50kibs+6YwoKNdWGPa1UtyxfDG99/C/xj4DeBHwNXztjms8CTDH9X4zpg/6naAv8BuLUt3wrctYzH+hXg3y71+M7UWNu6fw58Enh5Rpuz6rgu4DiX1TEF1gCfbMsfBf5mGf+32hvrgh7X5XxG8f//9EdV/R9g+k9/jNoGPFxDPwQ+lmTNKdpuA3a35d3ADQs9kDEs1FjPRvMZK1X1A2Bqln7PtuO6UOM8G815rFV1pKp+BFBV7wKvMvxrDtNtzqZjCgs31gW1nINitj/9MfN/1A/aptf2sqo6AtDeLz2D+zxXCzVWgC+2098Hz5JT9/mMtedsO64LNU5Ypsc0yQbgE8D+Vjrbjiks3FhhAY/rcg6KU/7pj84247Q9myzUWO8DPg5cDRwBvjbXHTyD5jPWc8lCjXNZHtMkHwG+DXy5qn5+BvftTFuosS7ocV3OQTHOn/74oG16bd+ePr1v70fP4D7P1YKMtarerqr3q+qXwDcYnjYvtfmMtedsO64LMs7leEyTnM/wH85vVtV3RrY5244pLNBYF/q4LuegGOdPf+wFbmpPGVwHvNNOUXtt9wI72vIO4ImFHsgYFmSs0/+RNZ8DXmbpzWesPWfbcV2QcS63Y5okwAPAq1X19VnanE3HFBZorAt+XBfqLvnZ8GL49MDfMHzK4N+32h8Df9yWw3BSpL8FXgIGvbatfjHwNHCova9a6nEu4Fgfadu+yPD/vGuWepxnYKzfYnhq/n8ZfnPbebYe1wUa57I6psA/Y3hZ5kXghfb67Nl6TBdwrAt6XP0THpKkruV86UmSdAYYFJKkLoNCktRlUEiSugwKSVKXQSFJ6jIoJEld/w/SAXfJX5eGYAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "trackvar = X[:,:,0]\n",
    "trackvar = trackvar[~(trackvar == np.NaN)]\n",
    "print(len(trackvar))\n",
    "\n",
    "plt.hist(trackvar,bins='scott')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "trackvar = Xscaled[:,:,0]\n",
    "trackvar = trackvar[~(trackvar == np.NaN)]\n",
    "print(len(trackvar))\n",
    "\n",
    "\n",
    "plt.hist(trackvar,bins='scott')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xscaled = np.nan_to_num(Xscaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we scale the outputs, actually more simple, we change units from metres to millimetres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the secondary and tertiary vertices as our labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = bjets_DF[['secVtx_x','secVtx_y','secVtx_z','terVtx_x','terVtx_y','terVtx_z']].values \n",
    "y = y*1000 # change units of vertices from m to mm, keep vals close to unity\n",
    "# again convention call labels 'y'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split first 80000 jets as train and next 20000 as test. Below some plots to show these jets are equivalently distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.nan_to_num(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=X[:90000]\n",
    "X_test=X[90000:]\n",
    "y_train=y[:90000]\n",
    "y_test=y[90000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_s1=np.linalg.norm(bjets_DF[['secVtx_x','secVtx_y','secVtx_z']],axis=1)[:90000]\n",
    "b_s2=np.linalg.norm(bjets_DF[['secVtx_x','secVtx_y','secVtx_z']],axis=1)[90000:]\n",
    "b_t1=np.linalg.norm(bjets_DF[['terVtx_x','terVtx_y','terVtx_z']],axis=1)[:90000]\n",
    "b_t2=np.linalg.norm(bjets_DF[['terVtx_x','terVtx_y','terVtx_z']],axis=1)[90000:]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.003680845648113255\n",
      "0.007007205248563903\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(b_s1,bins='scott',range=[0,0.01])\n",
    "#plt.hist(c_SecVtx,bins='scott',range=[0,0.01])\n",
    "plt.hist(b_t1,bins='scott',range=[0,0.01])\n",
    "\n",
    "print(np.mean(b_s1))\n",
    "print(np.mean(b_t1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.003685262818106623\n",
      "0.007117036917486118\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(b_s2,bins='scott',range=[0,0.01])\n",
    "#plt.hist(c_SecVtx,bins='scott',range=[0,0.01])\n",
    "plt.hist(b_t2,bins='scott',range=[0,0.01])\n",
    "\n",
    "print(np.mean(b_s2))\n",
    "print(np.mean(b_t2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So finally we have our features, X, and labels, y. Split into training and testing samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Plots and Sanity Checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I should have some plots on the track variables to ensure everything is logical and working fine, especially when I make changes to the data. Can potentially use seaborn..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing and Training an RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create an RNN based on LSTM cells using keras and tensorflow. The RNN will for each jet candidate take the tracks as inputs and attempt to predict the secondary and tertiary vertex positions. Let's see how well it does.\n",
    "\n",
    "I anticipate having to set a tolerance on the predicted values, it will never get them perfectly but we need to tell it how close it has to get for it to be considered successful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin by creating the RNN architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select number of hidden and dense layers. Initially use same as RNNIP but these can be tuned going forward.\n",
    "\n",
    "nHidden = 100\n",
    "nDense = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "nJets, nTrks, nFeatures = X_train.shape\n",
    "nOutputs = y.shape[1] # ie sec and ter vtx xyz, so 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "trk_inputs = Input(shape=(nTrks,nFeatures),name=\"Trk_inputs\")\n",
    "masked_input = Masking()(trk_inputs)\n",
    "\n",
    "# Feed this merged layer to an RNN\n",
    "lstm = LSTM(nHidden, return_sequences=False, name='LSTM')(masked_input)\n",
    "dpt = Dropout(rate=0.2)(lstm) # this is a very high dropout rate, reduce it\n",
    "\n",
    "my_inputs = trk_inputs\n",
    "\n",
    "# Fully connected layer: This will convert the output of the RNN to our vtx postion predicitons\n",
    "FC = Dense(nDense, activation='relu', name=\"Dense\")(dpt) # is relu fine here? i think so...\n",
    "\n",
    "# Ouptut layer. Sec and Ter Vtx. No activation as this is a regression problem\n",
    "output = Dense(nOutputs, name=\"Vertex_Predictions\")(FC)\n",
    "\n",
    "myRNN = Model(inputs=my_inputs, outputs=output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Trk_inputs (InputLayer)      (None, 30, 5)             0         \n",
      "_________________________________________________________________\n",
      "masking_3 (Masking)          (None, 30, 5)             0         \n",
      "_________________________________________________________________\n",
      "LSTM (LSTM)                  (None, 100)               42400     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "Dense (Dense)                (None, 20)                2020      \n",
      "_________________________________________________________________\n",
      "Vertex_Predictions (Dense)   (None, 6)                 126       \n",
      "=================================================================\n",
      "Total params: 44,546\n",
      "Trainable params: 44,546\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "myRNN.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "myRNN.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mae']) # do i want to add a metric like mse to evaluate during training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " checkpoints, training, evaluation of performance\n",
    " different ways of evaluating performance obviously\n",
    " either akin to Nicole's method for RNNIP\n",
    " or the slighlty different method in https://github.com/agu3rra/NeuralNetwork-RegressionExample/blob/master/Tutorial.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "myRNN_mChkPt = ModelCheckpoint('myRNN_weights.h5',monitor='val_loss', verbose=True,\n",
    "                               save_best_only=True,\n",
    "                               save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStop = EarlyStopping(monitor='val_loss', verbose=True, patience=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 72000 samples, validate on 18000 samples\n",
      "Epoch 1/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 1.9789 - mean_absolute_error: 1.9789Epoch 00001: val_loss improved from inf to 1.49518, saving model to myRNN_weights.h5\n",
      "72000/72000 [==============================] - 23s 325us/step - loss: 1.9783 - mean_absolute_error: 1.9783 - val_loss: 1.4952 - val_mean_absolute_error: 1.4952\n",
      "Epoch 2/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 1.3268 - mean_absolute_error: 1.3268Epoch 00002: val_loss improved from 1.49518 to 1.12483, saving model to myRNN_weights.h5\n",
      "72000/72000 [==============================] - 23s 318us/step - loss: 1.3266 - mean_absolute_error: 1.3266 - val_loss: 1.1248 - val_mean_absolute_error: 1.1248\n",
      "Epoch 3/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 1.1412 - mean_absolute_error: 1.1412Epoch 00003: val_loss improved from 1.12483 to 0.99325, saving model to myRNN_weights.h5\n",
      "72000/72000 [==============================] - 24s 328us/step - loss: 1.1415 - mean_absolute_error: 1.1415 - val_loss: 0.9933 - val_mean_absolute_error: 0.9933\n",
      "Epoch 4/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 1.0760 - mean_absolute_error: 1.0760Epoch 00004: val_loss improved from 0.99325 to 0.94297, saving model to myRNN_weights.h5\n",
      "72000/72000 [==============================] - 25s 345us/step - loss: 1.0760 - mean_absolute_error: 1.0760 - val_loss: 0.9430 - val_mean_absolute_error: 0.9430\n",
      "Epoch 5/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 1.0335 - mean_absolute_error: 1.0335Epoch 00005: val_loss did not improve\n",
      "72000/72000 [==============================] - 25s 341us/step - loss: 1.0334 - mean_absolute_error: 1.0334 - val_loss: 0.9523 - val_mean_absolute_error: 0.9523\n",
      "Epoch 6/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 1.0137 - mean_absolute_error: 1.0137Epoch 00006: val_loss improved from 0.94297 to 0.92664, saving model to myRNN_weights.h5\n",
      "72000/72000 [==============================] - 24s 337us/step - loss: 1.0136 - mean_absolute_error: 1.0136 - val_loss: 0.9266 - val_mean_absolute_error: 0.9266\n",
      "Epoch 7/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 1.0003 - mean_absolute_error: 1.0003Epoch 00007: val_loss improved from 0.92664 to 0.90210, saving model to myRNN_weights.h5\n",
      "72000/72000 [==============================] - 24s 335us/step - loss: 1.0002 - mean_absolute_error: 1.0002 - val_loss: 0.9021 - val_mean_absolute_error: 0.9021\n",
      "Epoch 8/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.9769 - mean_absolute_error: 0.9769Epoch 00008: val_loss improved from 0.90210 to 0.88012, saving model to myRNN_weights.h5\n",
      "72000/72000 [==============================] - 24s 339us/step - loss: 0.9769 - mean_absolute_error: 0.9769 - val_loss: 0.8801 - val_mean_absolute_error: 0.8801\n",
      "Epoch 9/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.9697 - mean_absolute_error: 0.9697Epoch 00009: val_loss did not improve\n",
      "72000/72000 [==============================] - 25s 342us/step - loss: 0.9698 - mean_absolute_error: 0.9698 - val_loss: 0.9023 - val_mean_absolute_error: 0.9023\n",
      "Epoch 10/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.9521 - mean_absolute_error: 0.9521Epoch 00010: val_loss did not improve\n",
      "72000/72000 [==============================] - 25s 343us/step - loss: 0.9520 - mean_absolute_error: 0.9520 - val_loss: 0.9019 - val_mean_absolute_error: 0.9019\n",
      "Epoch 11/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.9460 - mean_absolute_error: 0.9460Epoch 00011: val_loss improved from 0.88012 to 0.85799, saving model to myRNN_weights.h5\n",
      "72000/72000 [==============================] - 25s 346us/step - loss: 0.9460 - mean_absolute_error: 0.9460 - val_loss: 0.8580 - val_mean_absolute_error: 0.8580\n",
      "Epoch 12/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.9420 - mean_absolute_error: 0.9420Epoch 00012: val_loss did not improve\n",
      "72000/72000 [==============================] - 25s 345us/step - loss: 0.9418 - mean_absolute_error: 0.9418 - val_loss: 0.8585 - val_mean_absolute_error: 0.8585\n",
      "Epoch 13/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.9291 - mean_absolute_error: 0.9291- ETA: 1s - loss: 0.9260 - mean_absolute_eEpoch 00013: val_loss did not improve\n",
      "72000/72000 [==============================] - 26s 360us/step - loss: 0.9292 - mean_absolute_error: 0.9292 - val_loss: 0.8680 - val_mean_absolute_error: 0.8680\n",
      "Epoch 14/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.9264 - mean_absolute_error: 0.9264  ETA: 11s - lo - ETA: 9s - Epoch 00014: val_loss improved from 0.85799 to 0.85665, saving model to myRNN_weights.h5\n",
      "72000/72000 [==============================] - 28s 395us/step - loss: 0.9261 - mean_absolute_error: 0.9261 - val_loss: 0.8567 - val_mean_absolute_error: 0.8567\n",
      "Epoch 15/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.9232 - mean_absolute_error: 0.9232Epoch 00015: val_loss improved from 0.85665 to 0.82847, saving model to myRNN_weights.h5\n",
      "72000/72000 [==============================] - 25s 347us/step - loss: 0.9230 - mean_absolute_error: 0.9230 - val_loss: 0.8285 - val_mean_absolute_error: 0.8285\n",
      "Epoch 16/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.9167 - mean_absolute_error: 0.9167- ETA: 0s - loss: 0.9171 - mean_absolute_error: 0Epoch 00016: val_loss did not improve\n",
      "72000/72000 [==============================] - 26s 356us/step - loss: 0.9167 - mean_absolute_error: 0.9167 - val_loss: 0.8403 - val_mean_absolute_error: 0.8403\n",
      "Epoch 17/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.9112 - mean_absolute_error: 0.9112Epoch 00017: val_loss did not improve\n",
      "72000/72000 [==============================] - 26s 364us/step - loss: 0.9113 - mean_absolute_error: 0.9113 - val_loss: 0.8732 - val_mean_absolute_error: 0.8732\n",
      "Epoch 18/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.9061 - mean_absolute_error: 0.9061Epoch 00018: val_loss did not improve\n",
      "72000/72000 [==============================] - 24s 340us/step - loss: 0.9059 - mean_absolute_error: 0.9059 - val_loss: 0.8491 - val_mean_absolute_error: 0.8491\n",
      "Epoch 19/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.9200 - mean_absolute_error: 0.9200Epoch 00019: val_loss improved from 0.82847 to 0.82478, saving model to myRNN_weights.h5\n",
      "72000/72000 [==============================] - 27s 370us/step - loss: 0.9200 - mean_absolute_error: 0.9200 - val_loss: 0.8248 - val_mean_absolute_error: 0.8248\n",
      "Epoch 20/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.9018 - mean_absolute_error: 0.9018Epoch 00020: val_loss improved from 0.82478 to 0.81420, saving model to myRNN_weights.h5\n",
      "72000/72000 [==============================] - 24s 328us/step - loss: 0.9019 - mean_absolute_error: 0.9019 - val_loss: 0.8142 - val_mean_absolute_error: 0.8142\n",
      "Epoch 21/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.8963 - mean_absolute_error: 0.8963Epoch 00021: val_loss did not improve\n",
      "72000/72000 [==============================] - 26s 363us/step - loss: 0.8964 - mean_absolute_error: 0.8964 - val_loss: 0.8372 - val_mean_absolute_error: 0.8372\n",
      "Epoch 22/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.8886 - mean_absolute_error: 0.8886Epoch 00022: val_loss improved from 0.81420 to 0.80950, saving model to myRNN_weights.h5\n",
      "72000/72000 [==============================] - 25s 342us/step - loss: 0.8885 - mean_absolute_error: 0.8885 - val_loss: 0.8095 - val_mean_absolute_error: 0.8095\n",
      "Epoch 23/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.8861 - mean_absolute_error: 0.8861- ETA: 0s - loss: 0.8870 - mean_absolute_error: Epoch 00023: val_loss did not improve\n",
      "72000/72000 [==============================] - 27s 380us/step - loss: 0.8863 - mean_absolute_error: 0.8863 - val_loss: 0.8202 - val_mean_absolute_error: 0.8202\n",
      "Epoch 24/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.8839 - mean_absolute_error: 0.8839Epoch 00024: val_loss did not improve\n",
      "72000/72000 [==============================] - 25s 354us/step - loss: 0.8839 - mean_absolute_error: 0.8839 - val_loss: 0.8206 - val_mean_absolute_error: 0.8206\n",
      "Epoch 25/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.8882 - mean_absolute_error: 0.8882Epoch 00025: val_loss did not improve\n",
      "72000/72000 [==============================] - 24s 328us/step - loss: 0.8882 - mean_absolute_error: 0.8882 - val_loss: 0.8345 - val_mean_absolute_error: 0.8345\n",
      "Epoch 26/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.8785 - mean_absolute_error: 0.8785- ETA: 6s - loss: 0.8833 - ETA: 3s - lossEpoch 00026: val_loss did not improve\n",
      "72000/72000 [==============================] - 28s 385us/step - loss: 0.8785 - mean_absolute_error: 0.8785 - val_loss: 0.8369 - val_mean_absolute_error: 0.8369\n",
      "Epoch 27/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.8729 - mean_absolute_error: 0.8729Epoch 00027: val_loss improved from 0.80950 to 0.79046, saving model to myRNN_weights.h5\n",
      "72000/72000 [==============================] - 24s 335us/step - loss: 0.8729 - mean_absolute_error: 0.8729 - val_loss: 0.7905 - val_mean_absolute_error: 0.7905\n",
      "Epoch 28/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.8704 - mean_absolute_error: 0.8704Epoch 00028: val_loss did not improve\n",
      "72000/72000 [==============================] - 25s 350us/step - loss: 0.8703 - mean_absolute_error: 0.8703 - val_loss: 0.8263 - val_mean_absolute_error: 0.8263\n",
      "Epoch 29/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.8724 - mean_absolute_error: 0.8724Epoch 00029: val_loss did not improve\n",
      "72000/72000 [==============================] - 27s 381us/step - loss: 0.8724 - mean_absolute_error: 0.8724 - val_loss: 0.8039 - val_mean_absolute_error: 0.8039\n",
      "Epoch 30/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.8773 - mean_absolute_error: 0.8773Epoch 00030: val_loss improved from 0.79046 to 0.78937, saving model to myRNN_weights.h5\n",
      "72000/72000 [==============================] - 24s 329us/step - loss: 0.8773 - mean_absolute_error: 0.8773 - val_loss: 0.7894 - val_mean_absolute_error: 0.7894\n",
      "Epoch 31/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.8662 - mean_absolute_error: 0.8662Epoch 00031: val_loss did not improve\n",
      "72000/72000 [==============================] - 28s 388us/step - loss: 0.8661 - mean_absolute_error: 0.8661 - val_loss: 0.8060 - val_mean_absolute_error: 0.8060\n",
      "Epoch 32/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.8578 - mean_absolute_error: 0.8578Epoch 00032: val_loss did not improve\n",
      "72000/72000 [==============================] - 25s 349us/step - loss: 0.8582 - mean_absolute_error: 0.8582 - val_loss: 0.7980 - val_mean_absolute_error: 0.7980\n",
      "Epoch 33/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.8574 - mean_absolute_error: 0.8574Epoch 00033: val_loss improved from 0.78937 to 0.78459, saving model to myRNN_weights.h5\n",
      "72000/72000 [==============================] - 24s 340us/step - loss: 0.8572 - mean_absolute_error: 0.8572 - val_loss: 0.7846 - val_mean_absolute_error: 0.7846\n",
      "Epoch 34/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.8589 - mean_absolute_error: 0.8589- ETA: 3s - Epoch 00034: val_loss did not improve\n",
      "72000/72000 [==============================] - 28s 392us/step - loss: 0.8589 - mean_absolute_error: 0.8589 - val_loss: 0.7905 - val_mean_absolute_error: 0.7905\n",
      "Epoch 35/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.8588 - mean_absolute_error: 0.8588Epoch 00035: val_loss did not improve\n",
      "72000/72000 [==============================] - 24s 338us/step - loss: 0.8589 - mean_absolute_error: 0.8589 - val_loss: 0.8159 - val_mean_absolute_error: 0.8159\n",
      "Epoch 36/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.8447 - mean_absolute_error: 0.8447Epoch 00036: val_loss improved from 0.78459 to 0.76599, saving model to myRNN_weights.h5\n",
      "72000/72000 [==============================] - 27s 375us/step - loss: 0.8447 - mean_absolute_error: 0.8447 - val_loss: 0.7660 - val_mean_absolute_error: 0.7660\n",
      "Epoch 37/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.8463 - mean_absolute_error: 0.8463Epoch 00037: val_loss did not improve\n",
      "72000/72000 [==============================] - 25s 353us/step - loss: 0.8464 - mean_absolute_error: 0.8464 - val_loss: 0.7770 - val_mean_absolute_error: 0.7770\n",
      "Epoch 38/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.8504 - mean_absolute_error: 0.8504Epoch 00038: val_loss did not improve\n",
      "72000/72000 [==============================] - 27s 374us/step - loss: 0.8503 - mean_absolute_error: 0.8503 - val_loss: 0.7726 - val_mean_absolute_error: 0.7726\n",
      "Epoch 39/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.8531 - mean_absolute_error: 0.8531Epoch 00039: val_loss did not improve\n",
      "72000/72000 [==============================] - 26s 366us/step - loss: 0.8533 - mean_absolute_error: 0.8533 - val_loss: 0.7821 - val_mean_absolute_error: 0.7821\n",
      "Epoch 40/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.8474 - mean_absolute_error: 0.8474Epoch 00040: val_loss did not improve\n",
      "72000/72000 [==============================] - 27s 373us/step - loss: 0.8473 - mean_absolute_error: 0.8473 - val_loss: 0.7687 - val_mean_absolute_error: 0.7687\n",
      "Epoch 41/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.8387 - mean_absolute_error: 0.8387Epoch 00041: val_loss improved from 0.76599 to 0.75806, saving model to myRNN_weights.h5\n",
      "72000/72000 [==============================] - 27s 369us/step - loss: 0.8387 - mean_absolute_error: 0.8387 - val_loss: 0.7581 - val_mean_absolute_error: 0.7581\n",
      "Epoch 42/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.8479 - mean_absolute_error: 0.8479Epoch 00042: val_loss did not improve\n",
      "72000/72000 [==============================] - 26s 358us/step - loss: 0.8482 - mean_absolute_error: 0.8482 - val_loss: 0.7742 - val_mean_absolute_error: 0.7742\n",
      "Epoch 43/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.8424 - mean_absolute_error: 0.8424Epoch 00043: val_loss did not improve\n",
      "72000/72000 [==============================] - 28s 385us/step - loss: 0.8423 - mean_absolute_error: 0.8423 - val_loss: 0.8531 - val_mean_absolute_error: 0.8531\n",
      "Epoch 44/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.8287 - mean_absolute_error: 0.8287Epoch 00044: val_loss improved from 0.75806 to 0.74459, saving model to myRNN_weights.h5\n",
      "72000/72000 [==============================] - 25s 350us/step - loss: 0.8287 - mean_absolute_error: 0.8287 - val_loss: 0.7446 - val_mean_absolute_error: 0.7446\n",
      "Epoch 45/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.8257 - mean_absolute_error: 0.8257Epoch 00045: val_loss did not improve\n",
      "72000/72000 [==============================] - 27s 373us/step - loss: 0.8254 - mean_absolute_error: 0.8254 - val_loss: 0.7521 - val_mean_absolute_error: 0.7521\n",
      "Epoch 46/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.8313 - mean_absolute_error: 0.8313Epoch 00046: val_loss did not improve\n",
      "72000/72000 [==============================] - 26s 356us/step - loss: 0.8312 - mean_absolute_error: 0.8312 - val_loss: 0.7489 - val_mean_absolute_error: 0.7489\n",
      "Epoch 47/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.8301 - mean_absolute_error: 0.8301Epoch 00047: val_loss improved from 0.74459 to 0.74345, saving model to myRNN_weights.h5\n",
      "72000/72000 [==============================] - 28s 384us/step - loss: 0.8300 - mean_absolute_error: 0.8300 - val_loss: 0.7434 - val_mean_absolute_error: 0.7434\n",
      "Epoch 48/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.8267 - mean_absolute_error: 0.8267Epoch 00048: val_loss did not improve\n",
      "72000/72000 [==============================] - 25s 345us/step - loss: 0.8267 - mean_absolute_error: 0.8267 - val_loss: 0.7437 - val_mean_absolute_error: 0.7437\n",
      "Epoch 49/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.8222 - mean_absolute_error: 0.8222Epoch 00049: val_loss improved from 0.74345 to 0.74321, saving model to myRNN_weights.h5\n",
      "72000/72000 [==============================] - 27s 370us/step - loss: 0.8223 - mean_absolute_error: 0.8223 - val_loss: 0.7432 - val_mean_absolute_error: 0.7432\n",
      "Epoch 50/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.8251 - mean_absolute_error: 0.8251Epoch 00050: val_loss did not improve\n",
      "72000/72000 [==============================] - 24s 328us/step - loss: 0.8250 - mean_absolute_error: 0.8250 - val_loss: 0.7484 - val_mean_absolute_error: 0.7484\n",
      "Epoch 51/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.8215 - mean_absolute_error: 0.8215Epoch 00051: val_loss did not improve\n",
      "72000/72000 [==============================] - 27s 375us/step - loss: 0.8214 - mean_absolute_error: 0.8214 - val_loss: 0.7771 - val_mean_absolute_error: 0.7771\n",
      "Epoch 52/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.8222 - mean_absolute_error: 0.8222Epoch 00052: val_loss did not improve\n",
      "72000/72000 [==============================] - 24s 330us/step - loss: 0.8221 - mean_absolute_error: 0.8221 - val_loss: 0.7512 - val_mean_absolute_error: 0.7512\n",
      "Epoch 53/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.8176 - mean_absolute_error: 0.8176- ETA: 2s - loss: 0.Epoch 00053: val_loss improved from 0.74321 to 0.73238, saving model to myRNN_weights.h5\n",
      "72000/72000 [==============================] - 25s 353us/step - loss: 0.8176 - mean_absolute_error: 0.8176 - val_loss: 0.7324 - val_mean_absolute_error: 0.7324\n",
      "Epoch 54/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.8117 - mean_absolute_error: 0.8117Epoch 00054: val_loss did not improve\n",
      "72000/72000 [==============================] - 24s 337us/step - loss: 0.8118 - mean_absolute_error: 0.8118 - val_loss: 0.7331 - val_mean_absolute_error: 0.7331\n",
      "Epoch 55/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.8062 - mean_absolute_error: 0.8062Epoch 00055: val_loss did not improve\n",
      "72000/72000 [==============================] - 26s 362us/step - loss: 0.8061 - mean_absolute_error: 0.8061 - val_loss: 0.7409 - val_mean_absolute_error: 0.7409\n",
      "Epoch 56/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.8081 - mean_absolute_error: 0.8081Epoch 00056: val_loss did not improve\n",
      "72000/72000 [==============================] - 24s 333us/step - loss: 0.8080 - mean_absolute_error: 0.8080 - val_loss: 0.7712 - val_mean_absolute_error: 0.7712\n",
      "Epoch 57/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.7998 - mean_absolute_error: 0.7998- ETA: 0s - loss: 0.7996 - mean_absolute_error - ETA: 0s - loss: 0.7994 - mean_absolute_error: 0.7Epoch 00057: val_loss improved from 0.73238 to 0.72472, saving model to myRNN_weights.h5\n",
      "72000/72000 [==============================] - 26s 359us/step - loss: 0.7999 - mean_absolute_error: 0.7999 - val_loss: 0.7247 - val_mean_absolute_error: 0.7247\n",
      "Epoch 58/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.7984 - mean_absolute_error: 0.7984Epoch 00058: val_loss improved from 0.72472 to 0.71797, saving model to myRNN_weights.h5\n",
      "72000/72000 [==============================] - 25s 353us/step - loss: 0.7984 - mean_absolute_error: 0.7984 - val_loss: 0.7180 - val_mean_absolute_error: 0.7180\n",
      "Epoch 59/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.7941 - mean_absolute_error: 0.7941Epoch 00059: val_loss did not improve\n",
      "72000/72000 [==============================] - 25s 343us/step - loss: 0.7939 - mean_absolute_error: 0.7939 - val_loss: 0.7629 - val_mean_absolute_error: 0.7629\n",
      "Epoch 60/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.7980 - mean_absolute_error: 0.7980Epoch 00060: val_loss improved from 0.71797 to 0.71308, saving model to myRNN_weights.h5\n",
      "72000/72000 [==============================] - 26s 367us/step - loss: 0.7979 - mean_absolute_error: 0.7979 - val_loss: 0.7131 - val_mean_absolute_error: 0.7131\n",
      "Epoch 61/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.7939 - mean_absolute_error: 0.7939Epoch 00061: val_loss did not improve\n",
      "72000/72000 [==============================] - 27s 368us/step - loss: 0.7940 - mean_absolute_error: 0.7940 - val_loss: 0.7282 - val_mean_absolute_error: 0.7282\n",
      "Epoch 62/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.7900 - mean_absolute_error: 0.7900Epoch 00062: val_loss did not improve\n",
      "72000/72000 [==============================] - 26s 355us/step - loss: 0.7901 - mean_absolute_error: 0.7901 - val_loss: 0.7174 - val_mean_absolute_error: 0.7174\n",
      "Epoch 63/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.7857 - mean_absolute_error: 0.7857- ETA: 2s - loss: 0.Epoch 00063: val_loss improved from 0.71308 to 0.70233, saving model to myRNN_weights.h5\n",
      "72000/72000 [==============================] - 24s 332us/step - loss: 0.7857 - mean_absolute_error: 0.7857 - val_loss: 0.7023 - val_mean_absolute_error: 0.7023\n",
      "Epoch 64/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.7819 - mean_absolute_error: 0.7819Epoch 00064: val_loss improved from 0.70233 to 0.69914, saving model to myRNN_weights.h5\n",
      "72000/72000 [==============================] - 28s 389us/step - loss: 0.7818 - mean_absolute_error: 0.7818 - val_loss: 0.6991 - val_mean_absolute_error: 0.6991\n",
      "Epoch 65/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.7811 - mean_absolute_error: 0.7811Epoch 00065: val_loss did not improve\n",
      "72000/72000 [==============================] - 24s 338us/step - loss: 0.7812 - mean_absolute_error: 0.7812 - val_loss: 0.7278 - val_mean_absolute_error: 0.7278\n",
      "Epoch 66/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.7786 - mean_absolute_error: 0.7786Epoch 00066: val_loss did not improve\n",
      "72000/72000 [==============================] - 27s 369us/step - loss: 0.7787 - mean_absolute_error: 0.7787 - val_loss: 0.7257 - val_mean_absolute_error: 0.7257\n",
      "Epoch 67/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.7760 - mean_absolute_error: 0.7760Epoch 00067: val_loss improved from 0.69914 to 0.68545, saving model to myRNN_weights.h5\n",
      "72000/72000 [==============================] - 26s 357us/step - loss: 0.7759 - mean_absolute_error: 0.7759 - val_loss: 0.6854 - val_mean_absolute_error: 0.6854\n",
      "Epoch 68/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.7670 - mean_absolute_error: 0.7670Epoch 00068: val_loss did not improve\n",
      "72000/72000 [==============================] - 26s 357us/step - loss: 0.7675 - mean_absolute_error: 0.7675 - val_loss: 0.7385 - val_mean_absolute_error: 0.7385\n",
      "Epoch 69/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.7781 - mean_absolute_error: 0.7781Epoch 00069: val_loss did not improve\n",
      "72000/72000 [==============================] - 26s 360us/step - loss: 0.7780 - mean_absolute_error: 0.7780 - val_loss: 0.6996 - val_mean_absolute_error: 0.6996\n",
      "Epoch 70/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.7651 - mean_absolute_error: 0.7651Epoch 00070: val_loss did not improve\n",
      "72000/72000 [==============================] - 24s 338us/step - loss: 0.7651 - mean_absolute_error: 0.7651 - val_loss: 0.7130 - val_mean_absolute_error: 0.7130\n",
      "Epoch 71/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.7705 - mean_absolute_error: 0.7705- ETA: 6s - loss: 0.7712 - me - ETA:Epoch 00071: val_loss did not improve\n",
      "72000/72000 [==============================] - 28s 393us/step - loss: 0.7706 - mean_absolute_error: 0.7706 - val_loss: 0.6931 - val_mean_absolute_error: 0.6931\n",
      "Epoch 72/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.7606 - mean_absolute_error: 0.7606Epoch 00072: val_loss did not improve\n",
      "72000/72000 [==============================] - 25s 342us/step - loss: 0.7606 - mean_absolute_error: 0.7606 - val_loss: 0.6977 - val_mean_absolute_error: 0.6977\n",
      "Epoch 73/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.7698 - mean_absolute_error: 0.7698Epoch 00073: val_loss did not improve\n",
      "72000/72000 [==============================] - 27s 372us/step - loss: 0.7697 - mean_absolute_error: 0.7697 - val_loss: 0.7008 - val_mean_absolute_error: 0.7008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.7627 - mean_absolute_error: 0.7627Epoch 00074: val_loss did not improve\n",
      "72000/72000 [==============================] - 24s 335us/step - loss: 0.7627 - mean_absolute_error: 0.7627 - val_loss: 0.7037 - val_mean_absolute_error: 0.7037\n",
      "Epoch 75/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.7634 - mean_absolute_error: 0.7634Epoch 00075: val_loss improved from 0.68545 to 0.67417, saving model to myRNN_weights.h5\n",
      "72000/72000 [==============================] - 26s 363us/step - loss: 0.7634 - mean_absolute_error: 0.7634 - val_loss: 0.6742 - val_mean_absolute_error: 0.6742\n",
      "Epoch 76/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.7576 - mean_absolute_error: 0.7576Epoch 00076: val_loss did not improve\n",
      "72000/72000 [==============================] - 25s 340us/step - loss: 0.7578 - mean_absolute_error: 0.7578 - val_loss: 0.7367 - val_mean_absolute_error: 0.7367\n",
      "Epoch 77/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.7591 - mean_absolute_error: 0.7591- ETA: 2s - loss: 0.7564 - ETA: 0s - loss: 0.7589 - mean_absolute_error: 0.7Epoch 00077: val_loss did not improve\n",
      "72000/72000 [==============================] - 26s 359us/step - loss: 0.7590 - mean_absolute_error: 0.7590 - val_loss: 0.7180 - val_mean_absolute_error: 0.7180\n",
      "Epoch 78/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.7542 - mean_absolute_error: 0.7542Epoch 00078: val_loss improved from 0.67417 to 0.67042, saving model to myRNN_weights.h5\n",
      "72000/72000 [==============================] - 25s 342us/step - loss: 0.7543 - mean_absolute_error: 0.7543 - val_loss: 0.6704 - val_mean_absolute_error: 0.6704\n",
      "Epoch 79/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.7472 - mean_absolute_error: 0.7472- - ETA: 1s - loss: 0.7489 - mean_absolutEpoch 00079: val_loss did not improve\n",
      "72000/72000 [==============================] - 26s 366us/step - loss: 0.7473 - mean_absolute_error: 0.7473 - val_loss: 0.6839 - val_mean_absolute_error: 0.6839\n",
      "Epoch 80/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.7550 - mean_absolute_error: 0.7550Epoch 00080: val_loss did not improve\n",
      "72000/72000 [==============================] - 25s 351us/step - loss: 0.7551 - mean_absolute_error: 0.7551 - val_loss: 0.6726 - val_mean_absolute_error: 0.6726\n",
      "Epoch 81/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.7473 - mean_absolute_error: 0.7473Epoch 00081: val_loss improved from 0.67042 to 0.65665, saving model to myRNN_weights.h5\n",
      "72000/72000 [==============================] - 24s 336us/step - loss: 0.7472 - mean_absolute_error: 0.7472 - val_loss: 0.6567 - val_mean_absolute_error: 0.6567\n",
      "Epoch 82/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.7491 - mean_absolute_error: 0.7491Epoch 00082: val_loss did not improve\n",
      "72000/72000 [==============================] - 27s 379us/step - loss: 0.7491 - mean_absolute_error: 0.7491 - val_loss: 0.6801 - val_mean_absolute_error: 0.6801\n",
      "Epoch 83/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.7469 - mean_absolute_error: 0.7469Epoch 00083: val_loss did not improve\n",
      "72000/72000 [==============================] - 24s 335us/step - loss: 0.7470 - mean_absolute_error: 0.7470 - val_loss: 0.6796 - val_mean_absolute_error: 0.6796\n",
      "Epoch 84/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.7473 - mean_absolute_error: 0.7473Epoch 00084: val_loss did not improve\n",
      "72000/72000 [==============================] - 26s 360us/step - loss: 0.7474 - mean_absolute_error: 0.7474 - val_loss: 0.6972 - val_mean_absolute_error: 0.6972\n",
      "Epoch 85/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.7487 - mean_absolute_error: 0.7487Epoch 00085: val_loss did not improve\n",
      "72000/72000 [==============================] - 25s 343us/step - loss: 0.7485 - mean_absolute_error: 0.7485 - val_loss: 0.6999 - val_mean_absolute_error: 0.6999\n",
      "Epoch 86/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.7373 - mean_absolute_error: 0.7373Epoch 00086: val_loss did not improve\n",
      "72000/72000 [==============================] - 27s 374us/step - loss: 0.7372 - mean_absolute_error: 0.7372 - val_loss: 0.6748 - val_mean_absolute_error: 0.6748\n",
      "Epoch 87/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.7357 - mean_absolute_error: 0.7357Epoch 00087: val_loss did not improve\n",
      "72000/72000 [==============================] - 24s 338us/step - loss: 0.7356 - mean_absolute_error: 0.7356 - val_loss: 0.6734 - val_mean_absolute_error: 0.6734\n",
      "Epoch 88/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.7361 - mean_absolute_error: 0.7361Epoch 00088: val_loss did not improve\n",
      "72000/72000 [==============================] - 28s 391us/step - loss: 0.7361 - mean_absolute_error: 0.7361 - val_loss: 0.6696 - val_mean_absolute_error: 0.6696\n",
      "Epoch 89/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.7435 - mean_absolute_error: 0.7435Epoch 00089: val_loss did not improve\n",
      "72000/72000 [==============================] - 24s 339us/step - loss: 0.7435 - mean_absolute_error: 0.7435 - val_loss: 0.6948 - val_mean_absolute_error: 0.6948\n",
      "Epoch 90/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.7388 - mean_absolute_error: 0.7388Epoch 00090: val_loss did not improve\n",
      "72000/72000 [==============================] - 26s 361us/step - loss: 0.7388 - mean_absolute_error: 0.7388 - val_loss: 0.6844 - val_mean_absolute_error: 0.6844\n",
      "Epoch 91/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.7334 - mean_absolute_error: 0.7334Epoch 00091: val_loss did not improve\n",
      "72000/72000 [==============================] - 26s 364us/step - loss: 0.7334 - mean_absolute_error: 0.7334 - val_loss: 0.6669 - val_mean_absolute_error: 0.6669\n",
      "Epoch 00091: early stopping\n"
     ]
    }
   ],
   "source": [
    "nEpochs = 100\n",
    "\n",
    "myRNN_hist = myRNN.fit(X_train, y_train, epochs=nEpochs, batch_size=256,validation_split=0.20,\n",
    "                 callbacks=[earlyStop, myRNN_mChkPt],) # callbacks=[earlyStop, myRNN_mChkPt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1a991865c88>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = np.arange(1,len(myRNN_hist.history['loss'])+1)\n",
    "\n",
    "plt.plot(epochs,myRNN_hist.history['loss'],label='training')\n",
    "plt.plot(epochs,myRNN_hist.history['val_loss'],label='validation')\n",
    "plt.xlabel('epochs',fontsize=14)\n",
    "plt.ylabel('MSE loss',fontsize=14)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "results=myRNN.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.06539709e+00, -7.61532432e-01, -6.75432167e-01,\n",
       "        -2.36472757e+00, -1.78343179e+00, -1.45118979e+00],\n",
       "       [-2.50030854e-01,  6.98754926e-02, -8.97436086e-01,\n",
       "        -3.03779238e-01,  5.93151184e-02, -1.03808858e+00],\n",
       "       [-1.41926028e-03,  1.24233139e-02,  1.32501143e-02,\n",
       "        -2.73928483e-01,  3.18875444e+00,  3.31695037e+00],\n",
       "       ...,\n",
       "       [ 2.59502785e-01,  1.18875115e+00, -1.19260277e+00,\n",
       "         2.08426245e+00,  1.37403028e+01, -1.40827716e+01],\n",
       "       [-8.54168917e-01, -2.13091426e+00,  1.97510391e+00,\n",
       "        -1.34604489e+00, -3.59519445e+00,  3.74493634e+00],\n",
       "       [ 2.61064368e+00,  4.71863327e-01,  5.97985815e-01,\n",
       "         2.70212451e+00,  4.84300743e-01,  6.13586173e-01]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5925093147291125"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean((results-y_test)/y_test) # average error of 60%, so for vtx at 1mm, we get within 0.6mm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "jet_energy                                               43075.4\n",
       "jet_flavour                                                    5\n",
       "secVtx_x                                              0.00417528\n",
       "secVtx_y                                             -0.00100887\n",
       "secVtx_z                                              0.00289704\n",
       "terVtx_x                                              0.00559305\n",
       "terVtx_y                                             -0.00135187\n",
       "terVtx_z                                              0.00407726\n",
       "tracks         [[2.010621651929816e-06, -4.989406079403125e-0...\n",
       "Name: 99999, dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bjets_DF.iloc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at worst perfoming cases and examine the tracks\n",
    "# look at actual uncertainty on overlap for the vertex\n",
    "# re-run with minimised errors on tracks see if performance is better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.05486715, -0.45196092, -2.455217  , -0.1861686 , -0.99255735,\n",
       "        -5.3714914 ]], dtype=float32)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRNN.predict(np.array([X_test[2]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "couple of things, first the RNN looks like it predicts the same value for all cases. This might be because it uses the null tracks at the end of every single jet and just predicts off of those.\n",
    "Maybe should consider predicting values that are at least order unity, because mse is gonna be very small otherwise, so convert the vertices into different units (I dunno maybe millimeters or microns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
