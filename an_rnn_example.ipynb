{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import math\n",
    "import copy\n",
    "import pickle\n",
    "from keras.models import Sequential, Model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from keras.layers import BatchNormalization, Layer, TimeDistributed, Dropout\n",
    "from keras.layers import Dense, Input, Masking, LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by trying to load in our toy jets, and then separate into features (to train on) and labels (to predict).\n",
    "\n",
    "For features, first try d0, z0, phi, theta, qOverP, (refPx, refPy, refPz)?. Essentially the track parameters.\n",
    "\n",
    "For labels, Xs, Ys, Zs, Xt, Yt, Zt. That is the secondary and tertiary vertices. Omit the primary as this has been fixed to (0,0,0). This will require some smart selection for c and light jets, where not all vertices are present. If a vertex is not present, could try predicting (0,0,0) or (-1,-1,-1) or previous vertex (prim or sec)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "bjets_DF = pd.read_pickle(\"./bjets_reverse_order_prims.pkl\")\n",
    "#cjets_DF = pd.read_pickle(\"./cjets.pkl\")\n",
    "#ljets_DF = pd.read_pickle(\"./ljets.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.68124636e-03, 1.13621468e-02, 1.13927932e-02, 1.13750351e-02,\n",
       "        1.12864503e-02, 2.32627197e-05, 1.50793090e-05, 3.04670722e-05,\n",
       "        4.88763621e-06,            nan,            nan,            nan,\n",
       "                   nan,            nan,            nan,            nan,\n",
       "                   nan,            nan,            nan,            nan,\n",
       "                   nan,            nan,            nan,            nan,\n",
       "                   nan,            nan,            nan,            nan,\n",
       "                   nan,            nan]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(np.array([bjets_DF['tracks'][2]])[:,:,5:8], axis=2) #gives a feel for ordering, lists r of ref postion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the track parameters as our features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.58 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trks=np.zeros((len(bjets_DF), 30, 5))\n",
    "\n",
    "for i in range(len(bjets_DF)):\n",
    "    trks[i] = np.array([bjets_DF['tracks'][i]])[:,:,0:5]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = trks # following convention name the features as the vector 'X'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = np.nan_to_num(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Handling: Change 1/p to p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try changing the 1/p feature to p, see what happens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " No Change to end accuracy nor speed-up so won't do this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4850.46696803,  3060.74292196,  4054.83541082, ...,\n",
       "                   nan,            nan,            nan],\n",
       "       [ 2050.96833157,  5200.69668043,  1918.47163884, ...,\n",
       "                   nan,            nan,            nan],\n",
       "       [ 3774.24046227, 14472.05631949,  2894.37128725, ...,\n",
       "                   nan,            nan,            nan],\n",
       "       ...,\n",
       "       [ 2883.77907606,  5114.71929641,  2072.69149129, ...,\n",
       "                   nan,            nan,            nan],\n",
       "       [ 2566.318433  ,   620.46562403,   563.21545658, ...,\n",
       "                   nan,            nan,            nan],\n",
       "       [ 4284.22553045,  7638.42365794,  1493.57056224, ...,\n",
       "                   nan,            nan,            nan]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1/X[:,:,4]  # these are individual tracks remember so p<10,000 is allowed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X[:,:,4] = 1/X[:,:,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = np.nan_to_num(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Track Ordering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use numpy sort function should work, order in descending value say of impact parameter or something (shouldnt be any negatives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe want to move this section and order later, after normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(X[:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.76 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# This code should do the trick, ordering here by d0 in descinding order,\n",
    "# checked that there are no d0 negative vals, so i don't take absolute values\n",
    "\n",
    "for jet in range(len(bjets_DF)):\n",
    "    mat=np.nan_to_num(X[jet]) # Converts nan to 0.0 !!! May need to run this step later if want to minmax scale\n",
    "    mat_sort = mat[mat[:,0].argsort()[::-1]]\n",
    "    X[jet]=mat_sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think we want to ensure all paramters are approx unity in order not to bias the RNN towards a particular feature. phi and theta are already approx unity (order pi), but qOverP is very small order 1e-4 and IP are also small (but scale multiple orders of magnitude) 1e-6 to 1e-2 (unfortunately)\n",
    "\n",
    "So maybe want to use a min max scaler or some thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([4.14254e+05, 3.97440e+04, 2.73160e+04, 2.49970e+04, 2.28790e+04,\n",
       "        2.09220e+04, 1.94820e+04, 1.79300e+04, 1.62320e+04, 1.51510e+04,\n",
       "        1.36940e+04, 1.30840e+04, 1.19910e+04, 1.13580e+04, 1.06600e+04,\n",
       "        9.87000e+03, 9.15000e+03, 8.64700e+03, 8.09100e+03, 7.39100e+03,\n",
       "        7.12900e+03, 6.81400e+03, 6.21900e+03, 6.11100e+03, 5.64200e+03,\n",
       "        5.40500e+03, 5.19000e+03, 4.74400e+03, 4.50200e+03, 4.44700e+03,\n",
       "        4.12500e+03, 3.87000e+03, 3.77500e+03, 3.44600e+03, 3.51600e+03,\n",
       "        3.28500e+03, 3.16200e+03, 2.94100e+03, 2.76400e+03, 2.77500e+03,\n",
       "        2.52500e+03, 2.45500e+03, 2.30500e+03, 2.25500e+03, 2.27100e+03,\n",
       "        2.19700e+03, 2.02900e+03, 1.98700e+03, 1.85200e+03, 1.69000e+03,\n",
       "        1.74000e+03, 1.62300e+03, 1.64400e+03, 1.51700e+03, 1.49800e+03,\n",
       "        1.41400e+03, 1.43000e+03, 1.29700e+03, 1.25400e+03, 1.20700e+03,\n",
       "        1.21800e+03, 1.07400e+03, 1.14900e+03, 1.10000e+03, 1.00500e+03,\n",
       "        1.02200e+03, 9.81000e+02, 9.94000e+02, 8.79000e+02, 8.60000e+02,\n",
       "        8.38000e+02, 7.70000e+02, 8.14000e+02, 7.42000e+02, 7.00000e+02,\n",
       "        7.31000e+02, 7.34000e+02, 7.01000e+02, 6.98000e+02, 6.36000e+02,\n",
       "        6.43000e+02, 6.10000e+02, 5.48000e+02, 5.69000e+02, 5.32000e+02,\n",
       "        5.13000e+02, 5.09000e+02, 4.94000e+02, 4.93000e+02, 4.84000e+02,\n",
       "        4.60000e+02, 4.95000e+02, 4.82000e+02, 4.01000e+02, 4.16000e+02,\n",
       "        3.68000e+02, 3.81000e+02, 3.74000e+02, 3.41000e+02, 3.73000e+02,\n",
       "        3.37000e+02, 3.48000e+02, 3.13000e+02, 3.41000e+02, 3.28000e+02,\n",
       "        2.94000e+02, 2.93000e+02, 2.91000e+02, 2.83000e+02, 2.82000e+02,\n",
       "        2.76000e+02, 2.77000e+02, 2.42000e+02, 2.59000e+02, 2.56000e+02,\n",
       "        2.09000e+02, 2.49000e+02, 2.16000e+02, 2.34000e+02, 2.02000e+02,\n",
       "        1.92000e+02, 2.25000e+02, 2.04000e+02, 2.02000e+02, 2.03000e+02,\n",
       "        1.77000e+02, 1.76000e+02, 1.67000e+02, 1.75000e+02, 1.72000e+02,\n",
       "        1.58000e+02, 1.52000e+02, 1.65000e+02, 1.46000e+02, 1.54000e+02,\n",
       "        1.65000e+02, 1.50000e+02, 1.71000e+02, 1.22000e+02, 1.11000e+02,\n",
       "        1.40000e+02, 1.29000e+02, 1.24000e+02, 1.18000e+02, 1.35000e+02,\n",
       "        1.16000e+02, 1.08000e+02, 1.17000e+02, 1.14000e+02, 1.30000e+02,\n",
       "        1.35000e+02, 1.23000e+02, 9.30000e+01, 1.14000e+02, 9.90000e+01,\n",
       "        1.06000e+02, 1.01000e+02, 9.70000e+01, 9.50000e+01, 9.90000e+01,\n",
       "        8.40000e+01, 8.50000e+01, 8.50000e+01, 7.00000e+01, 8.40000e+01,\n",
       "        7.90000e+01, 8.10000e+01, 8.90000e+01, 7.00000e+01, 9.00000e+01,\n",
       "        6.70000e+01, 6.20000e+01, 8.70000e+01, 7.20000e+01, 6.30000e+01,\n",
       "        6.60000e+01, 7.90000e+01, 6.40000e+01, 6.20000e+01, 7.10000e+01,\n",
       "        5.70000e+01, 5.70000e+01, 5.30000e+01, 4.60000e+01, 6.70000e+01,\n",
       "        6.50000e+01, 6.30000e+01, 5.20000e+01, 5.30000e+01, 4.50000e+01,\n",
       "        4.30000e+01, 5.90000e+01, 4.10000e+01, 3.10000e+01, 4.70000e+01,\n",
       "        5.20000e+01, 4.10000e+01, 5.60000e+01, 3.80000e+01, 5.00000e+01,\n",
       "        4.40000e+01, 4.40000e+01, 4.20000e+01, 4.90000e+01, 3.30000e+01,\n",
       "        3.10000e+01, 3.80000e+01, 3.60000e+01, 3.70000e+01, 3.00000e+01,\n",
       "        3.50000e+01, 4.40000e+01, 3.80000e+01, 3.20000e+01, 4.30000e+01,\n",
       "        3.10000e+01, 2.80000e+01, 3.10000e+01, 2.80000e+01, 2.00000e+01,\n",
       "        3.20000e+01, 2.70000e+01, 2.30000e+01, 3.00000e+01, 2.00000e+01,\n",
       "        3.30000e+01, 2.10000e+01, 2.80000e+01, 3.00000e+01, 1.80000e+01,\n",
       "        3.00000e+01, 2.00000e+01, 2.40000e+01, 1.40000e+01, 2.80000e+01,\n",
       "        2.70000e+01, 1.60000e+01, 2.20000e+01, 2.50000e+01, 2.00000e+01,\n",
       "        1.80000e+01, 2.20000e+01, 3.50000e+01, 2.40000e+01, 2.80000e+01,\n",
       "        2.10000e+01, 1.30000e+01, 2.00000e+01, 2.50000e+01, 1.20000e+01,\n",
       "        2.60000e+01, 2.10000e+01, 2.30000e+01, 1.40000e+01, 2.20000e+01,\n",
       "        1.50000e+01, 1.50000e+01, 1.70000e+01, 1.60000e+01, 1.50000e+01,\n",
       "        1.80000e+01, 1.00000e+01, 1.20000e+01, 2.10000e+01, 8.00000e+00,\n",
       "        1.10000e+01, 2.00000e+01, 1.90000e+01, 1.30000e+01, 1.30000e+01,\n",
       "        1.70000e+01, 1.40000e+01, 1.40000e+01, 1.20000e+01, 1.20000e+01,\n",
       "        1.40000e+01, 1.50000e+01, 1.10000e+01, 9.00000e+00, 7.00000e+00,\n",
       "        1.10000e+01, 1.30000e+01, 4.00000e+00, 8.00000e+00, 8.00000e+00,\n",
       "        1.10000e+01, 1.20000e+01, 1.10000e+01, 7.00000e+00, 1.30000e+01,\n",
       "        9.00000e+00, 1.00000e+01, 4.00000e+00, 1.00000e+01, 1.30000e+01,\n",
       "        1.10000e+01, 1.10000e+01, 1.20000e+01, 1.10000e+01, 8.00000e+00,\n",
       "        3.00000e+00, 1.10000e+01, 6.00000e+00, 8.00000e+00, 1.20000e+01,\n",
       "        1.70000e+01, 4.00000e+00, 8.00000e+00, 1.40000e+01, 8.00000e+00,\n",
       "        8.00000e+00, 1.00000e+01, 1.50000e+01, 7.00000e+00, 4.00000e+00,\n",
       "        5.00000e+00, 3.00000e+00, 6.00000e+00, 7.00000e+00, 9.00000e+00,\n",
       "        5.00000e+00, 6.00000e+00, 9.00000e+00, 5.00000e+00, 4.00000e+00,\n",
       "        3.00000e+00, 5.00000e+00, 6.00000e+00, 3.00000e+00, 5.00000e+00,\n",
       "        5.00000e+00, 8.00000e+00, 8.00000e+00, 6.00000e+00, 3.00000e+00,\n",
       "        3.00000e+00, 9.00000e+00, 4.00000e+00, 4.00000e+00, 2.00000e+00,\n",
       "        5.00000e+00, 5.00000e+00, 5.00000e+00, 3.00000e+00, 5.00000e+00,\n",
       "        3.00000e+00, 3.00000e+00, 3.00000e+00, 4.00000e+00, 1.10000e+01,\n",
       "        4.00000e+00, 5.00000e+00, 2.00000e+00, 1.10000e+01, 0.00000e+00,\n",
       "        5.00000e+00, 4.00000e+00, 1.00000e+00, 2.00000e+00, 4.00000e+00,\n",
       "        4.00000e+00, 4.00000e+00, 7.00000e+00, 3.00000e+00, 5.00000e+00,\n",
       "        2.00000e+00, 4.00000e+00, 2.00000e+00, 3.00000e+00, 2.00000e+00,\n",
       "        4.00000e+00, 5.00000e+00, 1.00000e+00, 2.00000e+00, 2.00000e+00,\n",
       "        3.00000e+00, 8.00000e+00, 3.00000e+00, 4.00000e+00, 1.00000e+00,\n",
       "        5.00000e+00, 2.00000e+00, 6.00000e+00, 3.00000e+00, 1.00000e+00,\n",
       "        4.00000e+00, 2.00000e+00, 5.00000e+00, 4.00000e+00, 6.00000e+00,\n",
       "        3.00000e+00, 1.00000e+00, 3.00000e+00, 3.00000e+00, 3.00000e+00,\n",
       "        1.00000e+00, 0.00000e+00, 3.00000e+00, 3.00000e+00, 1.00000e+00,\n",
       "        1.00000e+00, 2.00000e+00, 0.00000e+00, 0.00000e+00, 4.00000e+00,\n",
       "        4.00000e+00, 4.00000e+00, 0.00000e+00, 3.00000e+00, 1.00000e+00,\n",
       "        1.00000e+00, 4.00000e+00, 2.00000e+00, 2.00000e+00, 0.00000e+00,\n",
       "        3.00000e+00, 3.00000e+00, 1.00000e+00, 4.00000e+00, 1.00000e+00,\n",
       "        0.00000e+00, 2.00000e+00, 2.00000e+00, 2.00000e+00, 2.00000e+00,\n",
       "        2.00000e+00, 4.00000e+00, 3.00000e+00, 2.00000e+00, 1.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 1.00000e+00, 2.00000e+00, 1.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 3.00000e+00, 0.00000e+00, 3.00000e+00,\n",
       "        2.00000e+00, 4.00000e+00, 4.00000e+00, 1.00000e+00, 1.00000e+00,\n",
       "        0.00000e+00, 1.00000e+00, 0.00000e+00, 1.00000e+00, 1.00000e+00,\n",
       "        0.00000e+00, 3.00000e+00, 1.00000e+00, 2.00000e+00, 2.00000e+00,\n",
       "        0.00000e+00, 2.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "        0.00000e+00, 2.00000e+00, 0.00000e+00, 1.00000e+00, 2.00000e+00,\n",
       "        3.00000e+00, 0.00000e+00, 1.00000e+00, 5.00000e+00, 2.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 1.00000e+00,\n",
       "        1.00000e+00, 1.00000e+00, 0.00000e+00, 1.00000e+00, 1.00000e+00,\n",
       "        1.00000e+00, 1.00000e+00, 0.00000e+00, 2.00000e+00, 1.00000e+00,\n",
       "        0.00000e+00, 1.00000e+00, 2.00000e+00, 1.00000e+00, 1.00000e+00,\n",
       "        2.00000e+00, 1.00000e+00, 0.00000e+00, 1.00000e+00, 0.00000e+00,\n",
       "        1.00000e+00, 1.00000e+00, 1.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "        0.00000e+00, 1.00000e+00, 0.00000e+00, 1.00000e+00, 0.00000e+00,\n",
       "        0.00000e+00, 1.00000e+00, 1.00000e+00, 2.00000e+00, 1.00000e+00,\n",
       "        0.00000e+00, 2.00000e+00, 0.00000e+00, 0.00000e+00, 1.00000e+00,\n",
       "        0.00000e+00, 2.00000e+00, 0.00000e+00, 1.00000e+00, 1.00000e+00,\n",
       "        1.00000e+00, 0.00000e+00, 1.00000e+00, 0.00000e+00, 1.00000e+00,\n",
       "        0.00000e+00, 1.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 1.00000e+00, 2.00000e+00, 0.00000e+00,\n",
       "        1.00000e+00, 0.00000e+00, 0.00000e+00, 1.00000e+00, 0.00000e+00,\n",
       "        1.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 1.00000e+00,\n",
       "        2.00000e+00, 0.00000e+00, 1.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 1.00000e+00, 0.00000e+00, 1.00000e+00,\n",
       "        1.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 1.00000e+00, 0.00000e+00, 1.00000e+00,\n",
       "        0.00000e+00, 1.00000e+00, 0.00000e+00, 0.00000e+00, 1.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 0.00000e+00, 1.00000e+00, 1.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 1.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "        0.00000e+00, 1.00000e+00, 0.00000e+00, 0.00000e+00, 1.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "        1.00000e+00, 1.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "        0.00000e+00, 1.00000e+00, 1.00000e+00, 1.00000e+00, 0.00000e+00,\n",
       "        1.00000e+00, 1.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 0.00000e+00, 1.00000e+00, 0.00000e+00,\n",
       "        1.00000e+00, 0.00000e+00, 1.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "        0.00000e+00, 1.00000e+00, 1.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "        1.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 1.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 0.00000e+00, 1.00000e+00, 1.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 0.00000e+00, 1.00000e+00, 0.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 2.00000e+00,\n",
       "        1.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 1.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 1.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 1.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 1.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 0.00000e+00, 1.00000e+00, 0.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 1.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "        0.00000e+00, 1.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 1.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 1.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 1.00000e+00]),\n",
       " array([1.22074451e-09, 2.47088906e-05, 4.94165605e-05, 7.41242304e-05,\n",
       "        9.88319003e-05, 1.23539570e-04, 1.48247240e-04, 1.72954910e-04,\n",
       "        1.97662580e-04, 2.22370250e-04, 2.47077920e-04, 2.71785590e-04,\n",
       "        2.96493259e-04, 3.21200929e-04, 3.45908599e-04, 3.70616269e-04,\n",
       "        3.95323939e-04, 4.20031609e-04, 4.44739279e-04, 4.69446949e-04,\n",
       "        4.94154619e-04, 5.18862288e-04, 5.43569958e-04, 5.68277628e-04,\n",
       "        5.92985298e-04, 6.17692968e-04, 6.42400638e-04, 6.67108308e-04,\n",
       "        6.91815978e-04, 7.16523648e-04, 7.41231317e-04, 7.65938987e-04,\n",
       "        7.90646657e-04, 8.15354327e-04, 8.40061997e-04, 8.64769667e-04,\n",
       "        8.89477337e-04, 9.14185007e-04, 9.38892677e-04, 9.63600346e-04,\n",
       "        9.88308016e-04, 1.01301569e-03, 1.03772336e-03, 1.06243103e-03,\n",
       "        1.08713870e-03, 1.11184637e-03, 1.13655404e-03, 1.16126171e-03,\n",
       "        1.18596938e-03, 1.21067705e-03, 1.23538472e-03, 1.26009239e-03,\n",
       "        1.28480006e-03, 1.30950772e-03, 1.33421539e-03, 1.35892306e-03,\n",
       "        1.38363073e-03, 1.40833840e-03, 1.43304607e-03, 1.45775374e-03,\n",
       "        1.48246141e-03, 1.50716908e-03, 1.53187675e-03, 1.55658442e-03,\n",
       "        1.58129209e-03, 1.60599976e-03, 1.63070743e-03, 1.65541510e-03,\n",
       "        1.68012277e-03, 1.70483044e-03, 1.72953811e-03, 1.75424578e-03,\n",
       "        1.77895345e-03, 1.80366112e-03, 1.82836879e-03, 1.85307646e-03,\n",
       "        1.87778413e-03, 1.90249180e-03, 1.92719947e-03, 1.95190714e-03,\n",
       "        1.97661481e-03, 2.00132248e-03, 2.02603015e-03, 2.05073782e-03,\n",
       "        2.07544549e-03, 2.10015316e-03, 2.12486083e-03, 2.14956850e-03,\n",
       "        2.17427617e-03, 2.19898384e-03, 2.22369151e-03, 2.24839918e-03,\n",
       "        2.27310685e-03, 2.29781452e-03, 2.32252219e-03, 2.34722986e-03,\n",
       "        2.37193753e-03, 2.39664520e-03, 2.42135287e-03, 2.44606054e-03,\n",
       "        2.47076821e-03, 2.49547588e-03, 2.52018355e-03, 2.54489122e-03,\n",
       "        2.56959889e-03, 2.59430656e-03, 2.61901423e-03, 2.64372190e-03,\n",
       "        2.66842957e-03, 2.69313724e-03, 2.71784491e-03, 2.74255258e-03,\n",
       "        2.76726025e-03, 2.79196792e-03, 2.81667559e-03, 2.84138326e-03,\n",
       "        2.86609093e-03, 2.89079860e-03, 2.91550627e-03, 2.94021394e-03,\n",
       "        2.96492161e-03, 2.98962928e-03, 3.01433695e-03, 3.03904462e-03,\n",
       "        3.06375229e-03, 3.08845996e-03, 3.11316763e-03, 3.13787530e-03,\n",
       "        3.16258297e-03, 3.18729064e-03, 3.21199831e-03, 3.23670598e-03,\n",
       "        3.26141365e-03, 3.28612132e-03, 3.31082899e-03, 3.33553666e-03,\n",
       "        3.36024433e-03, 3.38495200e-03, 3.40965967e-03, 3.43436734e-03,\n",
       "        3.45907501e-03, 3.48378268e-03, 3.50849035e-03, 3.53319801e-03,\n",
       "        3.55790568e-03, 3.58261335e-03, 3.60732102e-03, 3.63202869e-03,\n",
       "        3.65673636e-03, 3.68144403e-03, 3.70615170e-03, 3.73085937e-03,\n",
       "        3.75556704e-03, 3.78027471e-03, 3.80498238e-03, 3.82969005e-03,\n",
       "        3.85439772e-03, 3.87910539e-03, 3.90381306e-03, 3.92852073e-03,\n",
       "        3.95322840e-03, 3.97793607e-03, 4.00264374e-03, 4.02735141e-03,\n",
       "        4.05205908e-03, 4.07676675e-03, 4.10147442e-03, 4.12618209e-03,\n",
       "        4.15088976e-03, 4.17559743e-03, 4.20030510e-03, 4.22501277e-03,\n",
       "        4.24972044e-03, 4.27442811e-03, 4.29913578e-03, 4.32384345e-03,\n",
       "        4.34855112e-03, 4.37325879e-03, 4.39796646e-03, 4.42267413e-03,\n",
       "        4.44738180e-03, 4.47208947e-03, 4.49679714e-03, 4.52150481e-03,\n",
       "        4.54621248e-03, 4.57092015e-03, 4.59562782e-03, 4.62033549e-03,\n",
       "        4.64504316e-03, 4.66975083e-03, 4.69445850e-03, 4.71916617e-03,\n",
       "        4.74387384e-03, 4.76858151e-03, 4.79328918e-03, 4.81799685e-03,\n",
       "        4.84270452e-03, 4.86741219e-03, 4.89211986e-03, 4.91682753e-03,\n",
       "        4.94153520e-03, 4.96624287e-03, 4.99095054e-03, 5.01565821e-03,\n",
       "        5.04036588e-03, 5.06507355e-03, 5.08978122e-03, 5.11448889e-03,\n",
       "        5.13919656e-03, 5.16390423e-03, 5.18861190e-03, 5.21331957e-03,\n",
       "        5.23802724e-03, 5.26273491e-03, 5.28744258e-03, 5.31215025e-03,\n",
       "        5.33685792e-03, 5.36156559e-03, 5.38627326e-03, 5.41098093e-03,\n",
       "        5.43568860e-03, 5.46039627e-03, 5.48510394e-03, 5.50981161e-03,\n",
       "        5.53451928e-03, 5.55922695e-03, 5.58393462e-03, 5.60864229e-03,\n",
       "        5.63334996e-03, 5.65805763e-03, 5.68276530e-03, 5.70747297e-03,\n",
       "        5.73218064e-03, 5.75688831e-03, 5.78159597e-03, 5.80630364e-03,\n",
       "        5.83101131e-03, 5.85571898e-03, 5.88042665e-03, 5.90513432e-03,\n",
       "        5.92984199e-03, 5.95454966e-03, 5.97925733e-03, 6.00396500e-03,\n",
       "        6.02867267e-03, 6.05338034e-03, 6.07808801e-03, 6.10279568e-03,\n",
       "        6.12750335e-03, 6.15221102e-03, 6.17691869e-03, 6.20162636e-03,\n",
       "        6.22633403e-03, 6.25104170e-03, 6.27574937e-03, 6.30045704e-03,\n",
       "        6.32516471e-03, 6.34987238e-03, 6.37458005e-03, 6.39928772e-03,\n",
       "        6.42399539e-03, 6.44870306e-03, 6.47341073e-03, 6.49811840e-03,\n",
       "        6.52282607e-03, 6.54753374e-03, 6.57224141e-03, 6.59694908e-03,\n",
       "        6.62165675e-03, 6.64636442e-03, 6.67107209e-03, 6.69577976e-03,\n",
       "        6.72048743e-03, 6.74519510e-03, 6.76990277e-03, 6.79461044e-03,\n",
       "        6.81931811e-03, 6.84402578e-03, 6.86873345e-03, 6.89344112e-03,\n",
       "        6.91814879e-03, 6.94285646e-03, 6.96756413e-03, 6.99227180e-03,\n",
       "        7.01697947e-03, 7.04168714e-03, 7.06639481e-03, 7.09110248e-03,\n",
       "        7.11581015e-03, 7.14051782e-03, 7.16522549e-03, 7.18993316e-03,\n",
       "        7.21464083e-03, 7.23934850e-03, 7.26405617e-03, 7.28876384e-03,\n",
       "        7.31347151e-03, 7.33817918e-03, 7.36288685e-03, 7.38759452e-03,\n",
       "        7.41230219e-03, 7.43700986e-03, 7.46171753e-03, 7.48642520e-03,\n",
       "        7.51113287e-03, 7.53584054e-03, 7.56054821e-03, 7.58525588e-03,\n",
       "        7.60996355e-03, 7.63467122e-03, 7.65937889e-03, 7.68408656e-03,\n",
       "        7.70879423e-03, 7.73350190e-03, 7.75820957e-03, 7.78291724e-03,\n",
       "        7.80762491e-03, 7.83233258e-03, 7.85704025e-03, 7.88174792e-03,\n",
       "        7.90645559e-03, 7.93116326e-03, 7.95587093e-03, 7.98057860e-03,\n",
       "        8.00528626e-03, 8.02999393e-03, 8.05470160e-03, 8.07940927e-03,\n",
       "        8.10411694e-03, 8.12882461e-03, 8.15353228e-03, 8.17823995e-03,\n",
       "        8.20294762e-03, 8.22765529e-03, 8.25236296e-03, 8.27707063e-03,\n",
       "        8.30177830e-03, 8.32648597e-03, 8.35119364e-03, 8.37590131e-03,\n",
       "        8.40060898e-03, 8.42531665e-03, 8.45002432e-03, 8.47473199e-03,\n",
       "        8.49943966e-03, 8.52414733e-03, 8.54885500e-03, 8.57356267e-03,\n",
       "        8.59827034e-03, 8.62297801e-03, 8.64768568e-03, 8.67239335e-03,\n",
       "        8.69710102e-03, 8.72180869e-03, 8.74651636e-03, 8.77122403e-03,\n",
       "        8.79593170e-03, 8.82063937e-03, 8.84534704e-03, 8.87005471e-03,\n",
       "        8.89476238e-03, 8.91947005e-03, 8.94417772e-03, 8.96888539e-03,\n",
       "        8.99359306e-03, 9.01830073e-03, 9.04300840e-03, 9.06771607e-03,\n",
       "        9.09242374e-03, 9.11713141e-03, 9.14183908e-03, 9.16654675e-03,\n",
       "        9.19125442e-03, 9.21596209e-03, 9.24066976e-03, 9.26537743e-03,\n",
       "        9.29008510e-03, 9.31479277e-03, 9.33950044e-03, 9.36420811e-03,\n",
       "        9.38891578e-03, 9.41362345e-03, 9.43833112e-03, 9.46303879e-03,\n",
       "        9.48774646e-03, 9.51245413e-03, 9.53716180e-03, 9.56186947e-03,\n",
       "        9.58657714e-03, 9.61128481e-03, 9.63599248e-03, 9.66070015e-03,\n",
       "        9.68540782e-03, 9.71011549e-03, 9.73482316e-03, 9.75953083e-03,\n",
       "        9.78423850e-03, 9.80894617e-03, 9.83365384e-03, 9.85836151e-03,\n",
       "        9.88306918e-03, 9.90777685e-03, 9.93248452e-03, 9.95719219e-03,\n",
       "        9.98189986e-03, 1.00066075e-02, 1.00313152e-02, 1.00560229e-02,\n",
       "        1.00807305e-02, 1.01054382e-02, 1.01301459e-02, 1.01548535e-02,\n",
       "        1.01795612e-02, 1.02042689e-02, 1.02289766e-02, 1.02536842e-02,\n",
       "        1.02783919e-02, 1.03030996e-02, 1.03278072e-02, 1.03525149e-02,\n",
       "        1.03772226e-02, 1.04019302e-02, 1.04266379e-02, 1.04513456e-02,\n",
       "        1.04760533e-02, 1.05007609e-02, 1.05254686e-02, 1.05501763e-02,\n",
       "        1.05748839e-02, 1.05995916e-02, 1.06242993e-02, 1.06490069e-02,\n",
       "        1.06737146e-02, 1.06984223e-02, 1.07231300e-02, 1.07478376e-02,\n",
       "        1.07725453e-02, 1.07972530e-02, 1.08219606e-02, 1.08466683e-02,\n",
       "        1.08713760e-02, 1.08960836e-02, 1.09207913e-02, 1.09454990e-02,\n",
       "        1.09702067e-02, 1.09949143e-02, 1.10196220e-02, 1.10443297e-02,\n",
       "        1.10690373e-02, 1.10937450e-02, 1.11184527e-02, 1.11431603e-02,\n",
       "        1.11678680e-02, 1.11925757e-02, 1.12172834e-02, 1.12419910e-02,\n",
       "        1.12666987e-02, 1.12914064e-02, 1.13161140e-02, 1.13408217e-02,\n",
       "        1.13655294e-02, 1.13902370e-02, 1.14149447e-02, 1.14396524e-02,\n",
       "        1.14643600e-02, 1.14890677e-02, 1.15137754e-02, 1.15384831e-02,\n",
       "        1.15631907e-02, 1.15878984e-02, 1.16126061e-02, 1.16373137e-02,\n",
       "        1.16620214e-02, 1.16867291e-02, 1.17114367e-02, 1.17361444e-02,\n",
       "        1.17608521e-02, 1.17855598e-02, 1.18102674e-02, 1.18349751e-02,\n",
       "        1.18596828e-02, 1.18843904e-02, 1.19090981e-02, 1.19338058e-02,\n",
       "        1.19585134e-02, 1.19832211e-02, 1.20079288e-02, 1.20326365e-02,\n",
       "        1.20573441e-02, 1.20820518e-02, 1.21067595e-02, 1.21314671e-02,\n",
       "        1.21561748e-02, 1.21808825e-02, 1.22055901e-02, 1.22302978e-02,\n",
       "        1.22550055e-02, 1.22797132e-02, 1.23044208e-02, 1.23291285e-02,\n",
       "        1.23538362e-02, 1.23785438e-02, 1.24032515e-02, 1.24279592e-02,\n",
       "        1.24526668e-02, 1.24773745e-02, 1.25020822e-02, 1.25267899e-02,\n",
       "        1.25514975e-02, 1.25762052e-02, 1.26009129e-02, 1.26256205e-02,\n",
       "        1.26503282e-02, 1.26750359e-02, 1.26997435e-02, 1.27244512e-02,\n",
       "        1.27491589e-02, 1.27738666e-02, 1.27985742e-02, 1.28232819e-02,\n",
       "        1.28479896e-02, 1.28726972e-02, 1.28974049e-02, 1.29221126e-02,\n",
       "        1.29468202e-02, 1.29715279e-02, 1.29962356e-02, 1.30209433e-02,\n",
       "        1.30456509e-02, 1.30703586e-02, 1.30950663e-02, 1.31197739e-02,\n",
       "        1.31444816e-02, 1.31691893e-02, 1.31938969e-02, 1.32186046e-02,\n",
       "        1.32433123e-02, 1.32680200e-02, 1.32927276e-02, 1.33174353e-02,\n",
       "        1.33421430e-02, 1.33668506e-02, 1.33915583e-02, 1.34162660e-02,\n",
       "        1.34409736e-02, 1.34656813e-02, 1.34903890e-02, 1.35150967e-02,\n",
       "        1.35398043e-02, 1.35645120e-02, 1.35892197e-02, 1.36139273e-02,\n",
       "        1.36386350e-02, 1.36633427e-02, 1.36880503e-02, 1.37127580e-02,\n",
       "        1.37374657e-02, 1.37621733e-02, 1.37868810e-02, 1.38115887e-02,\n",
       "        1.38362964e-02, 1.38610040e-02, 1.38857117e-02, 1.39104194e-02,\n",
       "        1.39351270e-02, 1.39598347e-02, 1.39845424e-02, 1.40092500e-02,\n",
       "        1.40339577e-02, 1.40586654e-02, 1.40833731e-02, 1.41080807e-02,\n",
       "        1.41327884e-02, 1.41574961e-02, 1.41822037e-02, 1.42069114e-02,\n",
       "        1.42316191e-02, 1.42563267e-02, 1.42810344e-02, 1.43057421e-02,\n",
       "        1.43304498e-02, 1.43551574e-02, 1.43798651e-02, 1.44045728e-02,\n",
       "        1.44292804e-02, 1.44539881e-02, 1.44786958e-02, 1.45034034e-02,\n",
       "        1.45281111e-02, 1.45528188e-02, 1.45775265e-02, 1.46022341e-02,\n",
       "        1.46269418e-02, 1.46516495e-02, 1.46763571e-02, 1.47010648e-02,\n",
       "        1.47257725e-02, 1.47504801e-02, 1.47751878e-02, 1.47998955e-02,\n",
       "        1.48246032e-02, 1.48493108e-02, 1.48740185e-02, 1.48987262e-02,\n",
       "        1.49234338e-02, 1.49481415e-02, 1.49728492e-02, 1.49975568e-02,\n",
       "        1.50222645e-02, 1.50469722e-02, 1.50716799e-02, 1.50963875e-02,\n",
       "        1.51210952e-02, 1.51458029e-02, 1.51705105e-02, 1.51952182e-02,\n",
       "        1.52199259e-02, 1.52446335e-02, 1.52693412e-02, 1.52940489e-02,\n",
       "        1.53187566e-02, 1.53434642e-02, 1.53681719e-02, 1.53928796e-02,\n",
       "        1.54175872e-02, 1.54422949e-02, 1.54670026e-02, 1.54917102e-02,\n",
       "        1.55164179e-02, 1.55411256e-02, 1.55658333e-02, 1.55905409e-02,\n",
       "        1.56152486e-02, 1.56399563e-02, 1.56646639e-02, 1.56893716e-02,\n",
       "        1.57140793e-02, 1.57387869e-02, 1.57634946e-02, 1.57882023e-02,\n",
       "        1.58129100e-02, 1.58376176e-02, 1.58623253e-02, 1.58870330e-02,\n",
       "        1.59117406e-02, 1.59364483e-02, 1.59611560e-02, 1.59858636e-02,\n",
       "        1.60105713e-02, 1.60352790e-02, 1.60599866e-02, 1.60846943e-02,\n",
       "        1.61094020e-02, 1.61341097e-02, 1.61588173e-02, 1.61835250e-02,\n",
       "        1.62082327e-02, 1.62329403e-02, 1.62576480e-02, 1.62823557e-02,\n",
       "        1.63070633e-02, 1.63317710e-02, 1.63564787e-02, 1.63811864e-02,\n",
       "        1.64058940e-02, 1.64306017e-02, 1.64553094e-02, 1.64800170e-02,\n",
       "        1.65047247e-02, 1.65294324e-02, 1.65541400e-02, 1.65788477e-02,\n",
       "        1.66035554e-02, 1.66282631e-02, 1.66529707e-02, 1.66776784e-02,\n",
       "        1.67023861e-02, 1.67270937e-02, 1.67518014e-02, 1.67765091e-02,\n",
       "        1.68012167e-02, 1.68259244e-02, 1.68506321e-02, 1.68753398e-02,\n",
       "        1.69000474e-02, 1.69247551e-02, 1.69494628e-02, 1.69741704e-02,\n",
       "        1.69988781e-02, 1.70235858e-02, 1.70482934e-02, 1.70730011e-02,\n",
       "        1.70977088e-02, 1.71224165e-02, 1.71471241e-02, 1.71718318e-02,\n",
       "        1.71965395e-02, 1.72212471e-02, 1.72459548e-02, 1.72706625e-02,\n",
       "        1.72953701e-02, 1.73200778e-02, 1.73447855e-02, 1.73694932e-02,\n",
       "        1.73942008e-02, 1.74189085e-02, 1.74436162e-02, 1.74683238e-02,\n",
       "        1.74930315e-02, 1.75177392e-02, 1.75424468e-02, 1.75671545e-02,\n",
       "        1.75918622e-02, 1.76165699e-02, 1.76412775e-02, 1.76659852e-02,\n",
       "        1.76906929e-02, 1.77154005e-02, 1.77401082e-02, 1.77648159e-02,\n",
       "        1.77895235e-02, 1.78142312e-02, 1.78389389e-02, 1.78636466e-02,\n",
       "        1.78883542e-02, 1.79130619e-02, 1.79377696e-02, 1.79624772e-02,\n",
       "        1.79871849e-02, 1.80118926e-02, 1.80366002e-02, 1.80613079e-02,\n",
       "        1.80860156e-02, 1.81107232e-02, 1.81354309e-02, 1.81601386e-02,\n",
       "        1.81848463e-02, 1.82095539e-02, 1.82342616e-02, 1.82589693e-02,\n",
       "        1.82836769e-02, 1.83083846e-02, 1.83330923e-02, 1.83577999e-02,\n",
       "        1.83825076e-02, 1.84072153e-02, 1.84319230e-02, 1.84566306e-02,\n",
       "        1.84813383e-02, 1.85060460e-02, 1.85307536e-02, 1.85554613e-02,\n",
       "        1.85801690e-02, 1.86048766e-02, 1.86295843e-02, 1.86542920e-02,\n",
       "        1.86789997e-02, 1.87037073e-02, 1.87284150e-02, 1.87531227e-02,\n",
       "        1.87778303e-02, 1.88025380e-02, 1.88272457e-02, 1.88519533e-02,\n",
       "        1.88766610e-02, 1.89013687e-02, 1.89260764e-02, 1.89507840e-02,\n",
       "        1.89754917e-02, 1.90001994e-02, 1.90249070e-02, 1.90496147e-02,\n",
       "        1.90743224e-02, 1.90990300e-02, 1.91237377e-02, 1.91484454e-02,\n",
       "        1.91731531e-02, 1.91978607e-02, 1.92225684e-02, 1.92472761e-02,\n",
       "        1.92719837e-02, 1.92966914e-02, 1.93213991e-02, 1.93461067e-02,\n",
       "        1.93708144e-02, 1.93955221e-02, 1.94202298e-02, 1.94449374e-02,\n",
       "        1.94696451e-02, 1.94943528e-02, 1.95190604e-02, 1.95437681e-02,\n",
       "        1.95684758e-02, 1.95931834e-02, 1.96178911e-02, 1.96425988e-02,\n",
       "        1.96673065e-02, 1.96920141e-02, 1.97167218e-02, 1.97414295e-02,\n",
       "        1.97661371e-02, 1.97908448e-02, 1.98155525e-02, 1.98402601e-02,\n",
       "        1.98649678e-02, 1.98896755e-02, 1.99143832e-02, 1.99390908e-02,\n",
       "        1.99637985e-02, 1.99885062e-02, 2.00132138e-02, 2.00379215e-02,\n",
       "        2.00626292e-02, 2.00873368e-02, 2.01120445e-02, 2.01367522e-02,\n",
       "        2.01614599e-02, 2.01861675e-02, 2.02108752e-02, 2.02355829e-02,\n",
       "        2.02602905e-02, 2.02849982e-02, 2.03097059e-02, 2.03344135e-02,\n",
       "        2.03591212e-02, 2.03838289e-02, 2.04085365e-02, 2.04332442e-02,\n",
       "        2.04579519e-02, 2.04826596e-02, 2.05073672e-02, 2.05320749e-02,\n",
       "        2.05567826e-02, 2.05814902e-02, 2.06061979e-02, 2.06309056e-02,\n",
       "        2.06556132e-02, 2.06803209e-02, 2.07050286e-02, 2.07297363e-02,\n",
       "        2.07544439e-02, 2.07791516e-02, 2.08038593e-02, 2.08285669e-02,\n",
       "        2.08532746e-02, 2.08779823e-02, 2.09026899e-02, 2.09273976e-02,\n",
       "        2.09521053e-02, 2.09768130e-02, 2.10015206e-02, 2.10262283e-02,\n",
       "        2.10509360e-02, 2.10756436e-02, 2.11003513e-02, 2.11250590e-02,\n",
       "        2.11497666e-02, 2.11744743e-02, 2.11991820e-02, 2.12238897e-02,\n",
       "        2.12485973e-02, 2.12733050e-02, 2.12980127e-02, 2.13227203e-02,\n",
       "        2.13474280e-02, 2.13721357e-02, 2.13968433e-02, 2.14215510e-02,\n",
       "        2.14462587e-02, 2.14709664e-02, 2.14956740e-02, 2.15203817e-02,\n",
       "        2.15450894e-02, 2.15697970e-02, 2.15945047e-02, 2.16192124e-02,\n",
       "        2.16439200e-02, 2.16686277e-02, 2.16933354e-02, 2.17180431e-02,\n",
       "        2.17427507e-02, 2.17674584e-02, 2.17921661e-02, 2.18168737e-02,\n",
       "        2.18415814e-02, 2.18662891e-02, 2.18909967e-02, 2.19157044e-02,\n",
       "        2.19404121e-02, 2.19651198e-02, 2.19898274e-02, 2.20145351e-02,\n",
       "        2.20392428e-02, 2.20639504e-02, 2.20886581e-02, 2.21133658e-02,\n",
       "        2.21380734e-02, 2.21627811e-02, 2.21874888e-02, 2.22121965e-02,\n",
       "        2.22369041e-02, 2.22616118e-02, 2.22863195e-02, 2.23110271e-02,\n",
       "        2.23357348e-02, 2.23604425e-02, 2.23851501e-02, 2.24098578e-02,\n",
       "        2.24345655e-02, 2.24592732e-02, 2.24839808e-02, 2.25086885e-02,\n",
       "        2.25333962e-02, 2.25581038e-02, 2.25828115e-02, 2.26075192e-02,\n",
       "        2.26322268e-02, 2.26569345e-02, 2.26816422e-02, 2.27063498e-02,\n",
       "        2.27310575e-02, 2.27557652e-02, 2.27804729e-02, 2.28051805e-02,\n",
       "        2.28298882e-02, 2.28545959e-02, 2.28793035e-02, 2.29040112e-02,\n",
       "        2.29287189e-02, 2.29534265e-02, 2.29781342e-02, 2.30028419e-02,\n",
       "        2.30275496e-02, 2.30522572e-02, 2.30769649e-02, 2.31016726e-02,\n",
       "        2.31263802e-02, 2.31510879e-02, 2.31757956e-02, 2.32005032e-02,\n",
       "        2.32252109e-02, 2.32499186e-02, 2.32746263e-02, 2.32993339e-02,\n",
       "        2.33240416e-02, 2.33487493e-02, 2.33734569e-02, 2.33981646e-02,\n",
       "        2.34228723e-02, 2.34475799e-02, 2.34722876e-02, 2.34969953e-02,\n",
       "        2.35217030e-02, 2.35464106e-02, 2.35711183e-02, 2.35958260e-02,\n",
       "        2.36205336e-02, 2.36452413e-02, 2.36699490e-02, 2.36946566e-02,\n",
       "        2.37193643e-02, 2.37440720e-02, 2.37687797e-02, 2.37934873e-02]),\n",
       " <a list of 963 Patch objects>)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAD4CAYAAADy46FuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAXx0lEQVR4nO3df4yd1Z3f8fdnMdtFTSA2GOS1UU2DVyqsVCdcGaRUVRq2tsWuaiIRrSt18R+WnEVESqStKtj+QRb+CdUmVEgFiRSEoWnAIllhZUOpF7KKKhHDOCWAIaxnCw0OFvZqvIT8Q2vy7R/3TLmejI+vZ+wZxn6/pEf3ud/nOWeeoyv7c58fMydVhSRJJ/Ibi30AkqSPNoNCktRlUEiSugwKSVKXQSFJ6lq22Adwul1yySW1du3axT4MSVpS9u3b93dVtXK2bWddUKxdu5aJiYnFPgxJWlKS/O8TbfPSkySpy6CQJHUZFJKkLoNCktRlUEiSugwKSVKXQSFJ6jIoJEldBoUkqcugkCR1GRSSpC6DQpLUZVBIkrrGDook5yX5n0m+196vSLInyYH2unxk39uTTCZ5Pcmmkfo1SV5u2+5Nklb/B0keb/W9SdaOtNnWfsaBJNtOx6AlSeM7lTOKLwOvjby/DXimqtYBz7T3JLkK2ApcDWwG7ktyXmtzP7ADWNeWza2+HThaVVcC9wB3t75WAHcA1wIbgDtGA0mSdOaNFRRJ1gC/D/znkfIWYGdb3wncOFJ/rKrer6o3gElgQ5JVwIVV9VxVFfDIjDbTfT0BXN/ONjYBe6pqqqqOAnv4MFwkSQtg3DOK/wj8O+BXI7XLquoQQHu9tNVXA2+N7Hew1Va39Zn149pU1THgXeDiTl/HSbIjyUSSiSNHjow5JEnSOE4aFEn+ADhcVfvG7DOz1KpTn2ubDwtVD1TVoKoGK1fOOpOfJGmOxjmj+Azwr5K8CTwGfC7JfwHeaZeTaK+H2/4HgctH2q8B3m71NbPUj2uTZBlwETDV6UuStEBOGhRVdXtVramqtQxvUj9bVf8G2A1MP4W0DXiyre8GtrYnma5geNP6+XZ56r0k17X7DzfPaDPd103tZxTwNLAxyfJ2E3tjq0mSFsiyebT9GrAryXbgZ8AXAKpqf5JdwKvAMeDWqvqgtbkFeBi4AHiqLQAPAo8mmWR4JrG19TWV5C7ghbbfnVU1NY9jliSdogy/uJ89BoNBTUxMLPZhSNKSkmRfVQ1m2+ZvZkuSugwKSVKXQSFJ6jIoJEldBoUkqcugkCR1GRSSpC6DQpLUZVBIkroMCklSl0EhSeoyKCRJXQaFJKnLoJAkdRkUkqQug0KS1HXSoEjyW0meT/KTJPuT/FmrfzXJz5O82JYbRtrcnmQyyetJNo3Ur0nyctt2b5sSlTZt6uOtvjfJ2pE225IcaMs2JEkLapypUN8HPldVv0xyPvA/kkxPYXpPVf356M5JrmI4lenVwG8Df5Xkd9p0qPcDO4AfAd8HNjOcDnU7cLSqrkyyFbgb+MMkK4A7gAFQwL4ku6vq6PyGLUka10nPKGrol+3t+W3pzZ+6BXisqt6vqjeASWBDklXAhVX1XA3nX30EuHGkzc62/gRwfTvb2ATsqaqpFg57GIaLJGmBjHWPIsl5SV4EDjP8j3tv2/SlJC8leSjJ8lZbDbw10vxgq61u6zPrx7WpqmPAu8DFnb5mHt+OJBNJJo4cOTLOkCRJYxorKKrqg6paD6xheHbwuwwvI30SWA8cAr7eds9sXXTqc20zenwPVNWgqgYrV67sjkWSdGpO6amnqvp74K+BzVX1TguQXwHfBDa03Q4Cl480WwO83eprZqkf1ybJMuAiYKrTlyRpgYzz1NPKJJ9o6xcAvwf8tN1zmPZ54JW2vhvY2p5kugJYBzxfVYeA95Jc1+4/3Aw8OdJm+ommm4Bn232Mp4GNSZa3S1sbW02StEDGeeppFbAzyXkMg2VXVX0vyaNJ1jO8FPQm8EWAqtqfZBfwKnAMuLU98QRwC/AwcAHDp52mn556EHg0ySTDM4mtra+pJHcBL7T97qyqqXmMV5J0ijL84n72GAwGNTExsdiHIUlLSpJ9VTWYbZu/mS1J6jIoJEldBoUkqcugkCR1GRSSpC6DQpLUZVBIkroMCklSl0EhSeoyKCRJXQaFJKnLoJAkdRkUkqQug0KS1GVQSJK6DApJUtc4U6H+VpLnk/wkyf4kf9bqK5LsSXKgvS4faXN7kskkryfZNFK/JsnLbdu9bUpU2rSpj7f63iRrR9psaz/jQJJtSJIW1DhnFO8Dn6uqfwqsBzYnuQ64DXimqtYBz7T3JLmK4VSmVwObgfvaNKoA9wM7GM6jva5tB9gOHK2qK4F7gLtbXyuAO4BrgQ3AHaOBJEk6804aFDX0y/b2/LYUsAXY2eo7gRvb+hbgsap6v6reACaBDUlWARdW1XM1nH/1kRltpvt6Ari+nW1sAvZU1VRVHQX28GG4SJIWwFj3KJKcl+RF4DDD/7j3ApdV1SGA9npp23018NZI84Ottrqtz6wf16aqjgHvAhd3+pp5fDuSTCSZOHLkyDhDkiSNaaygqKoPqmo9sIbh2cHvdnbPbF106nNtM3p8D1TVoKoGK1eu7ByaJOlUndJTT1X198BfM7z88067nER7Pdx2OwhcPtJsDfB2q6+ZpX5cmyTLgIuAqU5fkqQFMs5TTyuTfKKtXwD8HvBTYDcw/RTSNuDJtr4b2NqeZLqC4U3r59vlqfeSXNfuP9w8o810XzcBz7b7GE8DG5MsbzexN7aaJGmBLBtjn1XAzvbk0m8Au6rqe0meA3Yl2Q78DPgCQFXtT7ILeBU4BtxaVR+0vm4BHgYuAJ5qC8CDwKNJJhmeSWxtfU0luQt4oe13Z1VNzWfAkqRTk+EX97PHYDCoiYmJxT4MSVpSkuyrqsFs2/zNbElSl0EhSeoyKCRJXQaFJKnLoJAkdRkUkqQug0KS1GVQSJK6DApJUpdBIUnqMigkSV0GhSSpy6CQJHUZFJKkLoNCktRlUEiSusaZCvXyJD9I8lqS/Um+3OpfTfLzJC+25YaRNrcnmUzyepJNI/Vrkrzctt3bpkSlTZv6eKvvTbJ2pM22JAfasg1J0oIaZyrUY8CfVNWPk3wc2JdkT9t2T1X9+ejOSa5iOJXp1cBvA3+V5HfadKj3AzuAHwHfBzYznA51O3C0qq5MshW4G/jDJCuAO4ABUO1n766qo/MbtiRpXCc9o6iqQ1X147b+HvAasLrTZAvwWFW9X1VvAJPAhiSrgAur6rkazr/6CHDjSJudbf0J4Pp2trEJ2FNVUy0c9jAMF0nSAjmlexTtktCngL2t9KUkLyV5KMnyVlsNvDXS7GCrrW7rM+vHtamqY8C7wMWdvmYe144kE0kmjhw5cipDkiSdxNhBkeRjwHeAr1TVLxheRvoksB44BHx9etdZmlenPtc2HxaqHqiqQVUNVq5c2R2HJOnUjBUUSc5nGBLfqqrvAlTVO1X1QVX9CvgmsKHtfhC4fKT5GuDtVl8zS/24NkmWARcBU52+JEkLZJynngI8CLxWVd8Yqa8a2e3zwCttfTewtT3JdAWwDni+qg4B7yW5rvV5M/DkSJvpJ5puAp5t9zGeBjYmWd4ubW1sNUnSAhnnqafPAH8EvJzkxVb7U+BfJ1nP8FLQm8AXAapqf5JdwKsMn5i6tT3xBHAL8DBwAcOnnZ5q9QeBR5NMMjyT2Nr6mkpyF/BC2+/Oqpqa21AlSXOR4Rf3s8dgMKiJiYnFPgxJWlKS7KuqwWzb/M1sSVKXQSFJ6jIoJEldBoUkqcugkCR1GRSSpC6DQpLUZVBIkroMCklSl0EhSeoyKCRJXQaFJKnLoJAkdRkUkqQug0KS1GVQSJK6xpkK9fIkP0jyWpL9Sb7c6iuS7ElyoL0uH2lze5LJJK8n2TRSvybJy23bvW1KVNq0qY+3+t4ka0fabGs/40CSbUiSFtQ4ZxTHgD+pqn8CXAfcmuQq4DbgmapaBzzT3tO2bQWuBjYD9yU5r/V1P7CD4Tza69p2gO3A0aq6ErgHuLv1tQK4A7gW2ADcMRpIkqQz76RBUVWHqurHbf094DVgNbAF2Nl22wnc2Na3AI9V1ftV9QYwCWxIsgq4sKqeq+H8q4/MaDPd1xPA9e1sYxOwp6qmquoosIcPw0WStABO6R5FuyT0KWAvcFlVHYJhmACXtt1WA2+NNDvYaqvb+sz6cW2q6hjwLnBxp6+Zx7UjyUSSiSNHjpzKkCRJJzF2UCT5GPAd4CtV9YverrPUqlOfa5sPC1UPVNWgqgYrV67sHJok6VSNFRRJzmcYEt+qqu+28jvtchLt9XCrHwQuH2m+Bni71dfMUj+uTZJlwEXAVKcvSdICGeeppwAPAq9V1TdGNu0Gpp9C2gY8OVLf2p5kuoLhTevn2+Wp95Jc1/q8eUab6b5uAp5t9zGeBjYmWd5uYm9sNUnSAlk2xj6fAf4IeDnJi632p8DXgF1JtgM/A74AUFX7k+wCXmX4xNStVfVBa3cL8DBwAfBUW2AYRI8mmWR4JrG19TWV5C7ghbbfnVU1NcexSpLmIMMv7mePwWBQExMTi30YkrSkJNlXVYPZtvmb2ZKkLoNCktRlUEiSugwKSVKXQSFJ6jIoJEldBoUkqcugkCR1GRSSpC6DQpLUZVBIkroMCklSl0EhSeoyKCRJXQaFJKnLoJAkdY0zFepDSQ4neWWk9tUkP0/yYltuGNl2e5LJJK8n2TRSvybJy23bvW06VNqUqY+3+t4ka0fabEtyoC3TU6VKkhbQOGcUDwObZ6nfU1Xr2/J9gCRXMZzG9OrW5r4k57X97wd2MJxDe91In9uBo1V1JXAPcHfrawVwB3AtsAG4o82bLUlaQCcNiqr6IcN5rMexBXisqt6vqjeASWBDklXAhVX1XA3nXn0EuHGkzc62/gRwfTvb2ATsqaqpqjoK7GH2wJIknUHzuUfxpSQvtUtT09/0VwNvjexzsNVWt/WZ9ePaVNUx4F3g4k5fvybJjiQTSSaOHDkyjyFJkmaaa1DcD3wSWA8cAr7e6pll3+rU59rm+GLVA1U1qKrBypUre8ctSTpFcwqKqnqnqj6oql8B32R4DwGG3/ovH9l1DfB2q6+ZpX5cmyTLgIsYXuo6UV+SpAU0p6Bo9xymfR6YfiJqN7C1Pcl0BcOb1s9X1SHgvSTXtfsPNwNPjrSZfqLpJuDZdh/jaWBjkuXt0tbGVpMkLaBlJ9shybeBzwKXJDnI8EmkzyZZz/BS0JvAFwGqan+SXcCrwDHg1qr6oHV1C8MnqC4AnmoLwIPAo0kmGZ5JbG19TSW5C3ih7XdnVY17U12SdJpk+OX97DEYDGpiYmKxD0OSlpQk+6pqMNs2fzNbktRlUEiSugwKSVKXQSFJ6jIoJEldBoUkqcugkCR1GRSSpC6DQpLUZVBIkroMCklSl0EhSeoyKCRJXQaFJKnLoJAkdRkUkqSukwZFkoeSHE7yykhtRZI9SQ601+Uj225PMpnk9SSbRurXJHm5bbu3TYlKmzb18Vbfm2TtSJtt7WccSDI9XaokaQGNc0bxMLB5Ru024JmqWgc8096T5CqGU5le3drcl+S81uZ+YAfDebTXjfS5HThaVVcC9wB3t75WMJx29VpgA3DHaCBJkhbGSYOiqn7IcC7rUVuAnW19J3DjSP2xqnq/qt4AJoENSVYBF1bVczWce/WRGW2m+3oCuL6dbWwC9lTVVFUdBfbw64ElSTrD5nqP4rKqOgTQXi9t9dXAWyP7HWy11W19Zv24NlV1DHgXuLjT169JsiPJRJKJI0eOzHFIkqTZnO6b2ZmlVp36XNscX6x6oKoGVTVYuXLlWAcqSRrPXIPinXY5ifZ6uNUPApeP7LcGeLvV18xSP65NkmXARQwvdZ2oL0nSApprUOwGpp9C2gY8OVLf2p5kuoLhTevn2+Wp95Jc1+4/3DyjzXRfNwHPtvsYTwMbkyxvN7E3tpokaQEtO9kOSb4NfBa4JMlBhk8ifQ3YlWQ78DPgCwBVtT/JLuBV4Bhwa1V90Lq6heETVBcAT7UF4EHg0SSTDM8ktra+ppLcBbzQ9ruzqmbeVJcknWEZfnk/ewwGg5qYmFjsw5CkJSXJvqoazLbN38yWJHUZFJKkLoNCktRlUEiSugwKSVKXQSFJ6jIoJEldBoUkqcugkCR1GRSSpC6DQpLUZVBIkroMCklSl0EhSeoyKCRJXQaFJKlrXkGR5M0kLyd5MclEq61IsifJgfa6fGT/25NMJnk9yaaR+jWtn8kk97bpUmlTqj7e6nuTrJ3P8UqSTt3pOKP4F1W1fmRmpNuAZ6pqHfBMe0+SqxhOc3o1sBm4L8l5rc39wA6Gc2yva9sBtgNHq+pK4B7g7tNwvJKkU3AmLj1tAXa29Z3AjSP1x6rq/ap6A5gENiRZBVxYVc/VcF7WR2a0me7rCeD66bMNSdLCmG9QFPDfk+xLsqPVLquqQwDt9dJWXw28NdL2YKutbusz68e1qapjwLvAxTMPIsmOJBNJJo4cOTLPIUmSRi2bZ/vPVNXbSS4F9iT5aWff2c4EqlPvtTm+UPUA8ADAYDD4te2SpLmb1xlFVb3dXg8DfwFsAN5pl5Nor4fb7geBy0earwHebvU1s9SPa5NkGXARMDWfY5YknZo5B0WSf5jk49PrwEbgFWA3sK3ttg14sq3vBra2J5muYHjT+vl2eeq9JNe1+w83z2gz3ddNwLPtPoYkaYHM59LTZcBftHvLy4D/WlX/LckLwK4k24GfAV8AqKr9SXYBrwLHgFur6oPW1y3Aw8AFwFNtAXgQeDTJJMMzia3zOF5J0hzkbPuCPhgMamJiYrEPQ5KWlCT7Rn7N4Tj+ZrYkqcugkCR1GRSSpC6DQpLUZVBIkroMihnW3vaXi30IkvSRYlBIkroMCklSl0ExCy8/SdKHDApJUpdBIUnqMihOwMtPkjRkUEiSugyKDs8qJMmgOCnDQtK5zqAYw9rb/tLAkHTOWhJBkWRzkteTTCa5bbGOw8CQdC6az1SoCyLJecB/Av4lcBB4Icnuqnp1sY5pNCze/NrvL9ZhSNKC+MgHBbABmKyq/wWQ5DFgC8O5txfdXM8wDBhJS8VSCIrVwFsj7w8C147ukGQHsKO9/WWS1+fx8y4B/m4e7ceSu8/0T5izBRn/R5jjd/zn6vj/0Yk2LIWgyCy1Ou5N1QPAA6flhyUTJ5pg/Fzg+B2/4z93x38iS+Fm9kHg8pH3a4C3F+lYJOmcsxSC4gVgXZIrkvwmsBXYvcjHJEnnjI/8paeqOpbkS8DTwHnAQ1W1/wz+yNNyCWsJc/znNsevX5OqOvlekqRz1lK49CRJWkQGhSSp66wOipP96Y8M3du2v5Tk0ydrm2RFkj1JDrTX5Qs1nlN1hsb/1SQ/T/JiW25YqPGcqnmO/6Ekh5O8MqPNufL5n2j8Z/3nn+TyJD9I8lqS/Um+PNJmyXz+p1VVnZULwxvffwv8Y+A3gZ8AV83Y5wbgKYa/q3EdsPdkbYH/ANzW1m8D7l7ssS7w+L8K/NvFHt+ZHH/b9s+BTwOvzGhz1n/+Jxn/Wf/5A6uAT7f1jwN/s9T+/Z/u5Ww+o/j/f/qjqv4PMP2nP0ZtAR6poR8Bn0iy6iRttwA72/pO4MYzPZA5OlPjXyrmM36q6ofA1Cz9nguff2/8S8Wcx19Vh6rqxwBV9R7wGsO/EDHdZil8/qfV2RwUs/3pj9Vj7tNre1lVHQJor5eexmM+nc7U+AG+1E7VH/oIn3rPZ/w958LnfzLnzOefZC3wKWBvKy2Vz/+0OpuD4qR/+qOzzzhtP+rO1PjvBz4JrAcOAV+f6wGeYfMZ/9ngTI3/nPn8k3wM+A7wlar6xWk8tiXnbA6Kcf70x4n26bV9Z/r0vL0ePo3HfDqdkfFX1TtV9UFV/Qr4JsNT/I+i+Yy/51z4/E/oXPn8k5zPMCS+VVXfHdlnqXz+p9XZHBTj/OmP3cDN7emH64B32+lkr+1uYFtb3wY8eaYHMkdnZPzT/0iazwOv8NE0n/H3nAuf/wmdC59/kgAPAq9V1TdmabMUPv/Ta7Hvpp/JheFTDX/D8OmHf99qfwz8cVsPw0mR/hZ4GRj02rb6xcAzwIH2umKxx7nA43+07fsSw380qxZ7nGdo/N9meGnl/zL85rn9HPv8TzT+s/7zB/4Zw0tQLwEvtuWGpfb5n87FP+EhSeo6my89SZJOA4NCktRlUEiSugwKSVKXQSFJ6jIoJEldBoUkqev/ATYUeGGNfK+hAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "d0s = np.reshape(X[:,:,0],(3000000))\n",
    "d0s = d0s[~(d0s == np.NaN)]\n",
    "d0s\n",
    "plt.hist(d0s,bins='scott') # I believe this is the array containing all the d0s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9552606648306167\n",
      "-0.00024982974122436647\n",
      "2.025820302371854\n",
      "\n",
      "3.9995744091054446\n",
      "347.52365728756206\n",
      "7.301229406928312\n"
     ]
    }
   ],
   "source": [
    "X=np.nan_to_num(X)\n",
    "print(np.mean(X[:,:,0]*1e4))\n",
    "print(np.mean(X[:,:,1]*1e6)) # difficult one to handle this due to large variance\n",
    "print(np.mean(X[:,:,4]*1e4))\n",
    "print()\n",
    "print(np.std(X[:,:,0]*1e4)) # these are troublesome yes\n",
    "print(np.std(X[:,:,1]*1e6))\n",
    "print(np.std(X[:,:,4]*1e4))\n",
    "# so we will apply the above multiples, this will bring the averages to about unity (unfortunately some outliers will\n",
    "# be very large, so probs better to use a min max scaler or something but then exclude 0?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we scale all the features to bring them to average unity, this is very simplistic so we need a better way in future\n",
    "\n",
    "It means it works faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "firsttime=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "if firsttime==True:\n",
    "    X[:,:,0]=X[:,:,0]*1e4\n",
    "    X[:,:,1]=X[:,:,1]*1e6\n",
    "    X[:,:,4]=X[:,:,4]*1e4\n",
    "    firsttime = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we instead use a min-max scaler to try and solve the problem of scale i.e. certain IPs are 10^4 times larger than others. The min-max scaler means all features will be scaled equivalently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we go, minmax scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xscaled = copy.copy(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Greg\\Anaconda3\\lib\\site-packages\\numpy\\lib\\nanfunctions.py:959: RuntimeWarning: All-NaN slice encountered\n",
      "  result = np.apply_along_axis(_nanmedian1d, axis, a, overwrite_input)\n",
      "C:\\Users\\Greg\\Anaconda3\\lib\\site-packages\\numpy\\lib\\function_base.py:3405: RuntimeWarning: All-NaN slice encountered\n",
      "  r = func(a, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Greg\\Anaconda3\\lib\\site-packages\\numpy\\lib\\nanfunctions.py:959: RuntimeWarning: All-NaN slice encountered\n",
      "  result = np.apply_along_axis(_nanmedian1d, axis, a, overwrite_input)\n",
      "C:\\Users\\Greg\\Anaconda3\\lib\\site-packages\\numpy\\lib\\function_base.py:3405: RuntimeWarning: All-NaN slice encountered\n",
      "  r = func(a, **kwargs)\n",
      "C:\\Users\\Greg\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:355: RuntimeWarning: All-NaN slice encountered\n",
      "  data_min = np.nanmin(X, axis=0)\n",
      "C:\\Users\\Greg\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:356: RuntimeWarning: All-NaN slice encountered\n",
      "  data_max = np.nanmax(X, axis=0)\n",
      "C:\\Users\\Greg\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:355: RuntimeWarning: All-NaN slice encountered\n",
      "  data_min = np.nanmin(X, axis=0)\n",
      "C:\\Users\\Greg\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:356: RuntimeWarning: All-NaN slice encountered\n",
      "  data_max = np.nanmax(X, axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Greg\\Anaconda3\\lib\\site-packages\\numpy\\lib\\nanfunctions.py:959: RuntimeWarning: All-NaN slice encountered\n",
      "  result = np.apply_along_axis(_nanmedian1d, axis, a, overwrite_input)\n",
      "C:\\Users\\Greg\\Anaconda3\\lib\\site-packages\\numpy\\lib\\function_base.py:3405: RuntimeWarning: All-NaN slice encountered\n",
      "  r = func(a, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "for track_variable in range(5):\n",
    "    var_to_scale = Xscaled[:,:,track_variable]\n",
    "    if (track_variable == 0):\n",
    "        print((track_variable == 0))\n",
    "        print(track_variable)\n",
    "        scaler=RobustScaler()\n",
    "    elif (track_variable == 4):\n",
    "        print((track_variable == 4))\n",
    "        print(track_variable)\n",
    "        scaler=RobustScaler()\n",
    "    elif (track_variable == 1):\n",
    "        print(track_variable)\n",
    "        scaler = RobustScaler()\n",
    "    else:\n",
    "        scaler=MinMaxScaler([-1,1])\n",
    "    scaler.fit(var_to_scale)\n",
    "    Xscaled[:,:,track_variable] = scaler.transform(var_to_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Greg\\Anaconda3\\lib\\site-packages\\numpy\\lib\\histograms.py:391: RuntimeWarning: invalid value encountered in greater_equal\n",
      "  keep = (a >= first_edge)\n",
      "C:\\Users\\Greg\\Anaconda3\\lib\\site-packages\\numpy\\lib\\histograms.py:392: RuntimeWarning: invalid value encountered in less_equal\n",
      "  keep &= (a <= last_edge)\n",
      "C:\\Users\\Greg\\Anaconda3\\lib\\site-packages\\numpy\\lib\\histograms.py:824: RuntimeWarning: invalid value encountered in greater_equal\n",
      "  keep = (tmp_a >= first_edge)\n",
      "C:\\Users\\Greg\\Anaconda3\\lib\\site-packages\\numpy\\lib\\histograms.py:825: RuntimeWarning: invalid value encountered in less_equal\n",
      "  keep &= (tmp_a <= last_edge)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "var=4\n",
    "trackvar = X[:,:,var]\n",
    "trackvar = trackvar[~(trackvar == np.NaN)]\n",
    "print(len(trackvar))\n",
    "\n",
    "plt.hist(trackvar,bins='scott',log=True)\n",
    "plt.show()\n",
    "\n",
    "trackvar2 = Xscaled[:,:,var]\n",
    "trackvar2 = trackvar2[~(trackvar2 == np.NaN)]\n",
    "print(len(trackvar2))\n",
    "\n",
    "\n",
    "plt.hist(trackvar2,bins='scott',log=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xscaled = np.nan_to_num(Xscaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we scale the outputs, actually more simple, we change units from metres to millimetres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the secondary and tertiary vertices as our labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = bjets_DF[['secVtx_x','secVtx_y','secVtx_z','terVtx_x','terVtx_y','terVtx_z']].values \n",
    "y = y*1000 # change units of vertices from m to mm, keep vals close to unity\n",
    "# again convention call labels 'y'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split first 80000 jets as train and next 20000 as test. Below some plots to show these jets are equivalently distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.nan_to_num(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=Xscaled[:90000]\n",
    "X_test=Xscaled[90000:]\n",
    "y_train=y[:90000]\n",
    "y_test=y[90000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_s1=np.linalg.norm(bjets_DF[['secVtx_x','secVtx_y','secVtx_z']],axis=1)[:90000]\n",
    "b_s2=np.linalg.norm(bjets_DF[['secVtx_x','secVtx_y','secVtx_z']],axis=1)[90000:]\n",
    "b_t1=np.linalg.norm(bjets_DF[['terVtx_x','terVtx_y','terVtx_z']],axis=1)[:90000]\n",
    "b_t2=np.linalg.norm(bjets_DF[['terVtx_x','terVtx_y','terVtx_z']],axis=1)[90000:]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0037041116859276176\n",
      "0.007041554697748507\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(b_s1,bins='scott',range=[0,0.01])\n",
    "#plt.hist(c_SecVtx,bins='scott',range=[0,0.01])\n",
    "plt.hist(b_t1,bins='scott',range=[0,0.01])\n",
    "\n",
    "print(np.mean(b_s1))\n",
    "print(np.mean(b_t1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.003729520352108983\n",
      "0.007016291097179393\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(b_s2,bins='scott',range=[0,0.01])\n",
    "#plt.hist(c_SecVtx,bins='scott',range=[0,0.01])\n",
    "plt.hist(b_t2,bins='scott',range=[0,0.01])\n",
    "\n",
    "print(np.mean(b_s2))\n",
    "print(np.mean(b_t2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So finally we have our features, X, and labels, y. Split into training and testing samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Plots and Sanity Checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I should have some plots on the track variables to ensure everything is logical and working fine, especially when I make changes to the data. Can potentially use seaborn..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing and Training an RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create an RNN based on LSTM cells using keras and tensorflow. The RNN will for each jet candidate take the tracks as inputs and attempt to predict the secondary and tertiary vertex positions. Let's see how well it does.\n",
    "\n",
    "I anticipate having to set a tolerance on the predicted values, it will never get them perfectly but we need to tell it how close it has to get for it to be considered successful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin by creating the RNN architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select number of hidden and dense layers. Initially use same as RNNIP but these can be tuned going forward.\n",
    "\n",
    "nHidden = 100\n",
    "nDense = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "nJets, nTrks, nFeatures = X_train.shape\n",
    "nOutputs = y.shape[1] # ie sec and ter vtx xyz, so 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "trk_inputs = Input(shape=(nTrks,nFeatures),name=\"Trk_inputs\")\n",
    "masked_input = Masking()(trk_inputs)\n",
    "\n",
    "# Feed this merged layer to an RNN\n",
    "lstm = LSTM(nHidden, return_sequences=False, name='LSTM')(masked_input)\n",
    "dpt = Dropout(rate=0.2)(lstm) # this is a very high dropout rate, reduce it\n",
    "\n",
    "my_inputs = trk_inputs\n",
    "\n",
    "# Fully connected layer: This will convert the output of the RNN to our vtx postion predicitons\n",
    "FC = Dense(nDense, activation='relu', name=\"Dense\")(dpt) # is relu fine here? i think so...\n",
    "\n",
    "# Ouptut layer. Sec and Ter Vtx. No activation as this is a regression problem\n",
    "output = Dense(nOutputs, name=\"Vertex_Predictions\")(FC)\n",
    "\n",
    "myRNN = Model(inputs=my_inputs, outputs=output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Trk_inputs (InputLayer)      (None, 30, 5)             0         \n",
      "_________________________________________________________________\n",
      "masking_3 (Masking)          (None, 30, 5)             0         \n",
      "_________________________________________________________________\n",
      "LSTM (LSTM)                  (None, 100)               42400     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "Dense (Dense)                (None, 20)                2020      \n",
      "_________________________________________________________________\n",
      "Vertex_Predictions (Dense)   (None, 6)                 126       \n",
      "=================================================================\n",
      "Total params: 44,546\n",
      "Trainable params: 44,546\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "myRNN.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "myRNN.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mae']) # do i want to add a metric like mse to evaluate during training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " checkpoints, training, evaluation of performance\n",
    " different ways of evaluating performance obviously\n",
    " either akin to Nicole's method for RNNIP\n",
    " or the slighlty different method in https://github.com/agu3rra/NeuralNetwork-RegressionExample/blob/master/Tutorial.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "myRNN_mChkPt = ModelCheckpoint('myRNN_weights.h5',monitor='val_loss', verbose=True,\n",
    "                               save_best_only=True,\n",
    "                               save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStop = EarlyStopping(monitor='val_loss', verbose=True, patience=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 72000 samples, validate on 18000 samples\n",
      "Epoch 1/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 1.6225 - mean_absolute_error: 1.6225Epoch 00001: val_loss improved from inf to 1.16501, saving model to myRNN_weights.h5\n",
      "72000/72000 [==============================] - 24s 328us/step - loss: 1.6222 - mean_absolute_error: 1.6222 - val_loss: 1.1650 - val_mean_absolute_error: 1.1650\n",
      "Epoch 2/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 1.1257 - mean_absolute_error: 1.1257Epoch 00002: val_loss improved from 1.16501 to 0.99921, saving model to myRNN_weights.h5\n",
      "72000/72000 [==============================] - 23s 320us/step - loss: 1.1255 - mean_absolute_error: 1.1255 - val_loss: 0.9992 - val_mean_absolute_error: 0.9992\n",
      "Epoch 3/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 1.0067 - mean_absolute_error: 1.0067Epoch 00003: val_loss improved from 0.99921 to 0.92418, saving model to myRNN_weights.h5\n",
      "72000/72000 [==============================] - 23s 325us/step - loss: 1.0068 - mean_absolute_error: 1.0068 - val_loss: 0.9242 - val_mean_absolute_error: 0.9242\n",
      "Epoch 4/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.9344 - mean_absolute_error: 0.9344- ETA: 2s - loss: 0.9Epoch 00004: val_loss improved from 0.92418 to 0.85439, saving model to myRNN_weights.h5\n",
      "72000/72000 [==============================] - 25s 343us/step - loss: 0.9343 - mean_absolute_error: 0.9343 - val_loss: 0.8544 - val_mean_absolute_error: 0.8544\n",
      "Epoch 5/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.8913 - mean_absolute_error: 0.8913Epoch 00005: val_loss improved from 0.85439 to 0.81601, saving model to myRNN_weights.h5\n",
      "72000/72000 [==============================] - 26s 355us/step - loss: 0.8914 - mean_absolute_error: 0.8914 - val_loss: 0.8160 - val_mean_absolute_error: 0.8160\n",
      "Epoch 6/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.8632 - mean_absolute_error: 0.8632Epoch 00006: val_loss improved from 0.81601 to 0.81037, saving model to myRNN_weights.h5\n",
      "72000/72000 [==============================] - 26s 363us/step - loss: 0.8632 - mean_absolute_error: 0.8632 - val_loss: 0.8104 - val_mean_absolute_error: 0.8104\n",
      "Epoch 7/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.8416 - mean_absolute_error: 0.8416Epoch 00007: val_loss improved from 0.81037 to 0.77559, saving model to myRNN_weights.h5\n",
      "72000/72000 [==============================] - 25s 352us/step - loss: 0.8416 - mean_absolute_error: 0.8416 - val_loss: 0.7756 - val_mean_absolute_error: 0.7756\n",
      "Epoch 8/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.8260 - mean_absolute_error: 0.8260Epoch 00008: val_loss improved from 0.77559 to 0.76682, saving model to myRNN_weights.h5\n",
      "72000/72000 [==============================] - 25s 349us/step - loss: 0.8258 - mean_absolute_error: 0.8258 - val_loss: 0.7668 - val_mean_absolute_error: 0.7668\n",
      "Epoch 9/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.8128 - mean_absolute_error: 0.8128Epoch 00009: val_loss improved from 0.76682 to 0.75503, saving model to myRNN_weights.h5\n",
      "72000/72000 [==============================] - 26s 364us/step - loss: 0.8130 - mean_absolute_error: 0.8130 - val_loss: 0.7550 - val_mean_absolute_error: 0.7550\n",
      "Epoch 10/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.8013 - mean_absolute_error: 0.8013Epoch 00010: val_loss improved from 0.75503 to 0.74636, saving model to myRNN_weights.h5\n",
      "72000/72000 [==============================] - 26s 357us/step - loss: 0.8012 - mean_absolute_error: 0.8012 - val_loss: 0.7464 - val_mean_absolute_error: 0.7464\n",
      "Epoch 11/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.7912 - mean_absolute_error: 0.7912Epoch 00011: val_loss improved from 0.74636 to 0.73986, saving model to myRNN_weights.h5\n",
      "72000/72000 [==============================] - 26s 357us/step - loss: 0.7910 - mean_absolute_error: 0.7910 - val_loss: 0.7399 - val_mean_absolute_error: 0.7399\n",
      "Epoch 12/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.7816 - mean_absolute_error: 0.7816Epoch 00012: val_loss improved from 0.73986 to 0.72578, saving model to myRNN_weights.h5\n",
      "72000/72000 [==============================] - 27s 372us/step - loss: 0.7815 - mean_absolute_error: 0.7815 - val_loss: 0.7258 - val_mean_absolute_error: 0.7258\n",
      "Epoch 13/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.7721 - mean_absolute_error: 0.7721Epoch 00013: val_loss did not improve\n",
      "72000/72000 [==============================] - 26s 357us/step - loss: 0.7721 - mean_absolute_error: 0.7721 - val_loss: 0.7302 - val_mean_absolute_error: 0.7302\n",
      "Epoch 14/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.7644 - mean_absolute_error: 0.7644Epoch 00014: val_loss improved from 0.72578 to 0.72168, saving model to myRNN_weights.h5\n",
      "72000/72000 [==============================] - 25s 353us/step - loss: 0.7644 - mean_absolute_error: 0.7644 - val_loss: 0.7217 - val_mean_absolute_error: 0.7217\n",
      "Epoch 15/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.7572 - mean_absolute_error: 0.7572Epoch 00015: val_loss improved from 0.72168 to 0.70212, saving model to myRNN_weights.h5\n",
      "72000/72000 [==============================] - 25s 353us/step - loss: 0.7571 - mean_absolute_error: 0.7571 - val_loss: 0.7021 - val_mean_absolute_error: 0.7021\n",
      "Epoch 16/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.7471 - mean_absolute_error: 0.7471Epoch 00016: val_loss improved from 0.70212 to 0.68787, saving model to myRNN_weights.h5\n",
      "72000/72000 [==============================] - 25s 354us/step - loss: 0.7471 - mean_absolute_error: 0.7471 - val_loss: 0.6879 - val_mean_absolute_error: 0.6879\n",
      "Epoch 17/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.7408 - mean_absolute_error: 0.7408Epoch 00017: val_loss improved from 0.68787 to 0.68309, saving model to myRNN_weights.h5\n",
      "72000/72000 [==============================] - 26s 355us/step - loss: 0.7410 - mean_absolute_error: 0.7410 - val_loss: 0.6831 - val_mean_absolute_error: 0.6831\n",
      "Epoch 18/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.7318 - mean_absolute_error: 0.7318Epoch 00018: val_loss improved from 0.68309 to 0.67346, saving model to myRNN_weights.h5\n",
      "72000/72000 [==============================] - 25s 354us/step - loss: 0.7317 - mean_absolute_error: 0.7317 - val_loss: 0.6735 - val_mean_absolute_error: 0.6735\n",
      "Epoch 19/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.7247 - mean_absolute_error: 0.7247Epoch 00019: val_loss improved from 0.67346 to 0.66421, saving model to myRNN_weights.h5\n",
      "72000/72000 [==============================] - 25s 351us/step - loss: 0.7247 - mean_absolute_error: 0.7247 - val_loss: 0.6642 - val_mean_absolute_error: 0.6642\n",
      "Epoch 20/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.7161 - mean_absolute_error: 0.7161Epoch 00020: val_loss improved from 0.66421 to 0.66295, saving model to myRNN_weights.h5\n",
      "72000/72000 [==============================] - 25s 351us/step - loss: 0.7162 - mean_absolute_error: 0.7162 - val_loss: 0.6630 - val_mean_absolute_error: 0.6630\n",
      "Epoch 21/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.7021 - mean_absolute_error: 0.7021Epoch 00021: val_loss improved from 0.66295 to 0.64345, saving model to myRNN_weights.h5\n",
      "72000/72000 [==============================] - 25s 352us/step - loss: 0.7023 - mean_absolute_error: 0.7023 - val_loss: 0.6435 - val_mean_absolute_error: 0.6435\n",
      "Epoch 22/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.6907 - mean_absolute_error: 0.6907Epoch 00022: val_loss improved from 0.64345 to 0.64051, saving model to myRNN_weights.h5\n",
      "72000/72000 [==============================] - 25s 352us/step - loss: 0.6907 - mean_absolute_error: 0.6907 - val_loss: 0.6405 - val_mean_absolute_error: 0.6405\n",
      "Epoch 23/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.6799 - mean_absolute_error: 0.6799Epoch 00023: val_loss improved from 0.64051 to 0.62670, saving model to myRNN_weights.h5\n",
      "72000/72000 [==============================] - 26s 356us/step - loss: 0.6800 - mean_absolute_error: 0.6800 - val_loss: 0.6267 - val_mean_absolute_error: 0.6267\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.6674 - mean_absolute_error: 0.6674Epoch 00024: val_loss improved from 0.62670 to 0.61170, saving model to myRNN_weights.h5\n",
      "72000/72000 [==============================] - 26s 363us/step - loss: 0.6674 - mean_absolute_error: 0.6674 - val_loss: 0.6117 - val_mean_absolute_error: 0.6117\n",
      "Epoch 25/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.6574 - mean_absolute_error: 0.6574Epoch 00025: val_loss improved from 0.61170 to 0.59746, saving model to myRNN_weights.h5\n",
      "72000/72000 [==============================] - 25s 352us/step - loss: 0.6573 - mean_absolute_error: 0.6573 - val_loss: 0.5975 - val_mean_absolute_error: 0.5975\n",
      "Epoch 26/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.6476 - mean_absolute_error: 0.6476Epoch 00026: val_loss improved from 0.59746 to 0.59402, saving model to myRNN_weights.h5\n",
      "72000/72000 [==============================] - 25s 350us/step - loss: 0.6475 - mean_absolute_error: 0.6475 - val_loss: 0.5940 - val_mean_absolute_error: 0.5940\n",
      "Epoch 27/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.6394 - mean_absolute_error: 0.6394Epoch 00027: val_loss improved from 0.59402 to 0.58238, saving model to myRNN_weights.h5\n",
      "72000/72000 [==============================] - 25s 350us/step - loss: 0.6395 - mean_absolute_error: 0.6395 - val_loss: 0.5824 - val_mean_absolute_error: 0.5824\n",
      "Epoch 28/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.6319 - mean_absolute_error: 0.6319Epoch 00028: val_loss improved from 0.58238 to 0.57415, saving model to myRNN_weights.h5\n",
      "72000/72000 [==============================] - 25s 350us/step - loss: 0.6318 - mean_absolute_error: 0.6318 - val_loss: 0.5741 - val_mean_absolute_error: 0.5741\n",
      "Epoch 29/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.6263 - mean_absolute_error: 0.6263- ETA: 1s - loss: 0.6273 - meanEpoch 00029: val_loss did not improve\n",
      "72000/72000 [==============================] - 25s 352us/step - loss: 0.6263 - mean_absolute_error: 0.6263 - val_loss: 0.5830 - val_mean_absolute_error: 0.5830\n",
      "Epoch 30/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.6197 - mean_absolute_error: 0.6197Epoch 00030: val_loss improved from 0.57415 to 0.56646, saving model to myRNN_weights.h5\n",
      "72000/72000 [==============================] - 25s 350us/step - loss: 0.6196 - mean_absolute_error: 0.6196 - val_loss: 0.5665 - val_mean_absolute_error: 0.5665\n",
      "Epoch 31/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.6157 - mean_absolute_error: 0.6157- ETA: 2s - loss: 0.6165 - mEpoch 00031: val_loss improved from 0.56646 to 0.56370, saving model to myRNN_weights.h5\n",
      "72000/72000 [==============================] - 25s 350us/step - loss: 0.6157 - mean_absolute_error: 0.6157 - val_loss: 0.5637 - val_mean_absolute_error: 0.5637\n",
      "Epoch 32/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.6101 - mean_absolute_error: 0.6101Epoch 00032: val_loss improved from 0.56370 to 0.55617, saving model to myRNN_weights.h5\n",
      "72000/72000 [==============================] - 25s 344us/step - loss: 0.6101 - mean_absolute_error: 0.6101 - val_loss: 0.5562 - val_mean_absolute_error: 0.5562\n",
      "Epoch 33/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.6055 - mean_absolute_error: 0.6055Epoch 00033: val_loss improved from 0.55617 to 0.55543, saving model to myRNN_weights.h5\n",
      "72000/72000 [==============================] - 27s 373us/step - loss: 0.6057 - mean_absolute_error: 0.6057 - val_loss: 0.5554 - val_mean_absolute_error: 0.5554\n",
      "Epoch 34/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.6017 - mean_absolute_error: 0.6017- ETA: 1s - loss: 0.6014 - mean_absolEpoch 00034: val_loss improved from 0.55543 to 0.55174, saving model to myRNN_weights.h5\n",
      "72000/72000 [==============================] - 25s 352us/step - loss: 0.6017 - mean_absolute_error: 0.6017 - val_loss: 0.5517 - val_mean_absolute_error: 0.5517\n",
      "Epoch 35/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.5984 - mean_absolute_error: 0.5984- ETA: 3s - loss: Epoch 00035: val_loss did not improve\n",
      "72000/72000 [==============================] - 26s 367us/step - loss: 0.5984 - mean_absolute_error: 0.5984 - val_loss: 0.5534 - val_mean_absolute_error: 0.5534\n",
      "Epoch 36/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.5923 - mean_absolute_error: 0.5923Epoch 00036: val_loss improved from 0.55174 to 0.53712, saving model to myRNN_weights.h5\n",
      "72000/72000 [==============================] - 25s 354us/step - loss: 0.5923 - mean_absolute_error: 0.5923 - val_loss: 0.5371 - val_mean_absolute_error: 0.5371\n",
      "Epoch 37/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.5904 - mean_absolute_error: 0.5904Epoch 00037: val_loss improved from 0.53712 to 0.53232, saving model to myRNN_weights.h5\n",
      "72000/72000 [==============================] - 27s 372us/step - loss: 0.5905 - mean_absolute_error: 0.5905 - val_loss: 0.5323 - val_mean_absolute_error: 0.5323\n",
      "Epoch 38/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.5858 - mean_absolute_error: 0.5858Epoch 00038: val_loss improved from 0.53232 to 0.53224, saving model to myRNN_weights.h5\n",
      "72000/72000 [==============================] - 26s 359us/step - loss: 0.5858 - mean_absolute_error: 0.5858 - val_loss: 0.5322 - val_mean_absolute_error: 0.5322\n",
      "Epoch 39/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.5818 - mean_absolute_error: 0.5818Epoch 00039: val_loss did not improve\n",
      "72000/72000 [==============================] - 26s 357us/step - loss: 0.5818 - mean_absolute_error: 0.5818 - val_loss: 0.5374 - val_mean_absolute_error: 0.5374\n",
      "Epoch 40/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.5790 - mean_absolute_error: 0.5790Epoch 00040: val_loss improved from 0.53224 to 0.52467, saving model to myRNN_weights.h5\n",
      "72000/72000 [==============================] - 26s 355us/step - loss: 0.5790 - mean_absolute_error: 0.5790 - val_loss: 0.5247 - val_mean_absolute_error: 0.5247\n",
      "Epoch 41/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.5777 - mean_absolute_error: 0.5777Epoch 00041: val_loss did not improve\n",
      "72000/72000 [==============================] - 31s 428us/step - loss: 0.5777 - mean_absolute_error: 0.5777 - val_loss: 0.5295 - val_mean_absolute_error: 0.5295\n",
      "Epoch 42/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.5735 - mean_absolute_error: 0.5735Epoch 00042: val_loss improved from 0.52467 to 0.51843, saving model to myRNN_weights.h5\n",
      "72000/72000 [==============================] - 26s 364us/step - loss: 0.5734 - mean_absolute_error: 0.5734 - val_loss: 0.5184 - val_mean_absolute_error: 0.5184\n",
      "Epoch 43/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.5734 - mean_absolute_error: 0.5734Epoch 00043: val_loss did not improve\n",
      "72000/72000 [==============================] - 26s 358us/step - loss: 0.5734 - mean_absolute_error: 0.5734 - val_loss: 0.5202 - val_mean_absolute_error: 0.5202\n",
      "Epoch 44/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.5670 - mean_absolute_error: 0.5670Epoch 00044: val_loss did not improve\n",
      "72000/72000 [==============================] - 25s 354us/step - loss: 0.5669 - mean_absolute_error: 0.5669 - val_loss: 0.5205 - val_mean_absolute_error: 0.5205\n",
      "Epoch 45/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.5649 - mean_absolute_error: 0.5649Epoch 00045: val_loss did not improve\n",
      "72000/72000 [==============================] - 26s 358us/step - loss: 0.5650 - mean_absolute_error: 0.5650 - val_loss: 0.5194 - val_mean_absolute_error: 0.5194\n",
      "Epoch 46/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.5635 - mean_absolute_error: 0.5635Epoch 00046: val_loss improved from 0.51843 to 0.51822, saving model to myRNN_weights.h5\n",
      "72000/72000 [==============================] - 26s 359us/step - loss: 0.5636 - mean_absolute_error: 0.5636 - val_loss: 0.5182 - val_mean_absolute_error: 0.5182\n",
      "Epoch 47/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.5606 - mean_absolute_error: 0.5606Epoch 00047: val_loss improved from 0.51822 to 0.51193, saving model to myRNN_weights.h5\n",
      "72000/72000 [==============================] - 25s 354us/step - loss: 0.5607 - mean_absolute_error: 0.5607 - val_loss: 0.5119 - val_mean_absolute_error: 0.5119\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.5596 - mean_absolute_error: 0.5596- ETA: 0s - loss: 0.5598 - mean_absolute_errEpoch 00048: val_loss improved from 0.51193 to 0.50864, saving model to myRNN_weights.h5\n",
      "72000/72000 [==============================] - 26s 358us/step - loss: 0.5596 - mean_absolute_error: 0.5596 - val_loss: 0.5086 - val_mean_absolute_error: 0.5086\n",
      "Epoch 49/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.5551 - mean_absolute_error: 0.5551Epoch 00049: val_loss improved from 0.50864 to 0.50738, saving model to myRNN_weights.h5\n",
      "72000/72000 [==============================] - 25s 354us/step - loss: 0.5549 - mean_absolute_error: 0.5549 - val_loss: 0.5074 - val_mean_absolute_error: 0.5074\n",
      "Epoch 50/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.5548 - mean_absolute_error: 0.5548Epoch 00050: val_loss improved from 0.50738 to 0.49932, saving model to myRNN_weights.h5\n",
      "72000/72000 [==============================] - 26s 361us/step - loss: 0.5548 - mean_absolute_error: 0.5548 - val_loss: 0.4993 - val_mean_absolute_error: 0.4993\n",
      "Epoch 51/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.5529 - mean_absolute_error: 0.5529Epoch 00051: val_loss did not improve\n",
      "72000/72000 [==============================] - 25s 354us/step - loss: 0.5528 - mean_absolute_error: 0.5528 - val_loss: 0.5055 - val_mean_absolute_error: 0.5055\n",
      "Epoch 52/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.5483 - mean_absolute_error: 0.5483Epoch 00052: val_loss improved from 0.49932 to 0.49868, saving model to myRNN_weights.h5\n",
      "72000/72000 [==============================] - 25s 350us/step - loss: 0.5482 - mean_absolute_error: 0.5482 - val_loss: 0.4987 - val_mean_absolute_error: 0.4987\n",
      "Epoch 53/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.5462 - mean_absolute_error: 0.5462Epoch 00053: val_loss did not improve\n",
      "72000/72000 [==============================] - 25s 353us/step - loss: 0.5464 - mean_absolute_error: 0.5464 - val_loss: 0.5003 - val_mean_absolute_error: 0.5003\n",
      "Epoch 54/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.5452 - mean_absolute_error: 0.5452Epoch 00054: val_loss did not improve\n",
      "72000/72000 [==============================] - 26s 356us/step - loss: 0.5452 - mean_absolute_error: 0.5452 - val_loss: 0.4991 - val_mean_absolute_error: 0.4991\n",
      "Epoch 55/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.5430 - mean_absolute_error: 0.5430Epoch 00055: val_loss improved from 0.49868 to 0.49529, saving model to myRNN_weights.h5\n",
      "72000/72000 [==============================] - 25s 352us/step - loss: 0.5431 - mean_absolute_error: 0.5431 - val_loss: 0.4953 - val_mean_absolute_error: 0.4953\n",
      "Epoch 56/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.5401 - mean_absolute_error: 0.5401Epoch 00056: val_loss improved from 0.49529 to 0.49241, saving model to myRNN_weights.h5\n",
      "72000/72000 [==============================] - 26s 355us/step - loss: 0.5402 - mean_absolute_error: 0.5402 - val_loss: 0.4924 - val_mean_absolute_error: 0.4924\n",
      "Epoch 57/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.5399 - mean_absolute_error: 0.5399Epoch 00057: val_loss did not improve\n",
      "72000/72000 [==============================] - 25s 351us/step - loss: 0.5400 - mean_absolute_error: 0.5400 - val_loss: 0.4951 - val_mean_absolute_error: 0.4951\n",
      "Epoch 58/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.5383 - mean_absolute_error: 0.5383Epoch 00058: val_loss improved from 0.49241 to 0.48385, saving model to myRNN_weights.h5\n",
      "72000/72000 [==============================] - 26s 357us/step - loss: 0.5382 - mean_absolute_error: 0.5382 - val_loss: 0.4838 - val_mean_absolute_error: 0.4838\n",
      "Epoch 59/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.5357 - mean_absolute_error: 0.5357Epoch 00059: val_loss did not improve\n",
      "72000/72000 [==============================] - 26s 356us/step - loss: 0.5359 - mean_absolute_error: 0.5359 - val_loss: 0.4885 - val_mean_absolute_error: 0.4885\n",
      "Epoch 60/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.5354 - mean_absolute_error: 0.5354Epoch 00060: val_loss did not improve\n",
      "72000/72000 [==============================] - 25s 350us/step - loss: 0.5354 - mean_absolute_error: 0.5354 - val_loss: 0.4989 - val_mean_absolute_error: 0.4989\n",
      "Epoch 61/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.5325 - mean_absolute_error: 0.5325Epoch 00061: val_loss did not improve\n",
      "72000/72000 [==============================] - 25s 352us/step - loss: 0.5327 - mean_absolute_error: 0.5327 - val_loss: 0.4845 - val_mean_absolute_error: 0.4845\n",
      "Epoch 62/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.5302 - mean_absolute_error: 0.5302Epoch 00062: val_loss did not improve\n",
      "72000/72000 [==============================] - 26s 355us/step - loss: 0.5305 - mean_absolute_error: 0.5305 - val_loss: 0.4892 - val_mean_absolute_error: 0.4892\n",
      "Epoch 63/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.5289 - mean_absolute_error: 0.5289Epoch 00063: val_loss did not improve\n",
      "72000/72000 [==============================] - 25s 352us/step - loss: 0.5289 - mean_absolute_error: 0.5289 - val_loss: 0.4875 - val_mean_absolute_error: 0.4875\n",
      "Epoch 64/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.5268 - mean_absolute_error: 0.5268Epoch 00064: val_loss improved from 0.48385 to 0.48264, saving model to myRNN_weights.h5\n",
      "72000/72000 [==============================] - 25s 351us/step - loss: 0.5268 - mean_absolute_error: 0.5268 - val_loss: 0.4826 - val_mean_absolute_error: 0.4826\n",
      "Epoch 65/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.5260 - mean_absolute_error: 0.5260Epoch 00065: val_loss improved from 0.48264 to 0.47998, saving model to myRNN_weights.h5\n",
      "72000/72000 [==============================] - 26s 357us/step - loss: 0.5259 - mean_absolute_error: 0.5259 - val_loss: 0.4800 - val_mean_absolute_error: 0.4800\n",
      "Epoch 66/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.5242 - mean_absolute_error: 0.5242Epoch 00066: val_loss did not improve\n",
      "72000/72000 [==============================] - 25s 354us/step - loss: 0.5243 - mean_absolute_error: 0.5243 - val_loss: 0.4822 - val_mean_absolute_error: 0.4822\n",
      "Epoch 67/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.5240 - mean_absolute_error: 0.5240Epoch 00067: val_loss improved from 0.47998 to 0.47573, saving model to myRNN_weights.h5\n",
      "72000/72000 [==============================] - 25s 354us/step - loss: 0.5239 - mean_absolute_error: 0.5239 - val_loss: 0.4757 - val_mean_absolute_error: 0.4757\n",
      "Epoch 68/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.5207 - mean_absolute_error: 0.5207Epoch 00068: val_loss did not improve\n",
      "72000/72000 [==============================] - 25s 354us/step - loss: 0.5207 - mean_absolute_error: 0.5207 - val_loss: 0.4852 - val_mean_absolute_error: 0.4852\n",
      "Epoch 69/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.5207 - mean_absolute_error: 0.5207Epoch 00069: val_loss did not improve\n",
      "72000/72000 [==============================] - 25s 352us/step - loss: 0.5206 - mean_absolute_error: 0.5206 - val_loss: 0.4770 - val_mean_absolute_error: 0.4770\n",
      "Epoch 70/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.5172 - mean_absolute_error: 0.5172Epoch 00070: val_loss did not improve\n",
      "72000/72000 [==============================] - 25s 352us/step - loss: 0.5173 - mean_absolute_error: 0.5173 - val_loss: 0.4790 - val_mean_absolute_error: 0.4790\n",
      "Epoch 71/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.5182 - mean_absolute_error: 0.5182Epoch 00071: val_loss improved from 0.47573 to 0.46950, saving model to myRNN_weights.h5\n",
      "72000/72000 [==============================] - 25s 354us/step - loss: 0.5182 - mean_absolute_error: 0.5182 - val_loss: 0.4695 - val_mean_absolute_error: 0.4695\n",
      "Epoch 72/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.5163 - mean_absolute_error: 0.5163Epoch 00072: val_loss improved from 0.46950 to 0.46779, saving model to myRNN_weights.h5\n",
      "72000/72000 [==============================] - 25s 352us/step - loss: 0.5163 - mean_absolute_error: 0.5163 - val_loss: 0.4678 - val_mean_absolute_error: 0.4678\n",
      "Epoch 73/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.5152 - mean_absolute_error: 0.5152Epoch 00073: val_loss did not improve\n",
      "72000/72000 [==============================] - 25s 352us/step - loss: 0.5152 - mean_absolute_error: 0.5152 - val_loss: 0.4767 - val_mean_absolute_error: 0.4767\n",
      "Epoch 74/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.5138 - mean_absolute_error: 0.5138Epoch 00074: val_loss did not improve\n",
      "72000/72000 [==============================] - 25s 351us/step - loss: 0.5138 - mean_absolute_error: 0.5138 - val_loss: 0.4730 - val_mean_absolute_error: 0.4730\n",
      "Epoch 75/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.5131 - mean_absolute_error: 0.5131Epoch 00075: val_loss did not improve\n",
      "72000/72000 [==============================] - 25s 353us/step - loss: 0.5131 - mean_absolute_error: 0.5131 - val_loss: 0.4723 - val_mean_absolute_error: 0.4723\n",
      "Epoch 76/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.5109 - mean_absolute_error: 0.5109Epoch 00076: val_loss did not improve\n",
      "72000/72000 [==============================] - 25s 354us/step - loss: 0.5109 - mean_absolute_error: 0.5109 - val_loss: 0.4708 - val_mean_absolute_error: 0.4708\n",
      "Epoch 77/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.5098 - mean_absolute_error: 0.5098Epoch 00077: val_loss improved from 0.46779 to 0.46146, saving model to myRNN_weights.h5\n",
      "72000/72000 [==============================] - 26s 365us/step - loss: 0.5098 - mean_absolute_error: 0.5098 - val_loss: 0.4615 - val_mean_absolute_error: 0.4615\n",
      "Epoch 78/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.5088 - mean_absolute_error: 0.5088Epoch 00078: val_loss did not improve\n",
      "72000/72000 [==============================] - 25s 353us/step - loss: 0.5088 - mean_absolute_error: 0.5088 - val_loss: 0.4637 - val_mean_absolute_error: 0.4637\n",
      "Epoch 79/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.5064 - mean_absolute_error: 0.5064Epoch 00079: val_loss did not improve\n",
      "72000/72000 [==============================] - 25s 351us/step - loss: 0.5064 - mean_absolute_error: 0.5064 - val_loss: 0.4635 - val_mean_absolute_error: 0.4635\n",
      "Epoch 80/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.5066 - mean_absolute_error: 0.5066Epoch 00080: val_loss did not improve\n",
      "72000/72000 [==============================] - 25s 352us/step - loss: 0.5066 - mean_absolute_error: 0.5066 - val_loss: 0.4681 - val_mean_absolute_error: 0.4681\n",
      "Epoch 81/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.5043 - mean_absolute_error: 0.5043Epoch 00081: val_loss did not improve\n",
      "72000/72000 [==============================] - 26s 357us/step - loss: 0.5043 - mean_absolute_error: 0.5043 - val_loss: 0.4656 - val_mean_absolute_error: 0.4656\n",
      "Epoch 82/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.5045 - mean_absolute_error: 0.5045Epoch 00082: val_loss did not improve\n",
      "72000/72000 [==============================] - 27s 374us/step - loss: 0.5045 - mean_absolute_error: 0.5045 - val_loss: 0.4674 - val_mean_absolute_error: 0.4674\n",
      "Epoch 83/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.5033 - mean_absolute_error: 0.5033Epoch 00083: val_loss did not improve\n",
      "72000/72000 [==============================] - 29s 396us/step - loss: 0.5032 - mean_absolute_error: 0.5032 - val_loss: 0.4730 - val_mean_absolute_error: 0.4730\n",
      "Epoch 84/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.5021 - mean_absolute_error: 0.5021Epoch 00084: val_loss improved from 0.46146 to 0.45825, saving model to myRNN_weights.h5\n",
      "72000/72000 [==============================] - 26s 367us/step - loss: 0.5019 - mean_absolute_error: 0.5019 - val_loss: 0.4582 - val_mean_absolute_error: 0.4582\n",
      "Epoch 85/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.4993 - mean_absolute_error: 0.4993Epoch 00085: val_loss did not improve\n",
      "72000/72000 [==============================] - 26s 358us/step - loss: 0.4995 - mean_absolute_error: 0.4995 - val_loss: 0.4613 - val_mean_absolute_error: 0.4613\n",
      "Epoch 86/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.5002 - mean_absolute_error: 0.5002Epoch 00086: val_loss did not improve\n",
      "72000/72000 [==============================] - 24s 328us/step - loss: 0.5002 - mean_absolute_error: 0.5002 - val_loss: 0.4661 - val_mean_absolute_error: 0.4661\n",
      "Epoch 87/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.4992 - mean_absolute_error: 0.4992Epoch 00087: val_loss improved from 0.45825 to 0.45224, saving model to myRNN_weights.h5\n",
      "72000/72000 [==============================] - 23s 322us/step - loss: 0.4991 - mean_absolute_error: 0.4991 - val_loss: 0.4522 - val_mean_absolute_error: 0.4522\n",
      "Epoch 88/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.4970 - mean_absolute_error: 0.4970Epoch 00088: val_loss did not improve\n",
      "72000/72000 [==============================] - 23s 322us/step - loss: 0.4970 - mean_absolute_error: 0.4970 - val_loss: 0.4597 - val_mean_absolute_error: 0.4597\n",
      "Epoch 89/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.4968 - mean_absolute_error: 0.4968Epoch 00089: val_loss did not improve\n",
      "72000/72000 [==============================] - 24s 329us/step - loss: 0.4969 - mean_absolute_error: 0.4969 - val_loss: 0.4551 - val_mean_absolute_error: 0.4551\n",
      "Epoch 90/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.4960 - mean_absolute_error: 0.4960Epoch 00090: val_loss did not improve\n",
      "72000/72000 [==============================] - 24s 328us/step - loss: 0.4960 - mean_absolute_error: 0.4960 - val_loss: 0.4551 - val_mean_absolute_error: 0.4551\n",
      "Epoch 91/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.4945 - mean_absolute_error: 0.4945Epoch 00091: val_loss did not improve\n",
      "72000/72000 [==============================] - 23s 322us/step - loss: 0.4945 - mean_absolute_error: 0.4945 - val_loss: 0.4556 - val_mean_absolute_error: 0.4556\n",
      "Epoch 92/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.4934 - mean_absolute_error: 0.4934Epoch 00092: val_loss did not improve\n",
      "72000/72000 [==============================] - 25s 345us/step - loss: 0.4934 - mean_absolute_error: 0.4934 - val_loss: 0.4532 - val_mean_absolute_error: 0.4532\n",
      "Epoch 93/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.4911 - mean_absolute_error: 0.4911- ETA: 3s - loss: 0.4899 - mean_absolute_error: 0. - ETA: 2s - loss: 0.4899 -Epoch 00093: val_loss did not improve\n",
      "72000/72000 [==============================] - 29s 408us/step - loss: 0.4911 - mean_absolute_error: 0.4911 - val_loss: 0.4553 - val_mean_absolute_error: 0.4553\n",
      "Epoch 94/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.4917 - mean_absolute_error: 0.4917Epoch 00094: val_loss improved from 0.45224 to 0.45152, saving model to myRNN_weights.h5\n",
      "72000/72000 [==============================] - 27s 380us/step - loss: 0.4917 - mean_absolute_error: 0.4917 - val_loss: 0.4515 - val_mean_absolute_error: 0.4515\n",
      "Epoch 95/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.4900 - mean_absolute_error: 0.4900Epoch 00095: val_loss improved from 0.45152 to 0.44814, saving model to myRNN_weights.h5\n",
      "72000/72000 [==============================] - 26s 359us/step - loss: 0.4900 - mean_absolute_error: 0.4900 - val_loss: 0.4481 - val_mean_absolute_error: 0.4481\n",
      "Epoch 96/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.4896 - mean_absolute_error: 0.4896Epoch 00096: val_loss did not improve\n",
      "72000/72000 [==============================] - 25s 350us/step - loss: 0.4896 - mean_absolute_error: 0.4896 - val_loss: 0.4526 - val_mean_absolute_error: 0.4526\n",
      "Epoch 97/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.4889 - mean_absolute_error: 0.4889Epoch 00097: val_loss did not improve\n",
      "72000/72000 [==============================] - 27s 382us/step - loss: 0.4889 - mean_absolute_error: 0.4889 - val_loss: 0.4482 - val_mean_absolute_error: 0.4482\n",
      "Epoch 98/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.4893 - mean_absolute_error: 0.4893Epoch 00098: val_loss improved from 0.44814 to 0.44704, saving model to myRNN_weights.h5\n",
      "72000/72000 [==============================] - 29s 407us/step - loss: 0.4892 - mean_absolute_error: 0.4892 - val_loss: 0.4470 - val_mean_absolute_error: 0.4470\n",
      "Epoch 99/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.4869 - mean_absolute_error: 0.4869Epoch 00099: val_loss did not improve\n",
      "72000/72000 [==============================] - 28s 396us/step - loss: 0.4868 - mean_absolute_error: 0.4868 - val_loss: 0.4473 - val_mean_absolute_error: 0.4473\n",
      "Epoch 100/100\n",
      "71936/72000 [============================>.] - ETA: 0s - loss: 0.4855 - mean_absolute_error: 0.4855Epoch 00100: val_loss did not improve\n",
      "72000/72000 [==============================] - 27s 379us/step - loss: 0.4855 - mean_absolute_error: 0.4855 - val_loss: 0.4551 - val_mean_absolute_error: 0.4551\n"
     ]
    }
   ],
   "source": [
    "nEpochs = 100\n",
    "\n",
    "myRNN_hist = myRNN.fit(X_train, y_train, epochs=nEpochs, batch_size=256,validation_split=0.20,\n",
    "                 callbacks=[earlyStop, myRNN_mChkPt],) # callbacks=[earlyStop, myRNN_mChkPt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2d8c168b160>"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = np.arange(1,len(myRNN_hist.history['loss'])+1)\n",
    "\n",
    "plt.plot(epochs,myRNN_hist.history['loss'],label='training')\n",
    "plt.plot(epochs,myRNN_hist.history['val_loss'],label='validation')\n",
    "plt.xlabel('epochs',fontsize=14)\n",
    "plt.ylabel('MSE loss',fontsize=14)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "results=myRNN.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.13304802,  0.23945089, -1.35111896,  0.39915973,  0.94487557,\n",
       "        -5.26029955],\n",
       "       [-0.01436078, -0.01610202, -1.68728142,  0.15514142, -0.2757465 ,\n",
       "        -2.40211707],\n",
       "       [-0.00901601, -1.79431503, -1.06938027, -0.00787202, -3.29133286,\n",
       "        -2.15652935],\n",
       "       ...,\n",
       "       [ 0.40685105, -1.4354374 , -0.26453552,  1.9248198 , -7.22246124,\n",
       "        -1.38853557],\n",
       "       [ 0.04860819,  0.15734347,  0.99811264,  0.12007086,  0.64105703,\n",
       "         3.33961321],\n",
       "       [-0.26486005, -0.09248168, -0.0862998 , -1.68380739, -0.45642696,\n",
       "         0.13947188]])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5267338543181113"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean((results-y_test)/y_test) # average error of 50%, so for vtx at 1mm, we get within 0.5mm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the sec -> ter vtx predicted. I.e ensure ter is further from prim than sec, look at direction sec -> ter,..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "jet_energy                                               43075.4\n",
       "jet_flavour                                                    5\n",
       "secVtx_x                                              0.00417528\n",
       "secVtx_y                                             -0.00100887\n",
       "secVtx_z                                              0.00289704\n",
       "terVtx_x                                              0.00559305\n",
       "terVtx_y                                             -0.00135187\n",
       "terVtx_z                                              0.00407726\n",
       "tracks         [[2.010621651929816e-06, -4.989406079403125e-0...\n",
       "Name: 99999, dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bjets_DF.iloc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at worst perfoming cases and examine the tracks\n",
    "# look at actual uncertainty on overlap for the vertex\n",
    "# re-run with minimised errors on tracks see if performance is better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.05486715, -0.45196092, -2.455217  , -0.1861686 , -0.99255735,\n",
       "        -5.3714914 ]], dtype=float32)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRNN.predict(np.array([X_test[2]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "couple of things, first the RNN looks like it predicts the same value for all cases. This might be because it uses the null tracks at the end of every single jet and just predicts off of those.\n",
    "Maybe should consider predicting values that are at least order unity, because mse is gonna be very small otherwise, so convert the vertices into different units (I dunno maybe millimeters or microns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
